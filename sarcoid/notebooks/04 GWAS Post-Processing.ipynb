{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this must be installed before gwaslab\n",
    "# !pip install pypandoc==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install gwaslab\n",
    "# !pip install gwaslab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldn't need\n",
    "# !pip install --force-reinstall \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldn't need\n",
    "# !pip install --force-reinstall \"matplotlib<3.9\" \"seaborn<0.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import re\n",
    "import io\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars string length to 100\n",
    "pl.Config.set_fmt_str_lengths(100)\n",
    "\n",
    "# Set the row limit to a higher value\n",
    "pl.Config.set_tbl_rows(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_gbq(query):\n",
    "    \"\"\"\n",
    "    Take a SQL query and return result as polars dataframe\n",
    "    :param query: BigQuery SQL query\n",
    "    :return: polars dataframe\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query)\n",
    "    rows = query_job.result()\n",
    "    df = pl.from_arrow(rows.to_arrow())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chr_pos_from_snpid(df: pd.DataFrame, snp_col: str = \"SNPID\") -> pd.DataFrame:\n",
    "    \"\"\"Extract CHR and POS from SNPID column (format CHR-POS-...).\"\"\"\n",
    "    parts = df[snp_col].str.split(\"-\", n=2, expand=True)\n",
    "    df = df.copy()\n",
    "    df[\"CHR\"] = pd.to_numeric(parts[0], errors=\"coerce\")\n",
    "    df[\"POS\"] = pd.to_numeric(parts[1], errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert SNP format\n",
    "def convert_snp_format(row):\n",
    "    snp_id = row['SNPID']\n",
    "    \n",
    "    if not isinstance(snp_id, str):\n",
    "        return snp_id\n",
    "    \n",
    "    if snp_id == '.':\n",
    "        return f\"chr{row['CHR']}:{row['POS']}:{row['NEA']}:{row['EA']}\"\n",
    "    \n",
    "    pattern = r'(\\d+|X|Y)_(\\d+)_([ACGT])_([ACGT])'\n",
    "    match = re.match(pattern, snp_id)\n",
    "    \n",
    "    if match:\n",
    "        chrom, pos, ref, alt = match.groups()\n",
    "        return f\"chr{chrom}:{pos}:{ref}:{alt}\"\n",
    "    else:\n",
    "        return snp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_loci(loci_list):\n",
    "    result = []\n",
    "    for locus in loci_list:\n",
    "        chr_str, pos_alleles = locus.split(':')\n",
    "        pos = int(pos_alleles.split('_')[0])\n",
    "        \n",
    "        # Handle X/Y chromosomes\n",
    "        if chr_str == 'X':\n",
    "            chr_num = 23\n",
    "        elif chr_str == 'Y':\n",
    "            chr_num = 24\n",
    "        else:\n",
    "            chr_num = int(chr_str)\n",
    "            \n",
    "        result.append((chr_num, pos))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GWAS Catalog Associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of conditions to their file(s)\n",
    "gwas_catalog_files = {\n",
    "    'condition__sarcoid': ['MONDO_0019338_associations_export.tsv']\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "gwas_catalog_dfs = {}\n",
    "\n",
    "for condition, files in gwas_catalog_files.items():\n",
    "    dfs_to_concat = []\n",
    "    \n",
    "    for file in files:\n",
    "        # Use 'condition - file' format\n",
    "        file_path = f\"{my_bucket}/data/saige_gwas/{file}\"\n",
    "        df = pl.read_csv(file_path, separator='\\t', columns=['riskAllele', 'efoTraits', 'locations'])\n",
    "        dfs_to_concat.append(df)\n",
    "        print(f\"Loaded {condition} - {file}: {len(df)} rows\")\n",
    "    \n",
    "    # Concatenate if multiple files, otherwise use single df\n",
    "    if len(dfs_to_concat) > 1:\n",
    "        gwas_catalog_dfs[condition] = pl.concat(dfs_to_concat)\n",
    "        print(f\"Concatenated {condition}: {len(gwas_catalog_dfs[condition])} total rows\")\n",
    "    else:\n",
    "        gwas_catalog_dfs[condition] = dfs_to_concat[0]\n",
    "        print(f\"Single file for {condition}: {len(gwas_catalog_dfs[condition])} rows\")\n",
    "\n",
    "# Access individual DataFrames\n",
    "sarcoid_catalog_df = gwas_catalog_dfs['condition__sarcoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcoid_catalog_df.group_by('efoTraits').len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcoid_traits = ['sarcoidosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_data = {\n",
    "    'condition__sarcoid': (sarcoid_catalog_df, sarcoid_traits),\n",
    "}\n",
    "\n",
    "filtered_dfs = {}\n",
    "for condition, (df, traits) in catalog_data.items():\n",
    "    filtered_df = df.filter(pl.col('efoTraits').is_in(traits))\n",
    "\n",
    "    # Remove SNP interactions\n",
    "    filtered_df = filtered_df.filter(~(pl.col('locations').str.contains(',')))\n",
    "    \n",
    "    # Fix missing locations using riskAllele formats:\n",
    "    # \"chr2:211694960-...\", \"10:94433455-A\", \"chrX:21534509-ACC\"\n",
    "    # This will still leave null locations for riskAllele == rsID with no location information\n",
    "    filtered_df = filtered_df.with_columns(\n",
    "        pl.when(pl.col('locations') == \"-\")\n",
    "        .then(\n",
    "            pl.when(pl.col('riskAllele').str.contains(r\"^chr([XY]|\\d+):\"))\n",
    "            .then(\n",
    "                # Handle \"chrX:21534509-ACC\" or \"chr2:211694960-...\"\n",
    "                pl.col('riskAllele').str.extract(r\"^chr([XY]|\\d+):(\\d+)\", 1) + \":\" + \n",
    "                pl.col('riskAllele').str.extract(r\"^chr([XY]|\\d+):(\\d+)\", 2)\n",
    "            )\n",
    "            .when(pl.col('riskAllele').str.contains(r\"^([XY]|\\d+):\"))\n",
    "            .then(\n",
    "                # Handle \"10:94433455-A\" \n",
    "                pl.col('riskAllele').str.extract(r\"^([XY]|\\d+):(\\d+)\", 1) + \":\" + \n",
    "                pl.col('riskAllele').str.extract(r\"^([XY]|\\d+):(\\d+)\", 2)\n",
    "            )\n",
    "            .otherwise(pl.col('locations'))  # Keep original if no pattern matches\n",
    "        )\n",
    "        .otherwise(pl.col('locations'))\n",
    "        .alias('locations')\n",
    "    )\n",
    "\n",
    "    # Add risk nucleotide to locations (only where locations != \"-\")\n",
    "    filtered_df = filtered_df.with_columns(\n",
    "        pl.when((pl.col('locations') != \"-\") & (~pl.col('riskAllele').str.ends_with(\"-?\")))\n",
    "        .then(\n",
    "            pl.col('locations') + \"-\" + \n",
    "            pl.col('riskAllele').str.extract(r\"-([ATCG]+)$\", 1)\n",
    "        )\n",
    "        .otherwise(pl.col('locations'))\n",
    "        .alias('locations')\n",
    "    )\n",
    "\n",
    "    # Sort by chromosome: 1-22, then X, Y\n",
    "    filtered_df = filtered_df.with_columns(\n",
    "        pl.col('locations')\n",
    "        .str.extract(r\"^([^:]+):\")  # Extract chromosome (including X, Y)\n",
    "        .map_elements(\n",
    "            lambda x: int(x) if x and x.isdigit() else (23 if x == 'X' else (24 if x == 'Y' else 99)), \n",
    "            return_dtype=pl.Int32\n",
    "        )\n",
    "        .alias('chr_sort_order')\n",
    "    ).sort(['chr_sort_order', 'locations']).drop('chr_sort_order')\n",
    "\n",
    "    filtered_dfs[condition] = filtered_df\n",
    "    print(f\"{condition}: Filtered from {len(df)} to {len(filtered_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract known loci for all conditions\n",
    "known_loci = {}\n",
    "for condition in ['condition__sarcoid']:\n",
    "    valid_loci_df = filtered_dfs[condition].filter(pl.col('locations') != '-')\n",
    "    total_count = len(valid_loci_df)\n",
    "    unique_loci = valid_loci_df['locations'].unique().to_list()\n",
    "    unique_count = len(unique_loci)\n",
    "    \n",
    "    known_loci[condition] = unique_loci\n",
    "    print(f\"{condition}: {total_count} loci ≠ '-', {unique_count} unique loci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gwaslab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gwaslab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only need to do once\n",
    "# gl.download_ref('1kg_eur_hg38')\n",
    "# gl.download_ref('1kg_afr_hg38')\n",
    "# gl.download_ref('1kg_amr_hg38')\n",
    "# gl.download_ref('1kg_pan_hg38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_populations = {\n",
    "    'eur': '1kg_eur_hg38',\n",
    "    'afr': '1kg_afr_hg38', \n",
    "    'amr': '1kg_amr_hg38',\n",
    "    'metal': '1kg_pan_hg38'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsv_file(query_dir: str) -> str | None:\n",
    "    \"\"\"Return the first .tsv file (not .tsv.info) in a GCS directory.\"\"\"\n",
    "    result = subprocess.run(f'gsutil ls {query_dir}', capture_output=True, shell=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(result.stderr.strip())\n",
    "\n",
    "    files = result.stdout.decode('utf-8').split('\\n')\n",
    "    tsvs = [f for f in files if f.endswith(\"1.tsv\") and not f.endswith(\"1.tsv.info\")]\n",
    "\n",
    "    if not tsvs:\n",
    "        return None  # no .tsv found\n",
    "    if len(tsvs) > 1:\n",
    "        print(f\"Warning: multiple 1.tsv files in {query_dir}, returning first\")\n",
    "\n",
    "    return tsvs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists_in_gcs(bucket_path: str) -> bool:\n",
    "    try:\n",
    "        result = subprocess.run(['gsutil', 'ls', bucket_path], \n",
    "                              capture_output=True, text=True, check=False)\n",
    "        return result.returncode == 0\n",
    "    except FileNotFoundError:\n",
    "        raise RuntimeError(\"gsutil not found - is Google Cloud SDK installed?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def annotate_with_known_loci(lead_snps_df, known_loci_list, window_kb=500):\n",
    "    \"\"\"\n",
    "    Annotate lead SNPs with closest known loci within specified window.\n",
    "    \n",
    "    Args:\n",
    "        lead_snps_df: DataFrame with columns CHR, POS, SNPID\n",
    "        known_loci_list: List of 'CHR:POS_REF_ALT' strings\n",
    "        window_kb: Window size in kb (default 500)\n",
    "    \"\"\"\n",
    "    lead_snps_df = lead_snps_df.copy()\n",
    "\n",
    "    # Parse known loci\n",
    "    known_loci = []\n",
    "    excluded_count = 0\n",
    "\n",
    "    for locus in known_loci_list:\n",
    "        try:\n",
    "            # Split on '-' but handle cases where there might be no allele\n",
    "            if '-' in locus:\n",
    "                chr_pos, alleles = locus.split('-', 1)  \n",
    "            else:\n",
    "                chr_pos = locus\n",
    "                alleles = None\n",
    "\n",
    "            chr_str, pos_str = chr_pos.split(':')\n",
    "            \n",
    "            # Try to convert to int, skip if it's X/Y/MT\n",
    "            try:\n",
    "                chr_num = int(chr_str)\n",
    "                locus_dict = {\n",
    "                    'chr': chr_num,\n",
    "                    'pos': int(pos_str),\n",
    "                    'original': locus\n",
    "                }\n",
    "                # Optionally store allele info if present\n",
    "                if alleles:\n",
    "                    locus_dict['alleles'] = alleles\n",
    "\n",
    "                known_loci.append(locus_dict)\n",
    "            except ValueError:\n",
    "                # X, Y, MT chromosomes\n",
    "                excluded_count += 1\n",
    "                continue\n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Warning: Could not parse known locus '{locus}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    if excluded_count > 0:\n",
    "        print(f\"Info: Excluded {excluded_count} known loci on non-autosomal chromosomes\")\n",
    "    \n",
    "    known_df = pd.DataFrame(known_loci)\n",
    "    window_bp = window_kb * 1000\n",
    "    \n",
    "    def find_closest_known(row):\n",
    "        # Filter to same chromosome\n",
    "        chr_matches = known_df[known_df['chr'] == row['CHR']]\n",
    "        if chr_matches.empty:\n",
    "            return 'None'\n",
    "        \n",
    "        # Calculate distances and filter by window\n",
    "        distances = np.abs(chr_matches['pos'] - row['POS'])\n",
    "        within_window = distances <= window_bp\n",
    "        \n",
    "        if not within_window.any():\n",
    "            return 'None'\n",
    "        \n",
    "        # Return closest match\n",
    "        closest_idx = distances[within_window].idxmin()\n",
    "        return chr_matches.loc[closest_idx, 'original']\n",
    "    \n",
    "    lead_snps_df['KNOWN'] = lead_snps_df.apply(find_closest_known, axis=1)\n",
    "    return lead_snps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def process_gwas(base_dir, ancestry, trait, known_loci_list, plot_params):\n",
    "    \"\"\"Process a single GWAS condition: load, parse, analyze, and plot.\"\"\"\n",
    "    \n",
    "    print(f\"\\nProcessing {trait} {ancestry} GWAS...\")\n",
    "    if ancestry == 'metal':\n",
    "        # Load summary statistics\n",
    "        sumstats = gl.Sumstats(\n",
    "            get_tsv_file(f'{base_dir}/metal/{trait}/'),\n",
    "            fmt=\"metal\",\n",
    "            snpid='MarkerName',\n",
    "            ea='Allele1', \n",
    "            nea='Allele2',\n",
    "            eaf='Freq1',\n",
    "            se='StdErr',\n",
    "            p='P-value',\n",
    "            beta='Effect',\n",
    "            direction='Direction',\n",
    "            build='38'\n",
    "        )\n",
    "        # Get CHR POS from SNPID\n",
    "        sumstats.data = add_chr_pos_from_snpid(sumstats.data, snp_col='SNPID')\n",
    "    else: \n",
    "        sumstats = gl.Sumstats(\n",
    "            f'{base_dir}/{ancestry}/condition__sarcoid/gwas_results.tsv.gz',\n",
    "            fmt=\"saige\",\n",
    "            snpid='vid',\n",
    "            rsid=None,\n",
    "            chrom='chromosome',\n",
    "            pos='base_pair_location',\n",
    "            ea='effect_allele',\n",
    "            nea='other_allele',\n",
    "            eaf='effect_allele_frequency',\n",
    "            se='standard_error',\n",
    "            p='p_value',\n",
    "            beta='beta',\n",
    "            build='38'\n",
    "        )\n",
    "        \n",
    "        # Assuming eur_sumstats.data is a pandas Dateurame\n",
    "        print(\"Converting SNPID\")\n",
    "        sumstats.data['SNPID'] = sumstats.data.apply(convert_snp_format, axis=1)\n",
    "        # Verify the conversion with a sample\n",
    "        print(\"Conversion done.\")\n",
    "        \n",
    "        sumstats.basic_check()\n",
    "        \n",
    "    # Get or load lead SNPs (500 kb window)    \n",
    "    lead_snps = sumstats.get_lead(sig_level=1e-5) #5e-8\n",
    "    lead_snps = annotate_with_known_loci(lead_snps, known_loci_list)\n",
    "    lead_snps.to_csv(f'{base_dir}/{ancestry}/{trait}/{trait}_lead_snps.tsv', \n",
    "                     sep='\\t', index=False)\n",
    "    \n",
    "    # Convert known loci matches to highlight format for plotting\n",
    "    known_matches = lead_snps[lead_snps['KNOWN'] != 'None']['SNPID'].tolist()\n",
    "    unknown_matches = lead_snps[lead_snps['KNOWN'] == 'None']['SNPID'].tolist()\n",
    "\n",
    "    # Build base plotting kwargs\n",
    "    base_plot_kwargs = {\n",
    "        'skip': 2,\n",
    "        'check': False,\n",
    "        'font_family': 'DejaVu Sans',\n",
    "        'marker_size': (5,5),\n",
    "        'ylabels': plot_params['ylabels'],\n",
    "        'fontsize': 11,\n",
    "        'verbose': False,\n",
    "        'fig_args': {\"figsize\":(20,6), \"dpi\":300},\n",
    "        'save_args': {\"dpi\":300, \"facecolor\":\"white\"}\n",
    "    }\n",
    "    \n",
    "    # Add cut and cutfactor if they exist and are not None\n",
    "    if plot_params.get('cut') is not None:\n",
    "        base_plot_kwargs['cut'] = plot_params['cut']\n",
    "    if plot_params.get('cutfactor') is not None:\n",
    "        base_plot_kwargs['cutfactor'] = plot_params['cutfactor']\n",
    "    \n",
    "    print(\"Plotting mqq plots (verbose mode off)\")\n",
    "    \n",
    "    # Plot highlighted MQQ\n",
    "    highlighted_kwargs = base_plot_kwargs.copy()\n",
    "    highlighted_kwargs.update({\n",
    "        'highlight_windowkb': 500,\n",
    "        'highlight': [known_matches, unknown_matches],\n",
    "        'highlight_color': ['#DC143C', '#FFA500'],\n",
    "        'save': f'manhattan_plots/{trait}_{ancestry}_min_2_highlighted_mqq.png'\n",
    "    })\n",
    "    sumstats.plot_mqq(**highlighted_kwargs)\n",
    "    \n",
    "    # Plot regular MQQ\n",
    "    regular_kwargs = base_plot_kwargs.copy()\n",
    "    regular_kwargs['save'] = f'manhattan_plots/{trait}_{ancestry}_min_2_mqq.png'\n",
    "    sumstats.plot_mqq(**regular_kwargs)\n",
    "    \n",
    "    return sumstats, lead_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_lead_snps_across_ancestries(results, trait, ancestries):\n",
    "    \"\"\"\n",
    "    Consolidate lead SNPs across all ancestries for a trait, keeping the SNP \n",
    "    with the lowest p-value for each locus.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'known': [(snp, known_locus, min_p)], 'novel': [(snp, min_p)]}\n",
    "    \"\"\"\n",
    "    # Collect all lead SNPs across ancestries\n",
    "    all_leads = []\n",
    "\n",
    "    for ancestry, data in results[trait].items():\n",
    "        if 'lead_snps' not in data:\n",
    "            continue\n",
    "\n",
    "        lead_snps = data['lead_snps']\n",
    "        for _, row in lead_snps.iterrows():\n",
    "            all_leads.append({\n",
    "                'snpid': row['SNPID'],\n",
    "                'chr': row['CHR'], \n",
    "                'pos': row['POS'],\n",
    "                'p': row['P'],\n",
    "                'known': row['KNOWN'],\n",
    "                'ancestry': ancestry\n",
    "            })\n",
    "                \n",
    "    if not all_leads:\n",
    "        return {'known': [], 'novel': []}\n",
    "    \n",
    "    leads_df = pd.DataFrame(all_leads)\n",
    "    \n",
    "    # Group nearby SNPs (within 500kb) and keep best p-value\n",
    "    consolidated = {'known': [], 'novel': []}\n",
    "    processed_positions = set()\n",
    "    \n",
    "    # Sort by p-value to process best hits first\n",
    "    leads_df = leads_df.sort_values('p')\n",
    "    \n",
    "    for _, row in leads_df.iterrows():\n",
    "        # Check if we've already processed a nearby SNP\n",
    "        pos_key = f\"{row['chr']}:{row['pos']}\"\n",
    "        if any(abs(row['pos'] - int(pos.split(':')[1])) < 500000 \n",
    "               for pos in processed_positions \n",
    "               if pos.split(':')[0] == str(row['chr'])):\n",
    "            continue\n",
    "            \n",
    "        processed_positions.add(pos_key)\n",
    "        \n",
    "        if row['known'] != 'None':\n",
    "            consolidated['known'].append((row['snpid'], row['known'], row['p']))\n",
    "        else:\n",
    "            consolidated['novel'].append((row['snpid'], row['p']))\n",
    "    \n",
    "    return consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_regional_plot_snps(consolidated_leads, results, trait, ancestry, known_loci_list):\n",
    "    \"\"\"\n",
    "    Prepare SNPs for regional plotting with proper annotation handling.\n",
    "    \n",
    "    Returns:\n",
    "        list: [(snp, ancestry, anno_alias)]\n",
    "    \"\"\"\n",
    "    plot_snps = []\n",
    "    \n",
    "    # Get all lead SNPs from all conditions for reference\n",
    "    all_trait_snps = set()\n",
    "    for ancestry, data in results[trait].items():\n",
    "        if 'lead_snps' in data:\n",
    "            all_trait_snps.update(data['lead_snps']['SNPID'].tolist())\n",
    "    \n",
    "    # Process known loci\n",
    "    for snp, known_locus, p_val in consolidated_leads['known']:\n",
    "        # Extract chr:pos from known_locus (format: 'CHR:POS-')\n",
    "        try:\n",
    "            known_chr_pos = known_locus.split('-')[0]  # Gets 'CHR:POS'\n",
    "        except (ValueError, IndexError):\n",
    "            print(f\"Warning: Could not parse known_locus format: {known_locus}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract chr:pos from snp (format: 'CHR-POS-REF-ALT')\n",
    "        try:\n",
    "            snp_parts = snp.split('-')\n",
    "            snp_chr_pos = f\"{snp_parts[0]}:{snp_parts[1]}\"  # Convert to 'CHR:POS'\n",
    "        except (ValueError, IndexError):\n",
    "            print(f\"Warning: Could not parse SNP format: {snp}\")\n",
    "            continue\n",
    "        \n",
    "        if snp_chr_pos == known_chr_pos:\n",
    "            # SNP is at same chr:pos as known locus\n",
    "            anno_alias = {snp: f'  {snp}\\n  Reported: {known_locus}'}\n",
    "        else:\n",
    "            # Find closest SNP in dataset to the known locus\n",
    "            closest_snp = find_closest_snp_in_dataset(known_locus, all_trait_snps)\n",
    "            if closest_snp:\n",
    "                anno_alias = {closest_snp: f'  {closest_snp}\\n  Reported: {known_locus}'}\n",
    "            else:\n",
    "                print(f\"Warning: No close SNP found for known locus {known_locus}\")\n",
    "                continue\n",
    "                \n",
    "        plot_snps.append((snp, 'reported', anno_alias, p_val))\n",
    "    \n",
    "    # Process novel loci  \n",
    "    for snp, p_val in consolidated_leads['novel']:\n",
    "        anno_alias = {snp: f'  Unreported: {snp}'}\n",
    "        plot_snps.append((snp, 'unreported', anno_alias, p_val))\n",
    "    \n",
    "    return plot_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_snp_in_dataset(known_locus, dataset_snps, max_distance_kb=500):\n",
    "    \"\"\"\n",
    "    Find the closest SNP in dataset to a known locus.\n",
    "    \n",
    "    Args:\n",
    "        known_locus: str in format 'CHR:POS-ALT'\n",
    "        dataset_snps: set of SNPs in format 'CHR-POS-REF-ALT'\n",
    "        max_distance_kb: maximum distance in kb\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse known locus\n",
    "        if '-' in known_locus:\n",
    "            chr_pos = known_locus.split('-')[0]\n",
    "        else:\n",
    "            chr_pos = known_locus\n",
    "        target_chr, target_pos = chr_pos.split(':')\n",
    "        target_chr = int(target_chr)\n",
    "        target_pos = int(target_pos)\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "    \n",
    "    closest_snp = None\n",
    "    min_distance = float('inf')\n",
    "    max_distance_bp = max_distance_kb * 1000\n",
    "    \n",
    "    # Parse dataset SNPs and find closest\n",
    "    for snp in dataset_snps:\n",
    "        match = re.match(r'^(\\d+|X|Y)-(\\d+)-([ATCG])-([ATCG])$', snp)\n",
    "        if not match:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            snp_chr = int(match.group(1))\n",
    "            snp_pos = int(match.group(2))\n",
    "        except ValueError:\n",
    "            continue  # Skip X, Y chromosomes\n",
    "            \n",
    "        if snp_chr != target_chr:\n",
    "            continue\n",
    "            \n",
    "        distance = abs(snp_pos - target_pos)\n",
    "        if distance < min_distance and distance <= max_distance_bp:\n",
    "            min_distance = distance\n",
    "            closest_snp = snp\n",
    "    \n",
    "    return closest_snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_regional_plots(results, trait, ancestries, trait_sumstats, plot_titles, ref_populations):\n",
    "    \"\"\"\n",
    "    Create stacked regional plots for consolidated lead SNPs.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating consolidated regional plots\")\n",
    "    \n",
    "    # Consolidate lead SNPs across conditions\n",
    "    consolidated = consolidate_lead_snps_across_ancestries(results, trait, ancestries)\n",
    "        \n",
    "    if not consolidated['known'] and not consolidated['novel']:\n",
    "        print(f\"No lead SNPs found for {trait}\")\n",
    "        return\n",
    "        \n",
    "    # Get known loci for this trait\n",
    "    known_loci_list = known_loci[trait]\n",
    "    \n",
    "    # Prepare SNPs for plotting\n",
    "    plot_snps = prepare_regional_plot_snps(\n",
    "        consolidated, results, trait, ancestry, known_loci_list\n",
    "    )\n",
    "    \n",
    "    if not plot_snps:\n",
    "        print(f\"No valid SNPs for regional plotting in {trait}\")\n",
    "        return\n",
    "    \n",
    "    # Create regional plots\n",
    "    for plot_snp, plot_type, anno_alias, p_val in plot_snps:\n",
    "        try:\n",
    "            # Parse coordinates for region\n",
    "            match = re.match(r'^(\\d+|X|Y)-(\\d+)-([ATCG])-([ATCG])$', plot_snp)\n",
    "            if not match:\n",
    "                print(f\"Warning: Could not parse SNP format: {plot_snp}\")\n",
    "                continue\n",
    "\n",
    "            chrom = int(match.group(1))\n",
    "            pos = int(match.group(2))\n",
    "\n",
    "            print(f\"Plotting {plot_type} locus: {plot_snp}\")\n",
    "\n",
    "            all_paths = [gl.get_path(ref_populations.get(ancestry)) for ancestry in ancestries]\n",
    "            \n",
    "            # Calculate the maximum -log10(p) value\n",
    "            neg_log10_p = -np.log10(p_val)\n",
    "\n",
    "            # Round up to next multiple of 5, then add 5\n",
    "            max_y = (np.ceil(neg_log10_p / 5) * 5) + 5\n",
    "\n",
    "            # Create labels in groups of 5\n",
    "            ylabels = list(range(0, int(max_y) + 1, 5))\n",
    "\n",
    "            gl.plot_stacked_mqq(\n",
    "                objects=trait_sumstats,\n",
    "                vcfs=all_paths,\n",
    "                region=(chrom, pos - 250000, pos + 250000),\n",
    "                region_ref=[plot_snp],\n",
    "                build=\"38\",\n",
    "                mode=\"r\",\n",
    "                anno=True,\n",
    "                anno_set=[plot_snp],\n",
    "                anno_alias=anno_alias,\n",
    "                anno_style=\"tight\",\n",
    "                fontfamily='DejaVu Sans',\n",
    "                marker_size=(50, 50),\n",
    "                titles=plot_titles,\n",
    "                track_n=3,\n",
    "                track_fontsize_ratio=1.2,\n",
    "                ylabels=ylabels,\n",
    "                title_args={\"size\": 20},\n",
    "                anno_args={\"rotation\": 0},\n",
    "                fig_args={\"figsize\": (14, 14), \"dpi\": 300},\n",
    "                save_args={\"dpi\": 300, \"facecolor\": \"white\"},\n",
    "                save=f'regional_plots/{trait}_{plot_type}_{chrom}_{pos}_stacked_regional.png',\n",
    "                verbose=True,\n",
    "                check=False\n",
    "            )            \n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting regional for {plot_snp}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = f'{my_bucket}/saige_gwas/min_2' # no trailing '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits = ['condition__sarcoid']\n",
    "\n",
    "# Define disease-specific plotting parameters\n",
    "trait_plot_params = {\n",
    "    'condition__sarcoid': {'cut': None, 'cutfactor': None, 'ylabels': [2,4,6,8,10]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all conditions for all traits\n",
    "results = {}\n",
    "for trait in traits:\n",
    "    print(f\"\\n=== Processing trait: {trait} ===\")\n",
    "    results[trait] = {}\n",
    "    \n",
    "    # Get trait-specific parameters\n",
    "    plot_params = trait_plot_params.get(trait, {'ylabels': [2,8,14,20,50,80,100]})\n",
    "\n",
    "    trait_sumstats = []\n",
    "    plot_titles = []\n",
    "    all_trait_known = []\n",
    "    all_trait_new = []\n",
    "    \n",
    "    # Single Ancestries\n",
    "    for ancestry in ['eur', 'afr']:\n",
    "        try:\n",
    "            sumstats, lead_snps = process_gwas(\n",
    "                base_dir, ancestry, trait, known_loci[trait], plot_params\n",
    "            )\n",
    "            results[trait][ancestry] = {\n",
    "                'sumstats': sumstats,\n",
    "                'lead_snps': lead_snps\n",
    "            }\n",
    "            trait_sumstats.append(sumstats)\n",
    "            plot_titles.append(f\"{ancestry.upper()}\")\n",
    "\n",
    "            # Collect loci for stacked plot highlighting\n",
    "            ancestry_known_loci = lead_snps[lead_snps['KNOWN'] != 'None']['SNPID'].tolist()\n",
    "            all_trait_known.extend(ancestry_known_loci)\n",
    "            new_loci = lead_snps[lead_snps['KNOWN'] == 'None']['SNPID'].tolist()\n",
    "            all_trait_new.extend(new_loci)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {ancestry}: {e}\")\n",
    "            continue\n",
    "\n",
    "#     # METAL\n",
    "#     try:\n",
    "#         sumstats, lead_snps = process_gwas(\n",
    "#             base_dir, 'metal', trait, known_loci[trait], plot_params\n",
    "#         )\n",
    "#         results[trait]['metal'] = {\n",
    "#             'sumstats': sumstats,\n",
    "#             'lead_snps': lead_snps\n",
    "#         }\n",
    "#         trait_sumstats.append(sumstats)\n",
    "#         plot_titles.append(f\"Meta-analysis\")\n",
    "\n",
    "#         # Collect matches for stacked plot highlighting\n",
    "#         ancestry_known_loci = lead_snps[lead_snps['KNOWN'] != 'None']['SNPID'].tolist()\n",
    "#         all_trait_known.extend(ancestry_known_loci)\n",
    "#         new_loci = lead_snps[lead_snps['KNOWN'] == 'None']['SNPID'].tolist()\n",
    "#         all_trait_new.extend(new_loci)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR processing Meta-analysis: {e}\")\n",
    "#         continue\n",
    "\n",
    "    # Create stacked Manhattan plot for this trait (only if we have data)\n",
    "    if trait_sumstats:\n",
    "        try:\n",
    "            unique_all_trait_known = list(set(all_trait_known))\n",
    "            unique_all_trait_new = list(set(all_trait_new))\n",
    "            # Build kwargs dictionary for stacked plot\n",
    "            plot_kwargs = {\n",
    "                'objects': trait_sumstats,\n",
    "                'mode': 'm',\n",
    "                'build': \"38\", \n",
    "                'check': False,\n",
    "                'skip': 2,\n",
    "                'ylabels': plot_params['ylabels'],\n",
    "                'titles': plot_titles,\n",
    "                'title_args': {\"size\": 16},\n",
    "                'fontsize': 11,\n",
    "                'font_family': 'DejaVu Sans',\n",
    "                'marker_size': (10,10),\n",
    "                'verbose': False,\n",
    "                'fig_args': {\"figsize\":(16,12), \"dpi\":300},\n",
    "                'common_ylabel': True,\n",
    "                'save_args': {\"dpi\":300, \"facecolor\":\"white\"}\n",
    "            }\n",
    "            \n",
    "            # Only add cut and cutfactor if they're not None\n",
    "            if plot_params.get('cut') is not None:\n",
    "                plot_kwargs['cut'] = plot_params['cut']\n",
    "            if plot_params.get('cutfactor') is not None:\n",
    "                plot_kwargs['cutfactor'] = plot_params['cutfactor']\n",
    "\n",
    "            # Plot regular stacked plot\n",
    "            regular_kwargs = plot_kwargs.copy()\n",
    "            regular_kwargs['save'] = f'manhattan_plots/{trait}_min_2_stacked_m.png'\n",
    "\n",
    "            print(\"Plotting stacked mqq plots (verbose mode off)\")\n",
    "            gl.plot_stacked_mqq(**regular_kwargs)\n",
    "            \n",
    "            # Plot highlighted stacked plot\n",
    "            highlighted_kwargs = plot_kwargs.copy()\n",
    "            highlighted_kwargs.update({\n",
    "                'highlight_windowkb': 500,\n",
    "                'highlight': [unique_all_trait_known, unique_all_trait_new],\n",
    "                'highlight_color': ['#DC143C', '#FFA500'],\n",
    "                'save': f'manhattan_plots/{trait}_min_2_highlighted_stacked_m.png'\n",
    "            })\n",
    "            gl.plot_stacked_mqq(**highlighted_kwargs)\n",
    "            plt.close('all')  # Clear all figures\n",
    "            \n",
    "            print(f\"✓ Stacked plots saved for {trait}\")\n",
    "            \n",
    "#             # Create consolidated regional plots\n",
    "#             create_stacked_regional_plots(\n",
    "#                 results, trait, ['eur', 'afr', 'metal'], trait_sumstats, plot_titles, ref_populations\n",
    "#             )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR creating stacked plot for {trait}: {e}\")\n",
    "\n",
    "print(\"\\n=== Processing complete! ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {base_dir}/manhattan_plots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # move all plots from manhattan_plots to bucket metal folder\n",
    "# args = [\"gsutil\", \"cp\", \"-r\", \"./manhattan_plots/\", f\"{my_bucket}/saige_gwas/min_1/\"]\n",
    "# output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# # print output from gsutil\n",
    "# output.stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METAL N by SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_df = pl.read_csv(f'{base_dir}/eur/condition__sarcoid/gwas_results.tsv.gz',\n",
    "                     separator='\\t')\n",
    "afr_df = pl.read_csv(f'{base_dir}/afr/condition__sarcoid/gwas_results.tsv.gz',\n",
    "                     separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afr_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_df.filter(pl.col('vid')=='3-76477573-T-C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal_df = pl.read_csv('{bucket or my_bucket}/saige_gwas/min_1/metal/condition__sarcoid/condition__sarcoid_afr_eur1.tsv',\n",
    "                      separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate n_effective for both datasets\n",
    "eur_df = eur_df.with_columns([\n",
    "    (pl.col(\"n_cases\") + pl.col(\"n_controls\")).alias(\"n_effective\")\n",
    "])\n",
    "\n",
    "afr_df = afr_df.with_columns([\n",
    "    (pl.col(\"n_cases\") + pl.col(\"n_controls\")).alias(\"n_effective\")\n",
    "])\n",
    "\n",
    "# Determine common axis limits\n",
    "x_min = min(eur_df[\"standard_error\"].min(), afr_df[\"standard_error\"].min())\n",
    "x_max = max(eur_df[\"standard_error\"].max(), afr_df[\"standard_error\"].max())\n",
    "y_min = min(eur_df[\"n_effective\"].min(), afr_df[\"n_effective\"].min())\n",
    "y_max = max(eur_df[\"n_effective\"].max(), afr_df[\"n_effective\"].max())\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# EUR plot\n",
    "sns.scatterplot(\n",
    "    data=eur_df.to_pandas(), \n",
    "    x=\"standard_error\", \n",
    "    y=\"n_effective\", \n",
    "    alpha=0.6, \n",
    "    s=20,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_xlim(x_min, x_max)\n",
    "ax1.set_ylim(y_min, y_max)\n",
    "ax1.set_title(\"EUR\")\n",
    "ax1.set_xlabel(\"Standard Error\")\n",
    "ax1.set_ylabel(\"Effective Sample Size\")\n",
    "\n",
    "# AFR plot\n",
    "sns.scatterplot(\n",
    "    data=afr_df.to_pandas(), \n",
    "    x=\"standard_error\", \n",
    "    y=\"n_effective\", \n",
    "    alpha=0.6, \n",
    "    s=20,\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_xlim(x_min, x_max)\n",
    "ax2.set_ylim(y_min, y_max)\n",
    "ax2.set_title(\"AFR\")\n",
    "ax2.set_xlabel(\"Standard Error\")\n",
    "ax2.set_ylabel(\"Effective Sample Size\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "347px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
