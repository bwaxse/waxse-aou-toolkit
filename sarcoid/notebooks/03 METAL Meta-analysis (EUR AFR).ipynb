{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METAL\n",
    "https://github.com/bwaxse/metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from google.cloud import storage\n",
    "from IPython.display import display, HTML\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import re\n",
    "import io\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_gwas_results(my_bucket, in_dir, trait_type, output_path):\n",
    "    \"\"\"\n",
    "    Combine chromosome-wise GWAS results from GCS.\n",
    "    \n",
    "    Parameters:\n",
    "    my_bucket: GCS bucket (with or without gs:// prefix)\n",
    "    in_dir: '{output_folder}//{anc}/{trait}/gwas'\n",
    "    trait_type: 'quantitative' or 'binary'\n",
    "    output_path: '{output_folder}/{anc}/{trait}/gwas_results.tsv.gz'\n",
    "    \"\"\"\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(my_bucket.removeprefix('gs://'))\n",
    "    \n",
    "    # Column mapping\n",
    "    col_dict = {\n",
    "        'CHR': 'chromosome',\n",
    "        'POS': 'base_pair_location', \n",
    "        'MarkerID': 'vid',\n",
    "        'Allele1': 'other_allele',\n",
    "        'Allele2': 'effect_allele',\n",
    "        'AF_Allele2': 'effect_allele_frequency',\n",
    "        'BETA': 'beta',\n",
    "        'SE': 'standard_error',\n",
    "        'Tstat': 'test_statistic_value',\n",
    "        'p.value': 'p_value'\n",
    "    }\n",
    "    \n",
    "    if trait_type == 'quantitative':\n",
    "        col_dict['N'] = 'n'\n",
    "    else:\n",
    "        col_dict['p.value.NA'] = 'p_value__nospa'\n",
    "        col_dict['Is.SPA'] = 'spa_converged'\n",
    "        col_dict['AF_case'] = 'effect_allele_frequency__cases'\n",
    "        col_dict['AF_ctrl'] = 'effect_allele_frequency__control'\n",
    "        col_dict['N_case'] = 'n_cases'\n",
    "        col_dict['N_ctrl'] = 'n_controls'\n",
    "        col_dict['N_case_hom'] = 'n_case__alt_homs'\n",
    "        col_dict['N_case_het'] = 'n_case__hets'\n",
    "        col_dict['N_ctrl_hom'] = 'n_controls__alt_homs'\n",
    "        col_dict['N_ctrl_het'] = 'n_controls__hets'\n",
    "    \n",
    "    # Column order\n",
    "    col_order = [\n",
    "        'chromosome', 'base_pair_location', 'vid', 'rsID', 'other_allele', 'effect_allele',\n",
    "        'effect_allele_frequency', 'r2', 'beta', 'standard_error', 'test_statistic_value',\n",
    "        'p_value', 'p_value__nospa', 'spa_converged',\n",
    "        'effect_allele_frequency__cases', 'effect_allele_frequency__control',\n",
    "        'n', 'n_cases', 'n_controls', 'n_case__alt_homs', 'n_case__hets', 'n_controls__alt_homs', 'n_controls__hets'\n",
    "    ]\n",
    "    \n",
    "    # Combine chromosome files\n",
    "    combined_dfs = []\n",
    "    \n",
    "    for chrom in range(1, 23): \n",
    "        blob_path = f'{in_dir}/gwas_results_chr{chrom}.txt'\n",
    "        blob = bucket.blob(blob_path)\n",
    "        \n",
    "        if not blob.exists():\n",
    "            print(f\"Skipping chromosome {chrom}: file not found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing chromosome {chrom}...\")\n",
    "        \n",
    "        # Download and read\n",
    "        data = blob.download_as_text()\n",
    "        chrom_df = pl.read_csv(io.StringIO(data), separator='\\t')\n",
    "        combined_dfs.append(chrom_df)\n",
    "    \n",
    "    if not combined_dfs:\n",
    "        raise FileNotFoundError(f\"No chromosome files found in {my_bucket}/{in_dir}\")\n",
    "    \n",
    "    # Combine all chromosomes\n",
    "    print(\"Combining chromosomes...\")\n",
    "    df = pl.concat(combined_dfs)\n",
    "    \n",
    "    # Rename columns\n",
    "    df = df.rename(col_dict)\n",
    "    \n",
    "    # Construct variant_id if it's missing/empty (like in your other function)\n",
    "    vid_missing_count = df.filter(pl.col('vid')=='.').height\n",
    "    if vid_missing_count > 0:\n",
    "        print(f\"Found {vid_missing_count} missing vid values, constructing from components when missing...\")\n",
    "        df = df.with_columns(\n",
    "            pl.when(pl.col('vid')=='.')\n",
    "            .then(\n",
    "                pl.concat_str([\n",
    "                    pl.col('chromosome').cast(pl.Utf8),\n",
    "                    pl.col('base_pair_location').cast(pl.Utf8),\n",
    "                    pl.col('other_allele'),\n",
    "                    pl.col('effect_allele')\n",
    "                ], separator='-')\n",
    "            )\n",
    "            .otherwise(pl.col('vid'))\n",
    "            .alias('vid')\n",
    "        )\n",
    "    \n",
    "    # Select and reorder columns\n",
    "    available_cols = [col for col in col_order if col in df.columns]\n",
    "    df = df.select(available_cols)\n",
    "    \n",
    "    # Save to GCS\n",
    "    output_blob = bucket.blob(output_path)\n",
    "    \n",
    "    output_buffer = io.BytesIO()\n",
    "    df.write_csv(output_buffer, separator='\\t')\n",
    "    output_buffer.seek(0)\n",
    "    compressed_output = gzip.compress(output_buffer.getvalue())\n",
    "    output_blob.upload_from_string(compressed_output, content_type='application/gzip')\n",
    "    \n",
    "    print(f\"Combined file saved to: {my_bucket}/{output_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(query_dir):\n",
    "    tmp = subprocess.run(\n",
    "        f'gsutil ls {query_dir}',\n",
    "        shell=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    files = tmp.stdout.decode('utf-8').split('\\n')\n",
    "    return(files)\n",
    "\n",
    "def gcs_file_exists(gs_path):\n",
    "    \"\"\"Check if specific GCS file exists using gsutil ls\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(f'gsutil ls {gs_path}', \n",
    "                              shell=True, capture_output=True)\n",
    "        return result.returncode == 0\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_saige_inputs(trait, ancestries, base_output_folder):\n",
    "    \"\"\"\n",
    "    Validate SAIGE output files for METAL meta-analysis\n",
    "    \"\"\"\n",
    "    required_file = \"gwas_results.tsv.gz\"\n",
    "    validated_inputs = {}\n",
    "    \n",
    "    print(f\"Validating inputs for {trait}\")\n",
    "    \n",
    "    for anc in ancestries:\n",
    "        in_dir = f\"{base_output_folder}/{anc}/{trait}\"\n",
    "        file_path = f\"{in_dir}/{required_file}\"\n",
    "    \n",
    "        # Check file exists\n",
    "        if not gcs_file_exists(file_path):\n",
    "            print(f\"Missing {required_file} for ancestry {anc}\")\n",
    "            continue\n",
    "            \n",
    "        # Extract sample sizes from the file\n",
    "        sample_info = extract_sample_sizes(file_path)\n",
    "        if not sample_info:\n",
    "            print(f\"Could not extract sample sizes for ancestry {anc}\")\n",
    "            continue\n",
    "\n",
    "        validated_inputs[anc] = {\n",
    "            'path': file_path,\n",
    "            'n_cases': sample_info['n_cases'],\n",
    "            'n_controls': sample_info['n_controls'], \n",
    "            'n_total': sample_info['n_total']\n",
    "        }\n",
    "        \n",
    "        print(f\"Ancestry {anc}: {sample_info['n_cases']} cases, {sample_info['n_controls']} controls (total: {sample_info['n_total']})\")\n",
    "    \n",
    "    if len(validated_inputs) < 2:\n",
    "        print(f\"Error: Need at least 2 ancestries for meta-analysis. Found: {len(validated_inputs)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"{len(validated_inputs)} validated studies available for {trait}\")    \n",
    "    return validated_inputs\n",
    "\n",
    "def extract_sample_sizes(file_path):\n",
    "    \"\"\"\n",
    "    Extract n_cases and n_controls from first data row of SAIGE file\n",
    "    Returns dict with sample size info or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read just the first few lines to get sample sizes\n",
    "        cmd = f\"gsutil cat '{file_path}' | gunzip | head -2 2>/dev/null\"\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if not result.stdout:\n",
    "            print(f\"No output from command for {file_path}\")\n",
    "            return None\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "\n",
    "        if len(lines) < 2:\n",
    "            print(f\"File {file_path} has insufficient data\")\n",
    "            return None\n",
    "            \n",
    "        header = lines[0].split('\\t')\n",
    "        data = lines[1].split('\\t')\n",
    "        \n",
    "        # Find column indices\n",
    "        try:\n",
    "            n_cases_idx = header.index('n_cases')\n",
    "            n_controls_idx = header.index('n_controls')\n",
    "        except ValueError as e:\n",
    "            print(f\"Missing required columns in {file_path}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract values\n",
    "        n_cases = int(float(data[n_cases_idx]))\n",
    "        n_controls = int(float(data[n_controls_idx]))\n",
    "        n_total = n_cases + n_controls\n",
    "        \n",
    "        return {\n",
    "            'n_cases': n_cases,\n",
    "            'n_controls': n_controls,\n",
    "            'n_total': n_total\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting sample sizes from {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsub_script(\n",
    "    label,\n",
    "    machine_type,\n",
    "    envs,\n",
    "    in_params,\n",
    "    out_params,\n",
    "    out_dirs,\n",
    "    boot_disk = 100,\n",
    "    disk_size = 150,\n",
    "    image = 'us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "    script = 'run_metal.sh',\n",
    "    preemptible = True\n",
    "):\n",
    "    \n",
    "    # get useful info\n",
    "    dsub_user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0].replace('.','-')\n",
    "\n",
    "    job_name = f'{label}'\n",
    "    \n",
    "    dsub_cmd = 'dsub '\n",
    "    dsub_cmd += '--provider google-batch '\n",
    "    dsub_cmd += '--user-project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--image \"{}\" '.format(image)\n",
    "    dsub_cmd += '--network \"global/networks/network\" '\n",
    "    dsub_cmd += '--subnetwork \"regions/us-central1/subnetworks/subnetwork\" '\n",
    "    dsub_cmd += '--service-account \"$(gcloud config get-value account)\" '\n",
    "    dsub_cmd += '--use-private-address '\n",
    "    dsub_cmd += '--user \"{}\" '.format(dsub_user_name)\n",
    "    dsub_cmd += '--regions us-central1 '\n",
    "    dsub_cmd += '--logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +\\'%Y%m%d\\')/{job-id}-{task-id}-{task-attempt}.log\" '\n",
    "    dsub_cmd += ' \"$@\" '\n",
    "    dsub_cmd += '--name \"{}\" '.format(job_name)\n",
    "    dsub_cmd += '--machine-type \"{}\" '.format(machine_type)\n",
    "    \n",
    "    if preemptible:\n",
    "        dsub_cmd += '--preemptible '\n",
    "        \n",
    "    if 'c4' in machine_type:\n",
    "        raise ValueError(\n",
    "            f\"c4 machine types ('{machine_type}') are not supported with dsub. \"\n",
    "            f\"c4 requires hyperdisk-balanced boot disks, but dsub doesn't allow \"\n",
    "            f\"setting boot disks. Use c2 or n2 instead.\"\n",
    "        )\n",
    "\n",
    "#        # c4 doesn't use pd-ssd\n",
    "#         dsub_cmd += '--disk-type \"hyperdisk-balanced\" '\n",
    "#     else:\n",
    "#         dsub_cmd += '--disk-type \"pd-ssd\" '\n",
    "        \n",
    "    dsub_cmd += '--boot-disk-size {} '.format(boot_disk)\n",
    "    dsub_cmd += '--disk-size {} '.format(disk_size)\n",
    "    dsub_cmd += '--script \"{}\" '.format(script)\n",
    "    \n",
    "    # Assign any environmental conditions\n",
    "    for env_key in envs.keys():\n",
    "        dsub_cmd += '--env {}=\"{}\" '.format(env_key, envs[env_key])\n",
    "        \n",
    "    # Assign any inputs\n",
    "    for in_key in in_params.keys():\n",
    "        dsub_cmd += '--input {}=\"{}\" '.format(in_key, in_params[in_key])\n",
    "        \n",
    "    # Assign any outputs\n",
    "    if out_params != None:\n",
    "        for out_key in out_params.keys():\n",
    "            dsub_cmd += '--output {}=\"{}\" '.format(out_key, out_params[out_key])\n",
    "        \n",
    "    for out_key in out_dirs.keys():\n",
    "        dsub_cmd += '--output-recursive {}=\"{}\" '.format(out_key, out_dirs[out_key])\n",
    "\n",
    "    os.system(dsub_cmd)\n",
    "#     print_dsub_readable(dsub_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_age_format(age: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate age format for dsub dstat command\n",
    "    \"\"\"\n",
    "    # Pattern: one or more digits followed by exactly one valid unit\n",
    "    pattern = r'^\\d+[smhdw]$'\n",
    "    return bool(re.match(pattern, age.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dsub_status(user: str = None, full: bool = False, age: str = '1d') -> subprocess.CompletedProcess:\n",
    "    \"\"\"\n",
    "    Check status of dsub jobs for the specified user\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user : str, optional\n",
    "        Username to check jobs for. Defaults to current user from OWNER_EMAIL\n",
    "    full : bool, default False\n",
    "        Include full job details in output\n",
    "    age : str, default '1d'\n",
    "        Maximum age of jobs to display. Format: <integer><unit>\n",
    "        Units: s (seconds), m (minutes), h (hours), d (days), w (weeks)\n",
    "        Examples: '3d', '12h', '30m', '7w'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    subprocess.CompletedProcess\n",
    "        Result of the dstat command\n",
    "        \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> check_dsub_status(age='3d', full=True)  # Last 3 days, full details\n",
    "    >>> check_dsub_status()  # Default: last day, summary view\n",
    "    \"\"\"\n",
    "    \n",
    "    if user is None:\n",
    "        # Get current user if not specified\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Validate age parameter\n",
    "    if age is not None:\n",
    "        if not validate_age_format(age):\n",
    "            raise ValueError(\n",
    "                f\"Invalid age format: '{age}'. \"\n",
    "                \"Expected format: <integer><unit> where unit is one of: s, m, h, d, w. \"\n",
    "                \"Examples: '3d', '12h', '30m', '7w'\"\n",
    "            )\n",
    "    \n",
    "    # Build command\n",
    "    cmd_parts = [\n",
    "        \"dstat\",\n",
    "        \"--provider google-batch\",\n",
    "        f\"--user {user}\",\n",
    "        \"--status '*'\",\n",
    "        f\"--project {project}\"\n",
    "    ]\n",
    "    \n",
    "    if full:\n",
    "        cmd_parts.append(\"--full\")\n",
    "    \n",
    "    if age:\n",
    "        cmd_parts.append(f\"--age {age}\")\n",
    "    \n",
    "    cmd = \" \".join(cmd_parts)\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_running_jobs():\n",
    "    \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Cancel only running jobs\n",
    "    cancel_cmd = f\"ddel --provider google-batch --project {project} --users 'bwaxse' --jobs '*'\"\n",
    "    print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "    \n",
    "    return subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_details(user=None, job=None):\n",
    "    \"\"\"List all jobs for the user, including failed ones\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    if user is None:\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "        \n",
    "    if job is None:\n",
    "        job = \"'*' \"\n",
    "    else:\n",
    "        job = f'--jobs {job} '\n",
    "    \n",
    "    cmd = f\"dstat --provider google-batch --project {project} --user {user} --status {job}--full\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_dsub_logs(log_path):\n",
    "    base_path = log_path.replace('.log', '')\n",
    "    \n",
    "    print(\"=== STDOUT ===\")\n",
    "    subprocess.run(['gsutil', 'cat', f'{base_path}-stdout.log'])\n",
    "    \n",
    "    print(\"\\n=== STDERR ===\") \n",
    "    subprocess.run(['gsutil', 'cat', f'{base_path}-stderr.log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dsub_readable(cmd):\n",
    "    \"\"\"\n",
    "    Simple readable format - newline before each --\n",
    "    \"\"\"\n",
    "    readable_cmd = cmd.replace(' --', ' \\\\\\n    --')\n",
    "    print(readable_cmd)\n",
    "    print()  # Extra line for separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge GWAS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries_considered = ['afr', 'eur'] \n",
    "traits = {\n",
    "    'condition__sarcoid': 'binary',\n",
    "}\n",
    "\n",
    "init_combine = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output folders\n",
    "output_folder = f'{my_bucket}/saige_gwas/min_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if init_combine:\n",
    "    for anc in ['eur', 'afr']:\n",
    "        print(f'combining {anc} files')\n",
    "        for trait in traits.keys():\n",
    "            rez = combine_gwas_results(\n",
    "                my_bucket=my_bucket,\n",
    "                in_dir=f'saige_gwas/min_1//{anc}/{trait}/gwas',\n",
    "                trait_type=traits[trait],\n",
    "                output_path=f'saige_gwas/min_1/{anc}/{trait}/gwas_results.tsv.gz'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pl.read_csv(f'{output_folder}/afr/condition__sarcoid/gwas_results.tsv.gz', separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {output_folder}/afr/condition__sarcoid/gwas_results.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_csv('{bucket or my_bucket}/saige_gwas/min_1/afr/condition__sarcoid/gwas_results.tsv.gz',\n",
    "                     separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run METAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%writefile run_metal.sh\n",
    "\n",
    "#!/bin/bash\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"Starting METAL meta-analysis for ${TRAIT}\"\n",
    "echo \"Number of input files: ${N_FILES}\"\n",
    "\n",
    "# Parse ancestries\n",
    "IFS=',' read -ra INPUTS <<< \"${INPUTS}\"\n",
    "\n",
    "# Parse sample sizes\n",
    "IFS=',' read -ra SIZES <<< \"${SAMPLE_SIZES}\"\n",
    "\n",
    "echo \"Preprocessing files to add ancestry-specific n_total columns...\"\n",
    "PROCESSED_FILES=()\n",
    "\n",
    "# Pre-processing adds sample size via n_cases and n_controls extracted from SAIGE GWAS results \n",
    "for i in \"${!INPUTS[@]}\"; do\n",
    "    var_name=\"${INPUTS[$i]}\"\n",
    "    input_file=\"${!var_name}\"\n",
    "    n_total=\"${SIZES[$i]}\"\n",
    "    processed_file=\"/tmp/processed_file_${i}.tsv\"\n",
    "    \n",
    "    echo \"Processing file $((i+1)): ${input_file} (n_total=${n_total})\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if [[ ! -f \"${input_file}\" ]]; then\n",
    "        echo \"ERROR: Input file not found: ${input_file}\"\n",
    "        echo \"Available files in /mnt/data/input/:\"\n",
    "        ls -la /mnt/data/input/ || echo \"Input directory not accessible\"\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    # Add constant n_total column for this ancestry\n",
    "    if [[ \"${input_file}\" == *.gz ]]; then\n",
    "        gzip -dc \"${input_file}\"\n",
    "    else\n",
    "        cat \"${input_file}\"\n",
    "    fi | awk -F'\\t' -v OFS='\\t' -v n_total=\"${n_total}\" '\n",
    "        NR==1 { print $0, \"n_total\"; next }\n",
    "               { print $0, n_total }' > \"${processed_file}\"\n",
    "    \n",
    "    PROCESSED_FILES+=(\"/tmp/processed_file_${i}.tsv\")\n",
    "    \n",
    "    # Verify output\n",
    "    n_lines=$(wc -l < \"${processed_file}\")\n",
    "    echo \"  Created ${processed_file} with ${n_lines} lines\"\n",
    "done\n",
    "\n",
    "# Convert processed files array to comma-separated string\n",
    "PROCESSED_FILES_STR=$(IFS=','; echo \"${PROCESSED_FILES[*]}\")\n",
    "echo \"Running METAL with processed files: ${PROCESSED_FILES_STR}\"\n",
    "\n",
    "# Create a METAL script dynamically\n",
    "METAL_SCRIPT=\"${OUTPUT_PATH}/metal_script.txt\"\n",
    "cat > \"${METAL_SCRIPT}\" <<EOF\n",
    "MARKER vid\n",
    "WEIGHT n_total\n",
    "ALLELE effect_allele other_allele\n",
    "FREQ effect_allele_frequency\n",
    "PVAL p_value\n",
    "EFFECT beta\n",
    "STDERR standard_error\n",
    "SEPARATOR TAB\n",
    "SCHEME STDERR\n",
    "\n",
    "CUSTOMVARIABLE CasesSampleSize     #This is the col header for n_cases in each of the meta files, same for controls and total\n",
    "LABEL CasesSampleSize as N_cases      # same here, just changing the label in the METAL output, totally optionjal\n",
    " \n",
    "CUSTOMVARIABLE ConrolSampleSize\n",
    "LABEL ControlsSampleSize as N_controls\n",
    " \n",
    "CUSTOMVARIABLE TotalSampleSize\n",
    "LABEL TotalSampleSize as N_total\n",
    "\n",
    "# Auto-flip alleles based on frequency\n",
    "AVERAGEFREQ ON\n",
    "MINMAXFREQ ON\n",
    "\n",
    "$(for file in \"${PROCESSED_FILES[@]}\"; do echo \"PROCESS $file\"; done)\n",
    "\n",
    "OUTFILE ${OUT_PREF} .tsv\n",
    "ANALYZE HETEROGENEITY\n",
    "QUIT\n",
    "EOF\n",
    "\n",
    "echo \"Metal script created\"\n",
    "\n",
    "# Run METAL\n",
    "metal \"${METAL_SCRIPT}\"\n",
    "\n",
    "mv ${OUT_PREF}1.tsv \"${OUTPUT_PATH}/\"\n",
    "mv ${OUT_PREF}1.tsv.info \"${OUTPUT_PATH}/\"\n",
    "\n",
    "# List all output files\n",
    "echo \"Final contents of OUTPUT_PATH (${OUTPUT_PATH}):\"\n",
    "ls -lh \"${OUTPUT_PATH}\" || echo \"OUTPUT_PATH not accessible\"\n",
    "\n",
    "echo \"METAL analysis completed successfully for ${TRAIT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metal(trait, ancestries, base_output_folder):\n",
    "    \"\"\"\n",
    "    Run METAL meta-analysis with SAIGE-formatted inputs\n",
    "    \"\"\"\n",
    "    artifact_registry = os.getenv('ARTIFACT_REGISTRY_DOCKER_REPO', '')\n",
    "            \n",
    "    # Validate SAIGE inputs\n",
    "    validated_inputs = validate_saige_inputs(trait, ancestries, base_output_folder)\n",
    "    if not validated_inputs:\n",
    "        return None\n",
    "\n",
    "    out_dir = f'{output_folder}/metal/{trait}'\n",
    "    existing_files = set(x.split('/')[-1] for x in get_file_list(out_dir))\n",
    "\n",
    "    # Create ancestry suffix from validated inputs\n",
    "    ancestry_suffix = '_'.join(sorted(validated_inputs.keys()))\n",
    "\n",
    "    result_file = f'{trait}_{ancestry_suffix}1.tsv'\n",
    "    results_exist = {result_file}.issubset(existing_files) # True if result_file is in existing_files\n",
    "\n",
    "    if not results_exist:\n",
    "        # METAL configuration\n",
    "        env_dict = {\n",
    "            'TRAIT': trait,\n",
    "            'OUT_PREF': f'{trait}_{ancestry_suffix}',\n",
    "        }\n",
    "\n",
    "        in_dict = {}\n",
    "        file_list = []\n",
    "        sample_sizes = []\n",
    "\n",
    "        for i, (anc, file_info) in enumerate(validated_inputs.items(), 1):\n",
    "            in_dict[f'INPUT_{anc.upper()}'] = file_info['path']\n",
    "            file_list.append(file_info['path'])\n",
    "            sample_sizes.append(str(file_info['n_total']))\n",
    "\n",
    "        # Pass file list and sample sizes as comma-separated strings\n",
    "        env_dict['INPUTS'] = \",\".join(in_dict.keys())\n",
    "        env_dict['SAMPLE_SIZES'] = ','.join(sample_sizes)  # n_total per ancestry\n",
    "        env_dict['N_FILES'] = str(len(file_list))\n",
    "\n",
    "        out_dirs = {\n",
    "            'OUTPUT_PATH': out_dir\n",
    "        }\n",
    "\n",
    "        dsub_script(\n",
    "            label=f'metal_{trait}',\n",
    "            machine_type='n2d-standard-8',\n",
    "            envs=env_dict,\n",
    "            in_params=in_dict,\n",
    "            out_params=None,\n",
    "            out_dirs=out_dirs,\n",
    "            boot_disk=100,\n",
    "            disk_size=150,\n",
    "            image=f'{artifact_registry}/bwaxse/metal',\n",
    "            script='run_metal.sh',\n",
    "            preemptible=True\n",
    "        )\n",
    "    else:\n",
    "        print(f'skipped METAL meta-analysis for {trait}_{ancestry_suffix}\\n Results exist in {out_dir}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run METAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run METAL\n",
    "_ = run_metal(\n",
    "    trait='condition__sarcoid',\n",
    "    ancestries=['afr', 'eur'],\n",
    "    base_output_folder=output_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsub_status(full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details(job='metal-cond--bwaxse--250828-150146-78')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=\"{bucket or my_bucket}/dsub/logs/metal-condition--sarcoid/bwaxse/20250828/metal-cond--bwaxse--250828-150146-78-task-None.log\"\n",
    "view_dsub_logs(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {output_folder}/metal/condition__sarcoid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel_running_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "# project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "\n",
    "# # Cancel only running jobs\n",
    "# cancel_cmd = f\"ddel --provider google-batch --project {project} --users 'bwaxse' --jobs 'metal-alle--bwaxse--250825-193010-56'\"\n",
    "# print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "\n",
    "# subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "175.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
