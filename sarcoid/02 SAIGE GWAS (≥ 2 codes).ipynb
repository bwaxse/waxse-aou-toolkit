{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "import polars as pl\n",
    "import scipy\n",
    "import numpy as np\n",
    "import copy\n",
    "import re\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "src_bucket = '{bucket or my_bucket}' # ancestry-specific PCs and biallelic GTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries_considered = ['eur', 'afr', 'amr', 'eas', 'sas']\n",
    "\n",
    "# GWAS info\n",
    "traits = {\n",
    "    'condition__sarcoid': 'binary'\n",
    "}\n",
    "# Ancestry-specific\n",
    "covarColList = ['sex_binary', 'end_of_study_age'] + ['ancPC{}'.format(str(x)) for x in range(1, 21)]\n",
    "qCovarColList = ['sex_binary'] # Categorical covariates to be used in the null model. \n",
    "    # All categorical covariates listed in qCovarCol must be also in covarColList,  e,g c(\"Sex\"). \n",
    "sex_qCovarColList = [] # when running sex-specific GWAS, sex_binary must be removed from this list \n",
    "    \n",
    "# Transancestry\n",
    "all_covarColList = ['sex_binary', 'end_of_study_age'] + ['PC{}'.format(str(x)) for x in range(1, 17)]\n",
    "all_qCovarColList = ['sex_binary'] # Categorical covariates to be used in the null model.    \n",
    "    # All categorical covariates listed in qCovarCol must be also in covarColList,  e,g c(\"Sex\"). \n",
    "sex_all_qCovarColList = [] # when running sex-specific GWAS, sex_binary must be removed from this list\n",
    "\n",
    "# Columns to manipulate\n",
    "covariates_binarize = ['imputed_sex::F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output folders\n",
    "output_folder = f'{my_bucket}/saige_gwas/min_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -u $GOOGLE_PROJECT -m cp gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/qc/genomic_metrics.tsv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df = pd.read_csv(f'genomic_metrics.tsv', \n",
    "                     sep='\\t',\n",
    "                     dtype={'research_id': str},\n",
    "                     usecols=['research_id', 'sex_at_birth', 'dragen_sex_ploidy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df = pd.read_parquet(f'{src_bucket}/data/ancestry_specific_pcs.parquet').astype({'research_id': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcoid_df = pd.read_csv(f'{my_bucket}/data/cohorts/all_sarcoid_cases.csv',\n",
    "                     dtype={'person_id': str})\n",
    "sarcoid_df.groupby('case', as_index = False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcoid_without_exclusions_df = pd.read_csv(f'{my_bucket}/data/cohorts/sarcoid_cases_without_exclusions.csv',\n",
    "                     dtype={'person_id': str})\n",
    "sarcoid_without_exclusions_df.groupby('case', as_index = False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcoid_1_rows = sarcoid_without_exclusions_df[sarcoid_without_exclusions_df['sarcoid_n']==1]\n",
    "if not sarcoid_1_rows.empty:\n",
    "    print(f\"FYI, found {len(sarcoid_1_rows)} people with sarcoid_n==1 (i.e. this is sarcoid_n ≥ 1)\")\n",
    "else:\n",
    "    print(\"No people found with sarcoid_n==1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Sarcoid Cases with Exclusions From Controls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove person_ids from sarcoid_without_exclusions_df that have case == 1 in sarcoid_df (i.e. with exclusions)\n",
    "# these sarcoid cases had other diagnoses similar to sarcoid and should be removed\n",
    "\n",
    "# Get person_ids that are case == 1 in sarcoid_df\n",
    "case_1_in_sarcoid = set(sarcoid_df[sarcoid_df['case'] == 1]['person_id'])\n",
    "\n",
    "# Get person_ids that are case == 1 in sarcoid_without_exclusions_df  \n",
    "case_1_in_without_exclusions = set(sarcoid_without_exclusions_df[sarcoid_without_exclusions_df['case'] == 1]['person_id'])\n",
    "\n",
    "# Find person_ids that are case == 1 in sarcoid_df but NOT case == 1 in sarcoid_without_exclusions_df\n",
    "ids_to_remove = case_1_in_sarcoid - case_1_in_without_exclusions\n",
    "\n",
    "# Filter out these person_ids from sarcoid_without_exclusions_df\n",
    "filtered_sarcoid_df = sarcoid_without_exclusions_df[~sarcoid_without_exclusions_df['person_id'].isin(ids_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(query_dir):\n",
    "    tmp = subprocess.run(\n",
    "        f'gsutil ls {query_dir}',\n",
    "        shell=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    files = tmp.stdout.decode('utf-8').split('\\n')\n",
    "    return(files)\n",
    "\n",
    "def gcs_file_exists(gs_path):\n",
    "    \"\"\"Check if specific GCS file exists using gsutil ls\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(f'gsutil ls {gs_path}', \n",
    "                              shell=True, capture_output=True)\n",
    "        return result.returncode == 0\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def dsub_script(\n",
    "    label,\n",
    "    machine_type,\n",
    "    envs,\n",
    "    in_params,\n",
    "    out_params,\n",
    "    boot_disk = 100,\n",
    "    disk_size = 150,\n",
    "    image = 'us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "    script = 'run_saige_null_model.sh',\n",
    "    preemptible = True\n",
    "):\n",
    "    \n",
    "    # get useful info\n",
    "    dsub_user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0].replace('.','-')\n",
    "\n",
    "    job_name = f'{label}_{script.replace(\".sh\", \"\")}'\n",
    "    \n",
    "    dsub_cmd = 'dsub '\n",
    "    dsub_cmd += '--provider google-batch '\n",
    "    dsub_cmd += '--user-project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--image \"{}\" '.format(image)\n",
    "    dsub_cmd += '--network \"global/networks/network\" '\n",
    "    dsub_cmd += '--subnetwork \"regions/us-central1/subnetworks/subnetwork\" '\n",
    "    dsub_cmd += '--service-account \"$(gcloud config get-value account)\" '\n",
    "    dsub_cmd += '--use-private-address '\n",
    "    dsub_cmd += '--user \"{}\" '.format(dsub_user_name)\n",
    "    dsub_cmd += '--regions us-central1 '\n",
    "    dsub_cmd += '--logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +\\'%Y%m%d\\')/{job-id}-{task-id}-{task-attempt}.log\" '\n",
    "    dsub_cmd += ' \"$@\" '\n",
    "    dsub_cmd += '--name \"{}\" '.format(job_name)\n",
    "    dsub_cmd += '--machine-type \"{}\" '.format(machine_type)\n",
    "    \n",
    "    if preemptible:\n",
    "        dsub_cmd += '--preemptible '\n",
    "        \n",
    "    if 'c4' in machine_type:\n",
    "        raise ValueError(\n",
    "            f\"c4 machine types ('{machine_type}') are not supported with dsub. \"\n",
    "            f\"c4 requires hyperdisk-balanced boot disks, but dsub doesn't allow \"\n",
    "            f\"setting boot disks. Use c2 or n2 instead.\"\n",
    "        )\n",
    "\n",
    "#        # c4 doesn't use pd-ssd\n",
    "#         dsub_cmd += '--disk-type \"hyperdisk-balanced\" '\n",
    "#     else:\n",
    "#         dsub_cmd += '--disk-type \"pd-ssd\" '\n",
    "        \n",
    "    dsub_cmd += '--boot-disk-size {} '.format(boot_disk)\n",
    "    dsub_cmd += '--disk-size {} '.format(disk_size)\n",
    "    dsub_cmd += '--script \"{}\" '.format(script)\n",
    "    \n",
    "    # Assign any environmental conditions\n",
    "    for env_key in envs.keys():\n",
    "        dsub_cmd += '--env {}=\"{}\" '.format(env_key, envs[env_key])\n",
    "        \n",
    "    # Assign any inputs\n",
    "    for in_key in in_params.keys():\n",
    "        dsub_cmd += '--input {}=\"{}\" '.format(in_key, in_params[in_key])\n",
    "        \n",
    "    # Assign any outputs\n",
    "    for out_key in out_params.keys():\n",
    "        dsub_cmd += '--output {}=\"{}\" '.format(out_key, out_params[out_key])\n",
    "        \n",
    "    os.system(dsub_cmd)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_age_format(age: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate age format for dsub dstat command\n",
    "    \"\"\"\n",
    "    # Pattern: one or more digits followed by exactly one valid unit\n",
    "    pattern = r'^\\d+[smhdw]$'\n",
    "    return bool(re.match(pattern, age.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dsub_status(user: str = None, full: bool = False, age: str = '1d') -> subprocess.CompletedProcess:\n",
    "    \"\"\"\n",
    "    Check status of dsub jobs for the specified user\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user : str, optional\n",
    "        Username to check jobs for. Defaults to current user from OWNER_EMAIL\n",
    "    full : bool, default False\n",
    "        Include full job details in output\n",
    "    age : str, default '1d'\n",
    "        Maximum age of jobs to display. Format: <integer><unit>\n",
    "        Units: s (seconds), m (minutes), h (hours), d (days), w (weeks)\n",
    "        Examples: '3d', '12h', '30m', '7w'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    subprocess.CompletedProcess\n",
    "        Result of the dstat command\n",
    "        \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> check_dsub_status(age='3d', full=True)  # Last 3 days, full details\n",
    "    >>> check_dsub_status()  # Default: last day, summary view\n",
    "    \"\"\"\n",
    "    \n",
    "    if user is None:\n",
    "        # Get current user if not specified\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Validate age parameter\n",
    "    if age is not None:\n",
    "        if not validate_age_format(age):\n",
    "            raise ValueError(\n",
    "                f\"Invalid age format: '{age}'. \"\n",
    "                \"Expected format: <integer><unit> where unit is one of: s, m, h, d, w. \"\n",
    "                \"Examples: '3d', '12h', '30m', '7w'\"\n",
    "            )\n",
    "    \n",
    "    # Build command\n",
    "    cmd_parts = [\n",
    "        \"dstat\",\n",
    "        \"--provider google-batch\",\n",
    "        f\"--user {user}\",\n",
    "        \"--status '*'\",\n",
    "        f\"--project {project}\"\n",
    "    ]\n",
    "    \n",
    "    if full:\n",
    "        cmd_parts.append(\"--full\")\n",
    "    \n",
    "    if age:\n",
    "        cmd_parts.append(f\"--age {age}\")\n",
    "    \n",
    "    cmd = \" \".join(cmd_parts)\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_running_jobs():\n",
    "    \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Cancel only running jobs\n",
    "    cancel_cmd = f\"ddel --provider google-batch --project {project} --users 'margaret.rencher' --jobs '*'\"\n",
    "    print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "    \n",
    "    return subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_details(user=None, job=None):\n",
    "    \"\"\"List all jobs for the user, including failed ones\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    if user is None:\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "        \n",
    "    if job is None:\n",
    "        job = \"'*' \"\n",
    "    else:\n",
    "        job = f'--jobs {job} '\n",
    "    \n",
    "    cmd = f\"dstat --provider google-batch --project {project} --user {user} --status {job}--full\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ancestry_summary(anc_metadata, anc, condition):\n",
    "    \"\"\"Print clean summary of cases/controls by sex for an ancestry group\"\"\"\n",
    "           \n",
    "    # Get counts by case status and sex\n",
    "    summary = anc_metadata.groupby([condition, 'sex_binary']).size().reset_index(name='count')\n",
    "    \n",
    "    # Calculate totals and percentages\n",
    "    case_total = anc_metadata[anc_metadata[condition] == 1].shape[0]\n",
    "    ctrl_total = anc_metadata[anc_metadata[condition] == 0].shape[0]\n",
    "    \n",
    "    # Get male counts (sex_binary = 1)\n",
    "    case_female = summary[(summary[condition] == 1) & (summary['sex_binary'] == 0)]['count'].sum()\n",
    "    ctrl_female = summary[(summary[condition] == 0) & (summary['sex_binary'] == 0)]['count'].sum()\n",
    "    \n",
    "    # Calculate percentages (handle division by zero)\n",
    "    case_female_pct = (case_female / case_total * 100) if case_total > 0 else 0\n",
    "    ctrl_female_pct = (ctrl_female / ctrl_total * 100) if ctrl_total > 0 else 0\n",
    "    \n",
    "    print(f'{anc} ancestry: {case_total:,} cases ({case_female:,}, {case_female_pct:.1f}% female), {ctrl_total:,} controls ({ctrl_female:,}, {ctrl_female_pct:.1f}% female)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_dsub_logs(log_path):\n",
    "    base_path = log_path.replace('.log', '')\n",
    "    \n",
    "    print(\"=== STDOUT ===\")\n",
    "    subprocess.run(['gsutil', 'cat', f'{base_path}-stdout.log'])\n",
    "    \n",
    "    print(\"\\n=== STDERR ===\") \n",
    "    subprocess.run(['gsutil', 'cat', f'{base_path}-stderr.log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pcs(df, pc_col, pc_prefix, num_pcs=20):\n",
    "    \"\"\"\n",
    "    Parse comma-separated PC string into individual PC columns\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with PC data\n",
    "    pc_col: column name containing comma-separated PC values\n",
    "    pc_prefix: prefix for new column names (e.g., 'PC', 'ancPC')\n",
    "    num_pcs: number of PCs to extract (default first 20)\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with new PC columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert list column to DataFrame and select first num_pcs columns\n",
    "    pc_data = pd.DataFrame(df[pc_col].tolist(), index=df.index).iloc[:, :num_pcs]\n",
    "    \n",
    "    # Create column names: PC1, PC2, etc. or ancPC1, ancPC2, etc.\n",
    "    pc_columns = [f'{pc_prefix}{i+1}' for i in range(num_pcs)]\n",
    "    pc_data.columns = pc_columns\n",
    "    \n",
    "    # Concatenate with original dataframe\n",
    "    for col in pc_columns:\n",
    "        df[col] = pc_data[col]\n",
    "    \n",
    "    print(f\"Parsed {num_pcs} PCs from {pc_col} as {pc_prefix}1-{pc_prefix}{num_pcs}\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binarize_columns(df, col_refs):\n",
    "#     # Iterate through each column, binarize it. Col_ref = trait1::ref\n",
    "#     for col_ref in col_refs:\n",
    "#         col = col_ref.split('::')[0]\n",
    "#         ref = col_ref.split('::')[1]\n",
    "\n",
    "#         col_vals = df[col].dropna().unique()\n",
    "#         if ((len(col_vals) > 2) and (ref in col_vals)):\n",
    "#             print('The term {} contains >2 levels. Skipping binarize...'.format(\n",
    "#                 col\n",
    "#             ))\n",
    "#             continue\n",
    "#         elif (ref not in col_vals):\n",
    "#             raise Exception('The reference {} is not a level for term {}. Please fix!'.format(\n",
    "#                 ref, col\n",
    "#             ))\n",
    "\n",
    "#         col_alt = np.setdiff1d(col_vals, [ref]).tolist()[0]\n",
    "#         val_map = {ref: 0, col_alt: 1}\n",
    "#         df[col] = [ val_map[x] if not pd.isna(x) else x for x in df[col] ]\n",
    "#     return df\n",
    "\n",
    "def binarize_sex_chromosomes(df, sex_col='dragen_sex_ploidy'):\n",
    "    \"\"\"\n",
    "    Binarize sex chromosome data: XX=0 (female), XY=1 (male), others=NaN\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with sex chromosome data\n",
    "    sex_col: column name containing sex chromosome calls\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with new binary sex column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "      \n",
    "    # Create binary sex variable\n",
    "    df['sex_binary'] = df[sex_col].map({\n",
    "        'XX': 0,  # Female reference\n",
    "        'XY': 1   # Male\n",
    "        # All others (XO, XXY, XYY, XXX, etc.) become NaN automatically\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nAfter binarization:\")\n",
    "    print(f\"XX (female): {(df['sex_binary'] == 0).sum():,}\")\n",
    "    print(f\"XY (male): {(df['sex_binary'] == 1).sum():,}\")\n",
    "    print(f\"Missing/Other: {df['sex_binary'].isna().sum():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# def _rank_inverse_normalize(data_ser, c=3.0/8):\n",
    "#     '''\n",
    "#     Inverse rank-normalize phenotype. Takes Series.\n",
    "#     '''\n",
    "#     # Shuffle by index\n",
    "#     orig_idx = data_ser.index\n",
    "\n",
    "#     data_ser = data_ser.loc[~pd.isnull(data_ser)]\n",
    "#     alg_input = data_ser.loc[np.random.permutation(data_ser.index.tolist())].copy()\n",
    "\n",
    "#     # Get rank, ties are determined by their position in the series (hence\n",
    "#     # why we randomised the series)\n",
    "#     rank = ss.rankdata(alg_input, method='ordinal')\n",
    "#     rank = pd.Series(rank, index=alg_input.index)\n",
    "\n",
    "#     # Convert rank to normal distribution\n",
    "#     norm_ser = rank.apply(\n",
    "#         lambda x, c, n: ss.norm.ppf((x - c) / (n - 2*c + 1)),\n",
    "#         n=len(rank),\n",
    "#         c=c\n",
    "#     )\n",
    "#     final = pd.Series(\n",
    "#         [ norm_ser[x] if x in norm_ser.index else pd.NA for x in orig_idx ],\n",
    "#         index=orig_idx\n",
    "#     )\n",
    "#     return(final)\n",
    "\n",
    "# def _zscore_standardize(data_ser):\n",
    "#     '''\n",
    "#     Standardize the phenotype using Z-score standardization. Takes Series.\n",
    "#     '''\n",
    "#     data_ser -= data_ser.mean(skipna=True)\n",
    "#     data_ser /= data_ser.std(skipna=True)\n",
    "#     return data_ser\n",
    "\n",
    "\n",
    "# def normalize_columns(df, col_method_dict):\n",
    "#     # Iterate through each column, normalize\n",
    "#     for col in col_method_dict.keys():\n",
    "#         df['{}__untransformed'.format(col)] = df[col]\n",
    "\n",
    "#         col_vals = df[col]\n",
    "#         method = col_method_dict[col]\n",
    "        \n",
    "#         if method == 'inv_rank_norm':\n",
    "#             col_vals = _rank_inverse_normalize(col_vals, 3.0/8)\n",
    "#         elif method == 'standardize':\n",
    "#             col_vals = _zscore_standardize(col_vals)\n",
    "#         else:\n",
    "#             print('Incorrect normalization method `{}`. Skipping...'.format(method))\n",
    "\n",
    "#         df[col] = col_vals\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sarcoid_df = filtered_sarcoid_df[['person_id', 'end_of_study_age', 'case']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sarcoid_df = filtered_sarcoid_df.rename(columns={'case': 'condition__sarcoid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = filtered_sarcoid_df.merge(\n",
    "    sex_df[['research_id', 'dragen_sex_ploidy']],\n",
    "    left_on='person_id',\n",
    "    right_on='research_id',\n",
    "    how='left'\n",
    ").drop(columns=['research_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.merge(\n",
    "    pc_df[['research_id', 'ancestry_pred', 'pca_features', 'ancestry_pred_other', 'anc_pca_features']],\n",
    "    left_on='person_id',\n",
    "    right_on='research_id',\n",
    "    how='left'\n",
    ").drop(columns=['research_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAIGE can handle sample IDs not found in genomic dataset, but here we'll drop to minimize later processing\n",
    "non_null_metadata = metadata.dropna(subset=['dragen_sex_ploidy', 'ancestry_pred_other', 'anc_pca_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anc in ancestries_considered:\n",
    "    print(f'=== Processing {anc} ancestry ===')\n",
    "    anc_metadata = non_null_metadata[non_null_metadata['ancestry_pred_other'] == anc]\n",
    "    \n",
    "    # This sets XX=0 (female), XY=1 (male), others (e.g., XXY, X0) = NaN\n",
    "    # For SAIGE Step 1 (fitNULLGLMM), missing` covariate data is excluded during null model fitting \n",
    "    # and therefore, Step 2\n",
    "    anc_metadata = binarize_sex_chromosomes(anc_metadata, 'dragen_sex_ploidy')\n",
    "\n",
    "    # If normalizing is required:\n",
    "#     anc_metadata = normalize_columns(anc_metadata, covariates_normalize)\n",
    "    \n",
    "    # Parse PCs\n",
    "    anc_metadata = parse_pcs(anc_metadata, 'anc_pca_features', 'ancPC', 20)\n",
    "\n",
    "    print_ancestry_summary(anc_metadata, anc, 'condition__sarcoid')\n",
    "    \n",
    "    # write\n",
    "    out_file = f'{output_folder}/{anc}/gwas_metadata.tsv'\n",
    "    anc_metadata.to_csv(out_file, sep='\\t', index=False, header=True)\n",
    "    \n",
    "    print(f'\\nSaved to {out_file}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'=== Processing Trans-ancestry ===')\n",
    "# transancestry_metadata = non_null_metadata\n",
    "\n",
    "# # This sets XX=0 (female), XY=1 (male), others (e.g., XXY, X0) = NaN\n",
    "# # For SAIGE Step 1 (fitNULLGLMM), missing` covariate data is excluded during null model fitting \n",
    "# # and therefore, Step 2\n",
    "# transancestry_metadata = binarize_sex_chromosomes(transancestry_metadata, 'dragen_sex_ploidy')\n",
    "\n",
    "# # If normalizing is required:\n",
    "# #     transancestry_metadata = normalize_columns(transancestry_metadata, covariates_normalize)\n",
    "\n",
    "# # Parse PCs\n",
    "# transancestry_metadata = parse_pcs(transancestry_metadata, 'pca_features', 'PC', 16)\n",
    "\n",
    "# print_ancestry_summary(transancestry_metadata, 'all', 'condition__sarcoid')\n",
    "\n",
    "# # write\n",
    "# out_file = f'{output_folder}/transancestry/gwas_metadata.tsv'\n",
    "# transancestry_metadata.to_csv(out_file, sep='\\t', index=False, header=True)\n",
    "\n",
    "# print(f'\\nSaved to {out_file}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_saige_null_model.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "in_base=$(echo $INPUT_BED | sed 's/.bed//g');\n",
    "out_base=$(echo $OUTPUT_NULL_RDA | sed 's/.rda//g');\n",
    "\n",
    "step1_fitNULLGLMM.R \\\n",
    "    --plinkFile=${in_base} \\\n",
    "    --phenoFile=${INPUT_METADATA} \\\n",
    "    --phenoCol=${TRAIT} \\\n",
    "    --covarColList=${COVARCOLLIST} \\\n",
    "    --qCovarColList=${QCOVARCOLLIST} \\\n",
    "    --sampleIDColinphenoFile=person_id \\\n",
    "    --traitType=${TRAIT_TYPE} \\\n",
    "    --invNormalize=FALSE \\\n",
    "    --nThreads=${THREADS} \\\n",
    "    --IsOverwriteVarianceRatioFile=TRUE \\\n",
    "    --skipVarianceRatioEstimation=FALSE \\\n",
    "    --outputPrefix=${out_base} \\\n",
    "    --useSparseGRMtoFitNULL=FALSE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_saige_null(\n",
    "    anc,\n",
    "    trait,\n",
    "    trait_type,\n",
    "    covarColList,\n",
    "    qCovarColList,\n",
    "    script\n",
    "):\n",
    "    \n",
    "    artifact_registry = os.getenv('ARTIFACT_REGISTRY_DOCKER_REPO', '')\n",
    "    \n",
    "    # get base files\n",
    "    ref_base = f'{src_bucket}/data/stg005/merged_genotypes/{anc}_merged_genotypes'\n",
    "    if anc == 'all':\n",
    "        anc_folder = 'transancestry'\n",
    "    else:\n",
    "        anc_folder = anc\n",
    "        \n",
    "    out_dir = f'{output_folder}/{anc_folder}/{trait}'\n",
    "\n",
    "    # check if both required files already exist\n",
    "    existing_files = [x.split('/')[-1] for x in get_file_list(out_dir)]\n",
    "    required_files = ['saige_null_model.rda', 'saige_null_model.varianceRatio.txt']\n",
    "    all_exist = all(file in existing_files for file in required_files)\n",
    "\n",
    "    if not all_exist:\n",
    "        env_dict = {\n",
    "            'TRAIT': trait,\n",
    "            'TRAIT_TYPE': trait_type,\n",
    "            'COVARCOLLIST': ','.join(covarColList),\n",
    "            'QCOVARCOLLIST': ','.join(qCovarColList),\n",
    "            'THREADS': 8\n",
    "        }\n",
    "\n",
    "        in_dict = {\n",
    "            'INPUT_BED': f'{ref_base}.bed',\n",
    "            'INPUT_BIM': f'{ref_base}.bim',\n",
    "            'INPUT_FAM': f'{ref_base}.fam',\n",
    "            'INPUT_METADATA': f'{output_folder}/{anc_folder}/gwas_metadata.tsv'\n",
    "        }\n",
    "\n",
    "        out_dict = {\n",
    "            'OUTPUT_NULL_RDA': f'{out_dir}/saige_null_model.rda',\n",
    "            'OUTPUT_NULL_VARRAT': f'{out_dir}/saige_null_model.varianceRatio.txt'\n",
    "        }\n",
    "\n",
    "        dsub_script(\n",
    "            label=f'step1_{anc}',\n",
    "            machine_type = 'n2d-standard-8', # recommended for 300K samples × 135K SNPs ≈ 40 billion genotypes,\n",
    "                # ~ 15-20GB RAM\n",
    "            envs = env_dict,\n",
    "            in_params = in_dict,\n",
    "            out_params = out_dict,\n",
    "            boot_disk = 100,\n",
    "            disk_size = 150,\n",
    "            image=f'{artifact_registry}/wzhou88/saige:1.3.6',\n",
    "            script = script,\n",
    "            preemptible = True\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping {anc}/{trait} - null model files already exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anc in ['eur', 'afr']: \n",
    "    for trait in traits.keys():\n",
    "        _ = run_saige_null(\n",
    "            anc,\n",
    "            trait,\n",
    "            traits[trait],\n",
    "            covarColList,\n",
    "            sex_qCovarColList if trait.startswith('fibroids') else qCovarColList,\n",
    "            'run_saige_null_model_f_only.sh' if trait.startswith('fibroids') else 'run_saige_null_model.sh'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsub_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_details(job='step1-eur---bwaxse--250630-173928-69')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=\"\"\n",
    "view_dsub_logs(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {output_folder}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SAIGE hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_saige_chrom_multi.sh\n",
    "\n",
    "# Single ancestry, single chromosome SAIGE step2\n",
    "echo \"Processing ancestry: $ANCESTRY, chromosome: $CHR\"\n",
    "\n",
    "# Extract plink base path from bed file\n",
    "plink_base=$(echo $INPUT_BED | sed 's/\\.bed$//')\n",
    "\n",
    "echo \"Using plink base: $plink_base\"\n",
    "\n",
    "step2_SPAtests.R \\\n",
    "    --bedFile=\"${plink_base}.bed\" \\\n",
    "    --bimFile=\"${plink_base}.bim\" \\\n",
    "    --famFile=\"${plink_base}.fam\" \\\n",
    "    --chrom=\"${CHR}\" \\\n",
    "    --is_imputed_data=\"FALSE\" \\\n",
    "    --AlleleOrder=\"alt-first\" \\\n",
    "    --GMMATmodelFile=\"${INPUT_NULL_RDA}\" \\\n",
    "    --varianceRatioFile=\"${INPUT_NULL_VARRAT}\" \\\n",
    "    --is_Firth_beta=\"TRUE\" \\\n",
    "    --pCutoffforFirth=\"0.05\" \\\n",
    "    --minMAC=20 \\\n",
    "    --is_output_moreDetails=\"TRUE\" \\\n",
    "    --SAIGEOutputFile=\"${OUTPUT_FILE}\" \\\n",
    "    --LOCO=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_saige_step2(\n",
    "    ancestries,\n",
    "    trait,\n",
    "    script,\n",
    "    chroms=range(1, 23),\n",
    "    machine_type='n2d-standard-8',\n",
    "    disk_size=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Run SAIGE step2 parallelized by ancestry and chromosome.\n",
    "    \n",
    "    Args:\n",
    "        ancestries: List of ancestry codes (e.g., ['eur', 'afr', 'amr'])\n",
    "        trait: Trait name\n",
    "        script: Path to the step2 script\n",
    "        chroms: Chromosomes to process\n",
    "        machine_type: VM machine type\n",
    "        disk_size: Disk size in GB\n",
    "    \"\"\"\n",
    "    \n",
    "    artifact_registry = os.getenv('ARTIFACT_REGISTRY_DOCKER_REPO', '')\n",
    "    jobs_submitted = 0\n",
    "    jobs_skipped = 0\n",
    "\n",
    "    # Pre-fetch all file lists and null model existence for all ancestries\n",
    "    print(f\"Fetching file lists for {trait}...\")\n",
    "    ancestry_files = {}\n",
    "    null_models_exist = {}\n",
    "\n",
    "    for anc in ancestries:\n",
    "        out_dir = f'{output_folder}//{anc}/{trait}/gwas'\n",
    "        null_model_file = f'{output_folder}//{anc}/{trait}/saige_null_model.rda'\n",
    "        \n",
    "        try:\n",
    "            ancestry_files[anc] = set(x.split('/')[-1] for x in get_file_list(out_dir))\n",
    "        except:\n",
    "            ancestry_files[anc] = set()\n",
    "            \n",
    "        try:\n",
    "            null_models_exist[anc] = gcs_file_exists(null_model_file)\n",
    "        except:\n",
    "            null_models_exist[anc] = False\n",
    "\n",
    "        print(f\"Submitting jobs for {anc}\")\n",
    "        existing_files = ancestry_files[anc]\n",
    "        null_model_exists = null_models_exist[anc]\n",
    "       \n",
    "        for chrom in chroms:\n",
    "            # Check if output files already exist\n",
    "            required_files = {f'gwas_results_chr{chrom}.txt', f'gwas_results_chr{chrom}.txt.index'}\n",
    "            results_exist = required_files.issubset(existing_files)\n",
    "\n",
    "            if results_exist and null_model_exists:\n",
    "                print(f\"  Skipping {anc} chr{chrom} - results already exist\")\n",
    "                jobs_skipped += 1\n",
    "            elif results_exist and not null_model_exists:\n",
    "                print(f\"  Skipping {anc} chr{chrom} - results exist but null model missing\")\n",
    "                jobs_skipped += 1\n",
    "            elif not results_exist and not null_model_exists:\n",
    "                print(f\"  Skipping {anc} chr{chrom} - null model missing\")\n",
    "                jobs_skipped += 1\n",
    "            else:  # not results_exist and null_model_exists\n",
    "                # Submit the job\n",
    "                plink_base = f'{src_bucket}/data/stg009/{anc}/genotypes_chr{chrom}'\n",
    "\n",
    "                env_dict = {\n",
    "                    'CHR': chrom,\n",
    "                    'ANCESTRY': anc,\n",
    "                }\n",
    "\n",
    "                in_dict = {\n",
    "                    'INPUT_BED': f'{plink_base}.bed',\n",
    "                    'INPUT_BIM': f'{plink_base}.bim',\n",
    "                    'INPUT_FAM': f'{plink_base}.fam',\n",
    "                    'INPUT_NULL_RDA': f'{output_folder}//{anc}/{trait}/saige_null_model.rda',\n",
    "                    'INPUT_NULL_VARRAT': f'{output_folder}//{anc}/{trait}/saige_null_model.varianceRatio.txt',\n",
    "                }\n",
    "\n",
    "                out_dict = {\n",
    "                    'OUTPUT_FILE': f'{output_folder}//{anc}/{trait}/gwas/gwas_results_chr{chrom}.txt',\n",
    "                    'OUTPUT_FILE_INDEX': f'{output_folder}//{anc}/{trait}/gwas/gwas_results_chr{chrom}.txt.index'\n",
    "                }\n",
    "\n",
    "                dsub_script(\n",
    "                    label=f's2_{anc}_{chrom}_{trait}',\n",
    "                    machine_type=machine_type,\n",
    "                    envs=env_dict,\n",
    "                    in_params=in_dict,\n",
    "                    out_params=out_dict,\n",
    "                    boot_disk=100,\n",
    "                    disk_size=disk_size,\n",
    "                    image=f'{artifact_registry}/wzhou88/saige:1.3.6',\n",
    "                    script=script,\n",
    "                    preemptible=True\n",
    "                )\n",
    "                jobs_submitted += 1\n",
    "    \n",
    "    print(f\"SAIGE step2 summary: {jobs_submitted} jobs submitted, {jobs_skipped} jobs skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ancestry-specific Step 2 for all traits\n",
    "for trait in traits.keys():\n",
    "    run_saige_step2(\n",
    "        ancestries=['eur', 'afr'],\n",
    "        trait=trait,\n",
    "        script='run_saige_chrom_multi.sh',\n",
    "#         chroms=[22] # [22] would run only chr 22; not setting 'chroms' would run all\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check All Statuses\n",
    "check_dsub_status(full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details(job='s2-afr-2-c--bwaxse--250827-210514-71')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=\"{bucket or my_bucket}/dsub/logs/s2-afr-2-condition--sarcoid-run-saige-chrom-multi/bwaxse/20250827/s2-afr-2-c--bwaxse--250827-210514-71-task-None.log\"\n",
    "view_dsub_logs(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {bucket or my_bucket}/dsub/logs/step2-eur-chr13-condition--sarcoid-run-saige-chrom-multi/bwaxse/20250701/step2-eur---bwaxse--250701-140545-85-task-None.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {my_bucket}/saige_gwas/afr/condition__sarcoid/gwas/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
