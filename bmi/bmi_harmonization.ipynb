{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMI Comprehensive Harmonization for All of Us\n",
    "\n",
    "**Purpose**: Extract, clean, and harmonize BMI data with advanced quality control  \n",
    "**Author**: Bennett Waxse  \n",
    "**Created**: June 2025 \n",
    "**CDR Version**: v8  \n",
    "\n",
    "## Features\n",
    "- Multiple validated concept IDs for weight, height, BMI\n",
    "- Unit conversion and validation\n",
    "- 4-sigma outlier removal by unit type\n",
    "- Quality control metrics and validation plots\n",
    "- Temporal matching for cohort studies\n",
    "\n",
    "## Dependencies\n",
    "```\n",
    "pandas, polars, seaborn, matplotlib, google-cloud-bigquery\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Configuration\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pl.Config.set_fmt_str_lengths(100)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of Us Workbench Setup\n",
    "version = %env WORKSPACE_CDR\n",
    "print(\"CDR version: \" + version)\n",
    "\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "print(\"Workspace bucket: \" + my_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_gbq(query):\n",
    "    \"\"\"\n",
    "    Execute BigQuery SQL and return result as polars dataframe\n",
    "    \n",
    "    Args:\n",
    "        query: BigQuery SQL query string\n",
    "    \n",
    "    Returns:\n",
    "        pl.DataFrame: Query results\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query)\n",
    "    rows = query_job.result()\n",
    "    df = pl.from_arrow(rows.to_arrow())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Validated Concept IDs\n",
    "\n",
    "These concept IDs have been validated against All of Us data to ensure they capture the relevant measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validated BMI-related concept IDs\n",
    "WEIGHT_CONCEPTS = [3010220, 3013762, 3025315, 3027492, 3023166]\n",
    "HEIGHT_CONCEPTS = [3036798, 3023540, 3019171, 3036277, 40655804]\n",
    "BMI_CONCEPTS = [3038553, 4245997]\n",
    "\n",
    "print(f\"Weight concepts: {len(WEIGHT_CONCEPTS)}\")\n",
    "print(f\"Height concepts: {len(HEIGHT_CONCEPTS)}\")\n",
    "print(f\"BMI concepts: {len(BMI_CONCEPTS)}\")\n",
    "print(f\"Total concepts: {len(WEIGHT_CONCEPTS + HEIGHT_CONCEPTS + BMI_CONCEPTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction\n",
    "\n",
    "**Note**: Replace `cohort_dfs` with your actual cohort DataFrames. This example shows the pattern for multiple cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace this with your actual cohort DataFrames\n",
    "# cohort_dfs = [df1, df2, df3]  # Your cohort DataFrames with person_id columns\n",
    "\n",
    "# For demonstration, create a sample cohort\n",
    "# In practice, use your real cohort data\n",
    "sample_person_ids = [1001, 1002, 1003, 1004, 1005]  # Replace with real person_ids\n",
    "\n",
    "# Extract person IDs from cohort DataFrames\n",
    "# person_id_series = [df['person_id'] for df in cohort_dfs]\n",
    "# combined_person_ids = pd.concat(person_id_series)\n",
    "# unique_person_ids = combined_person_ids.drop_duplicates()\n",
    "\n",
    "# For demo purposes:\n",
    "unique_person_ids = pd.Series(sample_person_ids)\n",
    "person_ids_str = ', '.join(map(str, unique_person_ids))\n",
    "\n",
    "print(f\"Number of unique persons: {len(unique_person_ids)}\")\n",
    "print(f\"Sample person IDs: {unique_person_ids.head().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BMI-related measurements\n",
    "bmi_extraction_query = f\"\"\"\n",
    "WITH combined AS (\n",
    "    SELECT \n",
    "        person_id, \n",
    "        measurement_date AS date, \n",
    "        MAX(IF(measurement_concept_id IN ({','.join(map(str, WEIGHT_CONCEPTS))}), \n",
    "            value_as_number, NULL)) AS wt,\n",
    "        MAX(IF(measurement_concept_id IN ({','.join(map(str, WEIGHT_CONCEPTS))}), \n",
    "            c2.concept_name, NULL)) AS wt_units,\n",
    "        MAX(IF(measurement_concept_id IN ({','.join(map(str, HEIGHT_CONCEPTS))}), \n",
    "            value_as_number, NULL)) AS ht,\n",
    "        MAX(IF(measurement_concept_id IN ({','.join(map(str, HEIGHT_CONCEPTS))}), \n",
    "            c2.concept_name, NULL)) AS ht_units,\n",
    "        MAX(IF(measurement_concept_id IN ({','.join(map(str, BMI_CONCEPTS))}), \n",
    "            value_as_number, NULL)) AS bmi,\n",
    "        MAX(IF(measurement_concept_id IN ({','.join(map(str, BMI_CONCEPTS))}), \n",
    "            c2.concept_name, NULL)) AS bmi_units\n",
    "    FROM \n",
    "        {version}.measurement m\n",
    "    INNER JOIN \n",
    "        {version}.concept c2 ON unit_concept_id = c2.concept_id\n",
    "    WHERE \n",
    "        measurement_concept_id IN ({','.join(map(str, WEIGHT_CONCEPTS + HEIGHT_CONCEPTS + BMI_CONCEPTS))})\n",
    "        AND person_id IN ({person_ids_str})\n",
    "    GROUP BY \n",
    "        person_id, measurement_date\n",
    ")\n",
    "SELECT *\n",
    "FROM combined\n",
    "ORDER BY person_id, date\n",
    "\"\"\"\n",
    "\n",
    "print(\"Extracting BMI data...\")\n",
    "# bmi_code_df = polars_gbq(bmi_extraction_query)\n",
    "# print(f\"Raw measurements extracted: {len(bmi_code_df):,}\")\n",
    "# print(f\"Unique persons: {bmi_code_df['person_id'].n_unique():,}\")\n",
    "\n",
    "# For demo purposes, create sample data\n",
    "print(\"Note: Replace the above commented lines with actual query execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "Before cleaning, let's examine the raw data quality and unit distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unit distributions (uncomment when running with real data)\n",
    "# print(\"Weight units:\")\n",
    "# print(bmi_code_df['wt_units'].value_counts())\n",
    "# print(\"\\nHeight units:\")\n",
    "# print(bmi_code_df['ht_units'].value_counts())\n",
    "# print(\"\\nBMI units:\")\n",
    "# print(bmi_code_df['bmi_units'].value_counts())\n",
    "\n",
    "print(\"Unit distribution analysis (replace with real data execution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unit Cleaning\n",
    "\n",
    "Remove measurements with problematic or ambiguous units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_units(df):\n",
    "    \"\"\"\n",
    "    Clean unit inconsistencies and invalid measurements\n",
    "    \"\"\"\n",
    "    print(\"Cleaning units...\")\n",
    "    \n",
    "    # Clean weight units - set to null where no matching concept\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col('wt_units') == 'No matching concept')\n",
    "          .then(pl.lit(None))\n",
    "          .otherwise(pl.col('wt_units'))\n",
    "          .alias('wt_units')\n",
    "    )\n",
    "    \n",
    "    # Clean height units - remove ambiguous 'percent' measurements\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col('ht_units') == \"percent\")\n",
    "          .then(pl.lit(None))\n",
    "          .otherwise(pl.col('ht_units'))\n",
    "          .alias('ht_units'),\n",
    "        pl.when(pl.col('ht_units') == \"percent\")\n",
    "          .then(pl.lit(None))\n",
    "          .otherwise(pl.col('ht'))\n",
    "          .alias('ht')\n",
    "    ])\n",
    "    \n",
    "    # Clean BMI units - remove ambiguous measurements\n",
    "    df = df.with_columns([\n",
    "        pl.when((pl.col('bmi_units') == \"ratio\") | (pl.col('bmi_units') == \"no value\"))\n",
    "          .then(pl.lit(None))\n",
    "          .otherwise(pl.col('bmi_units'))\n",
    "          .alias('bmi_units'),\n",
    "        pl.when((pl.col('bmi_units') == \"ratio\") | (pl.col('bmi_units') == \"no value\"))\n",
    "          .then(pl.lit(None))\n",
    "          .otherwise(pl.col('bmi'))\n",
    "          .alias('bmi')\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# bmi_code_df = clean_units(bmi_code_df)\n",
    "print(\"Unit cleaning function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outlier Detection and Removal\n",
    "\n",
    "Remove extreme outliers using 4-sigma thresholds calculated by unit type. This prevents removing valid measurements that appear extreme only because they're in different units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clean_outliers(df, column, unit=None):\n",
    "    \"\"\"\n",
    "    Remove outliers using 4-sigma threshold, calculated by unit type\n",
    "    \"\"\"\n",
    "    print(f\"Cleaning outliers for {column}...\")\n",
    "    \n",
    "    if unit:\n",
    "        # Group by unit and calculate mean and std\n",
    "        stats = df.filter(pl.col(unit).is_not_null()).group_by(unit).agg([\n",
    "            pl.col(column).mean().alias(column + '_mean'),\n",
    "            pl.col(column).std().alias(column + '_std')\n",
    "        ])\n",
    "        df = df.join(stats, on=unit, how='left')\n",
    "        \n",
    "        # Calculate bounds for outliers (4 sigma)\n",
    "        lower_bound = df[column + '_mean'] - 4 * df[column + '_std']\n",
    "        upper_bound = df[column + '_mean'] + 4 * df[column + '_std']\n",
    "        \n",
    "        # Count outliers before removal\n",
    "        # outliers = df.filter(\n",
    "        #     (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "        # ).height\n",
    "        # print(f\"  Removing {outliers} outliers from {column}\")\n",
    "        \n",
    "        # Set values outside 4 standard deviations to None\n",
    "        df = df.with_columns(\n",
    "            pl.when(\n",
    "                (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "            ).then(None).otherwise(df[column]).alias(column)\n",
    "        )\n",
    "        \n",
    "        # Drop the mean and std columns\n",
    "        df = df.drop([column + '_mean', column + '_std'])\n",
    "    else:\n",
    "        # For columns without units, calculate global mean and std\n",
    "        mean = df[column].mean()\n",
    "        std = df[column].std()\n",
    "        lower_bound = mean - 4 * std\n",
    "        upper_bound = mean + 4 * std\n",
    "        \n",
    "        # Set outliers to None\n",
    "        df = df.with_columns(\n",
    "            pl.when(\n",
    "                (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "            ).then(None).otherwise(df[column]).alias(column)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply cleaning for weight, height, and BMI\n",
    "# bmi_code_df = apply_clean_outliers(bmi_code_df, \"wt\", \"wt_units\")\n",
    "# bmi_code_df = apply_clean_outliers(bmi_code_df, \"ht\", \"ht_units\")\n",
    "# bmi_code_df = apply_clean_outliers(bmi_code_df, \"bmi\")\n",
    "\n",
    "print(\"Outlier cleaning functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Unit Conversion\n",
    "\n",
    "Convert all measurements to standard units (kg for weight, cm for height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_units(df):\n",
    "    \"\"\"\n",
    "    Convert measurements to standard units\n",
    "    \"\"\"\n",
    "    print(\"Converting units to standard (kg, cm)...\")\n",
    "    \n",
    "    # Convert height from inches to centimeters\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"ht_units\") == \"inch (US)\")\n",
    "        .then(pl.col(\"ht\") * 2.54)\n",
    "        .otherwise(pl.col(\"ht\"))\n",
    "        .alias(\"ht\")\n",
    "    )\n",
    "    \n",
    "    # Convert weight from pounds to kilograms\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"wt_units\") == \"pound (US)\")\n",
    "        .then(pl.col(\"wt\") * 0.45359237)\n",
    "        .otherwise(pl.col(\"wt\"))\n",
    "        .alias(\"wt\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# bmi_code_df = convert_units(bmi_code_df)\n",
    "print(\"Unit conversion function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Filtering and Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where all measurements are null\n",
    "# bmi_code_df = bmi_code_df.filter(\n",
    "#     pl.col(\"wt\").is_not_null() | pl.col(\"ht\").is_not_null() | pl.col(\"bmi\").is_not_null()\n",
    "# )\n",
    "\n",
    "# Rename columns to reflect standard units\n",
    "# bmi_code_df = bmi_code_df.rename({\n",
    "#     \"wt\": \"wt_kg\",\n",
    "#     \"ht\": \"ht_cm\",\n",
    "#     \"bmi\": \"bmi\"\n",
    "# })\n",
    "\n",
    "# Drop the unit columns\n",
    "# bmi_code_df = bmi_code_df.drop([\"wt_units\", \"ht_units\", \"bmi_units\"])\n",
    "\n",
    "print(\"Data filtering and renaming steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. BMI Calculation and Validation\n",
    "\n",
    "Calculate BMI from height and weight, then compare with recorded BMI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BMI using the formula: weight (kg) / (height (m)^2)\n",
    "# bmi_code_df = bmi_code_df.with_columns(\n",
    "#     (pl.col(\"wt_kg\") / (pl.col(\"ht_cm\") / 100) ** 2).alias(\"bmi_calc\")\n",
    "# )\n",
    "\n",
    "# Calculate difference between recorded and calculated BMI\n",
    "# bmi_code_df = bmi_code_df.with_columns(\n",
    "#     (pl.col(\"bmi\") - pl.col(\"bmi_calc\")).alias(\"bmi_diff\")\n",
    "# )\n",
    "\n",
    "print(\"BMI calculation steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quality Validation Plot\n",
    "\n",
    "Visualize the difference between recorded and calculated BMI to assess data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation plot\n",
    "# plot_df = bmi_code_df.to_pandas()\n",
    "\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# # BMI difference histogram\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.histplot(data=plot_df, x=\"bmi_diff\", bins=50)\n",
    "# plt.title(\"Difference: Recorded BMI - Calculated BMI\")\n",
    "# plt.xlabel(\"BMI Difference\")\n",
    "# plt.ylabel(\"Count\")\n",
    "\n",
    "# # Scatter plot: recorded vs calculated BMI\n",
    "# plt.subplot(1, 2, 2)\n",
    "# valid_bmi = plot_df.dropna(subset=['bmi', 'bmi_calc'])\n",
    "# plt.scatter(valid_bmi['bmi_calc'], valid_bmi['bmi'], alpha=0.5)\n",
    "# plt.plot([10, 70], [10, 70], 'r--', label='Perfect Agreement')\n",
    "# plt.xlabel(\"Calculated BMI\")\n",
    "# plt.ylabel(\"Recorded BMI\")\n",
    "# plt.title(\"Recorded vs Calculated BMI\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "print(\"Validation plotting code defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final BMI Harmonization\n",
    "\n",
    "Use calculated BMI where recorded BMI is missing, then create final clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'bmi' where it is null with the value from 'bmi_calc'\n",
    "# bmi_code_df = bmi_code_df.with_columns(\n",
    "#     pl.when(pl.col(\"bmi\").is_null())\n",
    "#     .then(pl.col(\"bmi_calc\"))\n",
    "#     .otherwise(pl.col(\"bmi\"))\n",
    "#     .alias(\"bmi\")\n",
    "# )\n",
    "\n",
    "# Select final columns\n",
    "# bmi_code_df = bmi_code_df.select(['person_id', 'date', 'wt_kg', 'ht_cm', 'bmi'])\n",
    "\n",
    "# Filter to rows with valid BMI\n",
    "# bmi_code_df = bmi_code_df.filter(pl.col('bmi').is_not_null())\n",
    "\n",
    "# Sort by person and date\n",
    "# bmi_code_df = bmi_code_df.sort(\"person_id\", \"date\")\n",
    "\n",
    "print(\"Final harmonization steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics\n",
    "\n",
    "Generate summary statistics for the harmonized BMI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "# print(\"=== BMI Harmonization Summary ===\")\n",
    "# print(f\"Total measurements: {len(bmi_code_df):,}\")\n",
    "# print(f\"Unique persons: {bmi_code_df['person_id'].n_unique():,}\")\n",
    "# print(f\"Date range: {bmi_code_df['date'].min()} to {bmi_code_df['date'].max()}\")\n",
    "# print(f\"\\nBMI statistics:\")\n",
    "# print(f\"  Mean: {bmi_code_df['bmi'].mean():.1f}\")\n",
    "# print(f\"  Median: {bmi_code_df['bmi'].median():.1f}\")\n",
    "# print(f\"  Range: {bmi_code_df['bmi'].min():.1f} - {bmi_code_df['bmi'].max():.1f}\")\n",
    "\n",
    "# # Measurements per person\n",
    "# measurements_per_person = bmi_code_df.group_by('person_id').count()\n",
    "# print(f\"\\nMeasurements per person:\")\n",
    "# print(f\"  Mean: {measurements_per_person['count'].mean():.1f}\")\n",
    "# print(f\"  Median: {measurements_per_person['count'].median():.1f}\")\n",
    "\n",
    "print(\"Summary statistics code defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Temporal Matching for Cohort Studies\n",
    "\n",
    "Function to merge BMI data with cohort DataFrames using closest temporal match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_temporal_matching(cohort_df, bmi_df, time_col='time_zero', \n",
    "                                 include_post=True, max_prior_days=365, \n",
    "                                 max_post_days=90):\n",
    "    \"\"\"\n",
    "    Hierarchical temporal matching with preference for prior measurements\n",
    "    \n",
    "    Args:\n",
    "        cohort_df: DataFrame with cohort and time reference column\n",
    "        bmi_df: DataFrame with BMI measurements and date column\n",
    "        time_col: Column name for reference time in cohort_df\n",
    "        include_post: Whether to include post-time_zero measurements\n",
    "        max_prior_days: Maximum days before time_zero to consider\n",
    "        max_post_days: Maximum days after time_zero to consider\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Cohort with matched BMI and timing flags\n",
    "    \"\"\"\n",
    "    # Ensure datetime format\n",
    "    cohort_df = cohort_df.copy()\n",
    "    bmi_df = bmi_df.copy()\n",
    "    \n",
    "    cohort_df[time_col] = pd.to_datetime(cohort_df[time_col]).dt.tz_localize(None)\n",
    "    bmi_df['date'] = pd.to_datetime(bmi_df['date']).dt.tz_localize(None)\n",
    "    \n",
    "    # Merge all possible matches\n",
    "    merged = pd.merge(cohort_df, bmi_df, on='person_id', how='left')\n",
    "    \n",
    "    # Calculate temporal relationships\n",
    "    merged['days_diff'] = (merged['date'] - merged[time_col]).dt.days\n",
    "    merged['abs_days_diff'] = merged['days_diff'].abs()\n",
    "    \n",
    "    # Apply time window filters\n",
    "    if include_post:\n",
    "        valid_measurements = (\n",
    "            (merged['days_diff'] <= 0) & (merged['abs_days_diff'] <= max_prior_days) |\n",
    "            (merged['days_diff'] > 0) & (merged['days_diff'] <= max_post_days)\n",
    "        )\n",
    "    else:\n",
    "        valid_measurements = (\n",
    "            (merged['days_diff'] <= 0) & (merged['abs_days_diff'] <= max_prior_days)\n",
    "        )\n",
    "    \n",
    "    merged = merged[valid_measurements]\n",
    "    \n",
    "    # Hierarchical priority scoring (lower = better)\n",
    "    merged['priority'] = np.where(\n",
    "        merged['days_diff'] <= 0,  # Prior measurements\n",
    "        merged['abs_days_diff'],   # Prefer closer to time_zero\n",
    "        1000 + merged['abs_days_diff']  # Heavily penalize post-time_zero\n",
    "    )\n",
    "    \n",
    "    # Keep best match per person\n",
    "    merged = merged.sort_values(['person_id', 'priority'])\n",
    "    result = merged.drop_duplicates('person_id', keep='first')\n",
    "    \n",
    "    # Add interpretive flags\n",
    "    result['bmi_timing'] = np.where(\n",
    "        result['days_diff'] <= 0, 'prior', 'post'\n",
    "    )\n",
    "    \n",
    "    result['bmi_quality'] = np.where(\n",
    "        result['abs_days_diff'] <= 30, 'excellent',\n",
    "        np.where(result['abs_days_diff'] <= 90, 'good',\n",
    "                np.where(result['abs_days_diff'] <= 180, 'acceptable', 'poor'))\n",
    "    )\n",
    "    \n",
    "    # Select final columns\n",
    "    original_cols = cohort_df.columns.tolist()\n",
    "    bmi_cols = ['date', 'bmi', 'wt_kg', 'ht_cm', 'days_diff', 'bmi_timing', 'bmi_quality']\n",
    "    result = result[original_cols + bmi_cols]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def merge_closest_bmi(cohort_df, bmi_df):\n",
    "    \"\"\"\n",
    "    Simple closest temporal match (legacy function for backward compatibility)\n",
    "    \"\"\"\n",
    "    return hierarchical_temporal_matching(\n",
    "        cohort_df, bmi_df, \n",
    "        time_col='time_zero',\n",
    "        include_post=True,\n",
    "        max_prior_days=365,\n",
    "        max_post_days=365\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with hierarchical temporal matching\n",
    "# Convert BMI data to pandas for temporal matching\n",
    "# bmi_pandas_df = bmi_code_df.to_pandas()\n",
    "\n",
    "# Option 1: Standard hierarchical matching (prefer prior, allow post if no prior available)\n",
    "# merged_cohorts_standard = [\n",
    "#     hierarchical_temporal_matching(df, bmi_pandas_df, time_col='time_zero')\n",
    "#     for df in cohort_dfs\n",
    "# ]\n",
    "\n",
    "# Option 2: Strict prior-only matching (research studies)\n",
    "# merged_cohorts_strict = [\n",
    "#     hierarchical_temporal_matching(\n",
    "#         df, bmi_pandas_df, \n",
    "#         time_col='diagnosis_date',\n",
    "#         include_post=False,\n",
    "#         max_prior_days=180\n",
    "#     )\n",
    "#     for df in cohort_dfs\n",
    "# ]\n",
    "\n",
    "# Option 3: Flexible matching with longer windows\n",
    "# merged_cohorts_flexible = [\n",
    "#     hierarchical_temporal_matching(\n",
    "#         df, bmi_pandas_df,\n",
    "#         time_col='enrollment_date',\n",
    "#         include_post=True,\n",
    "#         max_prior_days=730,  # 2 years prior\n",
    "#         max_post_days=180    # 6 months post\n",
    "#     )\n",
    "#     for df in cohort_dfs\n",
    "# ]\n",
    "\n",
    "print(\"Enhanced temporal matching options defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Complete Pipeline Function\n",
    "\n",
    "Comprehensive function that runs the entire BMI harmonization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_bmi_comprehensive(cohort_dfs, version):\n",
    "    \"\"\"\n",
    "    Complete BMI harmonization pipeline\n",
    "    \n",
    "    Args:\n",
    "        cohort_dfs: List of DataFrames with person_id columns\n",
    "        version: All of Us CDR version (workspace variable)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (harmonized_bmi_df, merged_cohort_dfs)\n",
    "    \"\"\"\n",
    "    print(\"=== Starting BMI Harmonization Pipeline ===\")\n",
    "    \n",
    "    # 1. Extract person IDs\n",
    "    person_id_series = [df['person_id'] for df in cohort_dfs]\n",
    "    combined_person_ids = pd.concat(person_id_series)\n",
    "    unique_person_ids = combined_person_ids.drop_duplicates()\n",
    "    person_ids_str = ', '.join(map(str, unique_person_ids))\n",
    "    \n",
    "    print(f\"Processing {len(unique_person_ids):,} unique persons\")\n",
    "    \n",
    "    # 2. Extract BMI data\n",
    "    query = f\"\"\"\n",
    "    WITH combined AS (\n",
    "        SELECT \n",
    "            person_id, \n",
    "            measurement_date AS date, \n",
    "            MAX(IF(measurement_concept_id IN ({','.join(map(str, WEIGHT_CONCEPTS))}), \n",
    "                value_as_number, NULL)) AS wt,\n",
    "            MAX(IF(measurement_concept_id IN ({','.join(map(str, WEIGHT_CONCEPTS))}), \n",
    "                c2.concept_name, NULL)) AS wt_units,\n",
    "            MAX(IF(measurement_concept_id IN ({','.join(map(str, HEIGHT_CONCEPTS))}), \n",
    "                value_as_number, NULL)) AS ht,\n",
    "            MAX(IF(measurement_concept_id IN ({','.join(map(str, HEIGHT_CONCEPTS))}), \n",
    "                c2.concept_name, NULL)) AS ht_units,\n",
    "            MAX(IF(measurement_concept_id IN ({','.join(map(str, BMI_CONCEPTS))}), \n",
    "                value_as_number, NULL)) AS bmi,\n",
    "            MAX(IF(measurement_concept_id IN ({','.join(map(str, BMI_CONCEPTS))}), \n",
    "                c2.concept_name, NULL)) AS bmi_units\n",
    "        FROM \n",
    "            {version}.measurement m\n",
    "        INNER JOIN \n",
    "            {version}.concept c2 ON unit_concept_id = c2.concept_id\n",
    "        WHERE \n",
    "            measurement_concept_id IN ({','.join(map(str, WEIGHT_CONCEPTS + HEIGHT_CONCEPTS + BMI_CONCEPTS))})\n",
    "            AND person_id IN ({person_ids_str})\n",
    "        GROUP BY \n",
    "            person_id, measurement_date\n",
    "    )\n",
    "    SELECT * FROM combined ORDER BY person_id, date\n",
    "    \"\"\"\n",
    "    \n",
    "    bmi_df = polars_gbq(query)\n",
    "    print(f\"Extracted {len(bmi_df):,} raw measurements\")\n",
    "    \n",
    "    # 3. Clean units\n",
    "    bmi_df = clean_units(bmi_df)\n",
    "    \n",
    "    # 4. Remove outliers\n",
    "    bmi_df = apply_clean_outliers(bmi_df, \"wt\", \"wt_units\")\n",
    "    bmi_df = apply_clean_outliers(bmi_df, \"ht\", \"ht_units\")\n",
    "    bmi_df = apply_clean_outliers(bmi_df, \"bmi\")\n",
    "    \n",
    "    # 5. Convert units\n",
    "    bmi_df = convert_units(bmi_df)\n",
    "    \n",
    "    # 6. Filter and rename\n",
    "    bmi_df = bmi_df.filter(\n",
    "        pl.col(\"wt\").is_not_null() | pl.col(\"ht\").is_not_null() | pl.col(\"bmi\").is_not_null()\n",
    "    )\n",
    "    bmi_df = bmi_df.rename({\"wt\": \"wt_kg\", \"ht\": \"ht_cm\"})\n",
    "    bmi_df = bmi_df.drop([\"wt_units\", \"ht_units\", \"bmi_units\"])\n",
    "    \n",
    "    # 7. Calculate BMI\n",
    "    bmi_df = bmi_df.with_columns(\n",
    "        (pl.col(\"wt_kg\") / (pl.col(\"ht_cm\") / 100) ** 2).alias(\"bmi_calc\")\n",
    "    )\n",
    "    bmi_df = bmi_df.with_columns(\n",
    "        (pl.col(\"bmi\") - pl.col(\"bmi_calc\")).alias(\"bmi_diff\")\n",
    "    )\n",
    "    \n",
    "    # 8. Use calculated BMI where recorded is missing\n",
    "    bmi_df = bmi_df.with_columns(\n",
    "        pl.when(pl.col(\"bmi\").is_null())\n",
    "        .then(pl.col(\"bmi_calc\"))\n",
    "        .otherwise(pl.col(\"bmi\"))\n",
    "        .alias(\"bmi\")\n",
    "    )\n",
    "    \n",
    "    # 9. Final cleanup\n",
    "    bmi_df = bmi_df.select(['person_id', 'date', 'wt_kg', 'ht_cm', 'bmi'])\n",
    "    bmi_df = bmi_df.filter(pl.col('bmi').is_not_null())\n",
    "    bmi_df = bmi_df.sort(\"person_id\", \"date\")\n",
    "    \n",
    "    print(f\"Final harmonized measurements: {len(bmi_df):,}\")\n",
    "    \n",
    "    # 10. Enhanced temporal matching with cohorts\n",
    "    bmi_pandas = bmi_df.to_pandas()\n",
    "    \n",
    "    # Use hierarchical matching (prefer prior, allow post as backup)\n",
    "    merged_cohorts = [\n",
    "        hierarchical_temporal_matching(\n",
    "            df, bmi_pandas,\n",
    "            time_col='time_zero',  # Adjust column name as needed\n",
    "            include_post=True,\n",
    "            max_prior_days=365,\n",
    "            max_post_days=90\n",
    "        ) \n",
    "        for df in cohort_dfs\n",
    "    ]\n",
    "    \n",
    "    print(\"=== BMI Harmonization Complete ===\")\n",
    "    \n",
    "    # Print matching quality summary\n",
    "    total_matched = sum(len(df.dropna(subset=['bmi'])) for df in merged_cohorts)\n",
    "    total_participants = sum(len(df) for df in merged_cohorts)\n",
    "    print(f\"Matching rate: {total_matched/total_participants:.1%}\")\n",
    "    \n",
    "    return bmi_df, merged_cohorts\n",
    "\n",
    "print(\"Complete pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Enhanced Height Carry-Forward\n",
    "\n",
    "Implement longer height carry-forward periods appropriate for adults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_height_carryforward(df, max_carryforward_days=1095):  # 3 years default\n",
    "    \"\"\"\n",
    "    Carry forward height measurements with configurable time limits\n",
    "    Appropriate for adult populations where height changes slowly\n",
    "    \"\"\"\n",
    "    df = df.sort(\"person_id\", \"date\")\n",
    "    \n",
    "    # Fill height forward within person groups\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"ht_cm\").forward_fill().over(\"person_id\").alias(\"ht_cm_filled\")\n",
    "    ])\n",
    "    \n",
    "    # Calculate days since last actual height measurement\n",
    "    height_measurements = df.filter(pl.col(\"ht_cm\").is_not_null())\n",
    "    \n",
    "    # For each row, find days since last height measurement\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"date\").diff().over(\"person_id\").dt.total_days().alias(\"days_since_last\")\n",
    "    ])\n",
    "    \n",
    "    # Use carried forward height only within time limit\n",
    "    df = df.with_columns([\n",
    "        pl.when(\n",
    "            (pl.col(\"ht_cm\").is_null()) & \n",
    "            (pl.col(\"days_since_last\") <= max_carryforward_days)\n",
    "        )\n",
    "        .then(pl.col(\"ht_cm_filled\"))\n",
    "        .otherwise(pl.col(\"ht_cm\"))\n",
    "        .alias(\"ht_cm_final\"),\n",
    "        \n",
    "        # Add source flag\n",
    "        pl.when(pl.col(\"ht_cm\").is_not_null())\n",
    "        .then(pl.lit(\"measured\"))\n",
    "        .when(\n",
    "            (pl.col(\"ht_cm\").is_null()) & \n",
    "            (pl.col(\"days_since_last\") <= max_carryforward_days)\n",
    "        )\n",
    "        .then(pl.lit(\"carried_forward\"))\n",
    "        .otherwise(pl.lit(\"missing\"))\n",
    "        .alias(\"height_source\")\n",
    "    ])\n",
    "    \n",
    "    # Clean up and rename\n",
    "    df = df.drop([\"ht_cm_filled\", \"days_since_last\"])\n",
    "    df = df.rename({\"ht_cm_final\": \"ht_cm\"})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply enhanced height carry-forward to BMI pipeline\n",
    "# bmi_with_height = enhanced_height_carryforward(bmi_code_df, max_carryforward_days=1095)\n",
    "\n",
    "print(\"Enhanced height carry-forward function defined (3-year default for adults)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Temporal Matching Quality Assessment\n",
    "\n",
    "Analyze and visualize the quality of temporal matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_matching_quality(merged_cohorts):\n",
    "    \"\"\"\n",
    "    Analyze the quality of temporal matching across cohorts\n",
    "    \"\"\"\n",
    "    all_matches = pd.concat(merged_cohorts, ignore_index=True)\n",
    "    \n",
    "    summary = {\n",
    "        'total_participants': len(all_matches),\n",
    "        'participants_with_bmi': len(all_matches.dropna(subset=['bmi'])),\n",
    "        'matching_rate': len(all_matches.dropna(subset=['bmi'])) / len(all_matches),\n",
    "        \n",
    "        # Timing distribution\n",
    "        'timing_distribution': all_matches['bmi_timing'].value_counts().to_dict(),\n",
    "        'timing_percentages': all_matches['bmi_timing'].value_counts(normalize=True).to_dict(),\n",
    "        \n",
    "        # Quality distribution  \n",
    "        'quality_distribution': all_matches['bmi_quality'].value_counts().to_dict(),\n",
    "        'quality_percentages': all_matches['bmi_quality'].value_counts(normalize=True).to_dict(),\n",
    "        \n",
    "        # Temporal statistics\n",
    "        'days_diff_stats': {\n",
    "            'mean': all_matches['days_diff'].mean(),\n",
    "            'median': all_matches['days_diff'].median(),\n",
    "            'std': all_matches['days_diff'].std(),\n",
    "            'min': all_matches['days_diff'].min(),\n",
    "            'max': all_matches['days_diff'].max()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def plot_temporal_matching_quality(merged_cohorts, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Create visualizations of temporal matching quality\n",
    "    \"\"\"\n",
    "    all_matches = pd.concat(merged_cohorts, ignore_index=True)\n",
    "    all_matches = all_matches.dropna(subset=['bmi'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Days difference distribution\n",
    "    axes[0].hist(all_matches['days_diff'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(0, color='red', linestyle='--', label='Time Zero')\n",
    "    axes[0].set_xlabel('Days from Time Zero')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('BMI Measurement Timing Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Timing categories\n",
    "    timing_counts = all_matches['bmi_timing'].value_counts()\n",
    "    axes[1].pie(timing_counts.values, labels=timing_counts.index, autopct='%1.1f%%')\n",
    "    axes[1].set_title('Prior vs Post Time Zero')\n",
    "    \n",
    "    # Quality categories\n",
    "    quality_counts = all_matches['bmi_quality'].value_counts()\n",
    "    axes[2].bar(quality_counts.index, quality_counts.values)\n",
    "    axes[2].set_xlabel('Temporal Quality')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].set_title('BMI Measurement Quality')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example quality assessment (uncomment when running with real data)\n",
    "# quality_summary = analyze_temporal_matching_quality(merged_cohorts)\n",
    "# print(f\"Overall matching rate: {quality_summary['matching_rate']:.1%}\")\n",
    "# print(f\"Prior vs Post distribution: {quality_summary['timing_percentages']}\")\n",
    "# print(f\"Quality distribution: {quality_summary['quality_percentages']}\")\n",
    "\n",
    "# plot_temporal_matching_quality(merged_cohorts)\n",
    "\n",
    "print(\"Temporal quality assessment functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Usage Examples with Different Matching Strategies\n",
    "\n",
    "Examples showing how to use different temporal matching approaches for various research scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: GWAS/PheWAS Study (strict prior-only)\n",
    "# Want baseline BMI before any outcomes occur\n",
    "\n",
    "# gwas_cohorts = [\n",
    "#     hierarchical_temporal_matching(\n",
    "#         cohort, bmi_df,\n",
    "#         time_col='enrollment_date',\n",
    "#         include_post=False,         # No post-enrollment BMI\n",
    "#         max_prior_days=365         # Up to 1 year prior\n",
    "#     )\n",
    "#     for cohort in cohort_dfs\n",
    "# ]\n",
    "\n",
    "print(\"Example 1: GWAS study (prior-only BMI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Clinical Outcomes Study (flexible matching)\n",
    "# Allow post-baseline BMI if no prior available, but prefer prior\n",
    "\n",
    "# outcomes_cohorts = [\n",
    "#     hierarchical_temporal_matching(\n",
    "#         cohort, bmi_df,\n",
    "#         time_col='diagnosis_date',\n",
    "#         include_post=True,          # Allow post if needed\n",
    "#         max_prior_days=180,        # 6 months prior\n",
    "#         max_post_days=30           # 1 month post (short window)\n",
    "#     )\n",
    "#     for cohort in cohort_dfs\n",
    "# ]\n",
    "\n",
    "print(\"Example 2: Clinical outcomes (flexible with short post window)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Drug Response Study (baseline characteristics)\n",
    "# Need BMI before drug initiation\n",
    "\n",
    "# drug_cohorts = [\n",
    "#     hierarchical_temporal_matching(\n",
    "#         cohort, bmi_df,\n",
    "#         time_col='drug_start_date',\n",
    "#         include_post=False,         # Strictly prior to drug start\n",
    "#         max_prior_days=90          # Recent baseline (3 months)\n",
    "#     )\n",
    "#     for cohort in cohort_dfs\n",
    "# ]\n",
    "\n",
    "print(\"Example 3: Drug response study (recent prior BMI only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Longitudinal Study (multiple BMI measurements)\n",
    "# Sometimes you want all BMI measurements, not just closest\n",
    "\n",
    "# def get_all_bmi_in_window(cohort_df, bmi_df, time_col='time_zero', \n",
    "#                          window_before=365, window_after=30):\n",
    "#     \"\"\"\n",
    "#     Get all BMI measurements within a time window\n",
    "#     \"\"\"\n",
    "#     merged = pd.merge(cohort_df, bmi_df, on='person_id', how='left')\n",
    "#     merged['days_diff'] = (merged['date'] - merged[time_col]).dt.days\n",
    "#     \n",
    "#     # Filter to window\n",
    "#     in_window = (\n",
    "#         (merged['days_diff'] >= -window_before) & \n",
    "#         (merged['days_diff'] <= window_after)\n",
    "#     )\n",
    "#     \n",
    "#     return merged[in_window].sort_values(['person_id', 'date'])\n",
    "\n",
    "# longitudinal_cohorts = [\n",
    "#     get_all_bmi_in_window(cohort, bmi_df, window_before=730, window_after=90)\n",
    "#     for cohort in cohort_dfs\n",
    "# ]\n",
    "\n",
    "print(\"Example 4: Longitudinal study (all BMI in window)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Updated Complete Pipeline\n",
    "\n",
    "Enhanced pipeline function with all new temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_bmi_enhanced(cohort_dfs, version, matching_strategy='standard',\n",
    "                          time_col='time_zero', **matching_kwargs):\n",
    "    \"\"\"\n",
    "    Enhanced BMI harmonization pipeline with flexible temporal matching\n",
    "    \n",
    "    Args:\n",
    "        cohort_dfs: List of DataFrames with person_id columns\n",
    "        version: All of Us CDR version\n",
    "        matching_strategy: 'standard', 'strict_prior', 'flexible', or 'drug_study'\n",
    "        time_col: Column name for reference time\n",
    "        **matching_kwargs: Additional arguments for hierarchical_temporal_matching\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (harmonized_bmi_df, merged_cohort_dfs, quality_summary)\n",
    "    \"\"\"\n",
    "    print(\"=== Enhanced BMI Harmonization Pipeline ===\")\n",
    "    \n",
    "    # Extract and harmonize BMI data (same as before)\n",
    "    person_id_series = [df['person_id'] for df in cohort_dfs]\n",
    "    combined_person_ids = pd.concat(person_id_series)\n",
    "    unique_person_ids = combined_person_ids.drop_duplicates()\n",
    "    person_ids_str = ', '.join(map(str, unique_person_ids))\n",
    "    \n",
    "    # [Previous extraction and cleaning steps would go here]\n",
    "    # bmi_df = extract_and_clean_bmi_data(person_ids_str, version)\n",
    "    \n",
    "    # Define matching parameters based on strategy\n",
    "    matching_params = {\n",
    "        'standard': {\n",
    "            'include_post': True,\n",
    "            'max_prior_days': 365,\n",
    "            'max_post_days': 90\n",
    "        },\n",
    "        'strict_prior': {\n",
    "            'include_post': False,\n",
    "            'max_prior_days': 365,\n",
    "            'max_post_days': 0\n",
    "        },\n",
    "        'flexible': {\n",
    "            'include_post': True,\n",
    "            'max_prior_days': 730,\n",
    "            'max_post_days': 180\n",
    "        },\n",
    "        'drug_study': {\n",
    "            'include_post': False,\n",
    "            'max_prior_days': 90,\n",
    "            'max_post_days': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Override with user-provided parameters\n",
    "    params = matching_params.get(matching_strategy, matching_params['standard'])\n",
    "    params.update(matching_kwargs)\n",
    "    \n",
    "    print(f\"Using {matching_strategy} matching strategy:\")\n",
    "    print(f\"  Include post: {params['include_post']}\")\n",
    "    print(f\"  Max prior days: {params['max_prior_days']}\")\n",
    "    print(f\"  Max post days: {params['max_post_days']}\")\n",
    "    \n",
    "    # Temporal matching with cohorts\n",
    "    # bmi_pandas = bmi_df.to_pandas()\n",
    "    # merged_cohorts = [\n",
    "    #     hierarchical_temporal_matching(\n",
    "    #         df, bmi_pandas,\n",
    "    #         time_col=time_col,\n",
    "    #         **params\n",
    "    #     )\n",
    "    #     for df in cohort_dfs\n",
    "    # ]\n",
    "    \n",
    "    # Quality assessment\n",
    "    # quality_summary = analyze_temporal_matching_quality(merged_cohorts)\n",
    "    \n",
    "    # print(f\"Overall matching rate: {quality_summary['matching_rate']:.1%}\")\n",
    "    # print(f\"Prior/Post distribution: {quality_summary['timing_percentages']}\")\n",
    "    \n",
    "    # return bmi_df, merged_cohorts, quality_summary\n",
    "    \n",
    "    print(\"Enhanced pipeline function defined\")\n",
    "\n",
    "print(\"Enhanced BMI harmonization pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Usage Examples\n",
    "\n",
    "Put it all together with real usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow example (uncomment and adapt for your data):\n",
    "\n",
    "# # 1. Standard approach for most studies\n",
    "# bmi_data, matched_cohorts, quality = harmonize_bmi_enhanced(\n",
    "#     cohort_dfs, \n",
    "#     version, \n",
    "#     matching_strategy='standard',\n",
    "#     time_col='enrollment_date'\n",
    "# )\n",
    "\n",
    "# # 2. Strict approach for genetic studies\n",
    "# bmi_data_strict, matched_strict, quality_strict = harmonize_bmi_enhanced(\n",
    "#     cohort_dfs, \n",
    "#     version, \n",
    "#     matching_strategy='strict_prior',\n",
    "#     time_col='diagnosis_date'\n",
    "# )\n",
    "\n",
    "# # 3. Custom approach\n",
    "# bmi_data_custom, matched_custom, quality_custom = harmonize_bmi_enhanced(\n",
    "#     cohort_dfs, \n",
    "#     version, \n",
    "#     matching_strategy='standard',\n",
    "#     time_col='surgery_date',\n",
    "#     max_prior_days=60,     # Override: only 2 months prior\n",
    "#     max_post_days=0       # Override: no post-surgery BMI\n",
    "# )\n",
    "\n",
    "# # 4. Quality assessment and visualization\n",
    "# plot_temporal_matching_quality(matched_cohorts)\n",
    "\n",
    "# # 5. Export with quality flags\n",
    "# for i, cohort in enumerate(matched_cohorts):\n",
    "#     cohort.to_csv(f'{my_bucket}/cohort_{i}_with_enhanced_bmi.csv', index=False)\n",
    "\n",
    "print(\"Complete workflow examples provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Enhanced Features\n",
    "\n",
    "This enhanced notebook now includes:\n",
    "\n",
    "### ðŸŽ¯ **Hierarchical Temporal Matching**\n",
    "- **Prefers prior measurements** (before time_zero)\n",
    "- **Configurable post-inclusion** (optional)\n",
    "- **Quality flags**: excellent/good/acceptable/poor\n",
    "- **Timing flags**: prior/post time_zero\n",
    "\n",
    "### ðŸ“Š **Predefined Strategies**\n",
    "- **Standard**: Flexible matching for most studies\n",
    "- **Strict Prior**: GWAS/genetic studies (no post-time_zero)\n",
    "- **Flexible**: Longitudinal studies (longer windows)\n",
    "- **Drug Study**: Recent baseline only\n",
    "\n",
    "### ðŸ”§ **Enhanced Height Handling**\n",
    "- **3-year carry-forward** for adult populations\n",
    "- **Source tracking**: measured vs carried_forward\n",
    "- **Configurable time limits**\n",
    "\n",
    "### ðŸ“ˆ **Quality Assessment**\n",
    "- **Matching rate analysis**\n",
    "- **Temporal distribution plots**\n",
    "- **Prior/post breakdowns**\n",
    "- **Quality score distributions**\n",
    "\n",
    "### ðŸ”¬ **Research Flexibility**\n",
    "- **Multiple time column support**\n",
    "- **Custom time windows**\n",
    "- **Strategy overrides**\n",
    "- **Comprehensive documentation**\n",
    "\n",
    "### ðŸ’¡ **Key Advantages**\n",
    "1. **Methodologically sound**: Prefers temporally appropriate measurements\n",
    "2. **Transparent**: Clear flags for how BMI was matched\n",
    "3. **Flexible**: Adapts to different study designs\n",
    "4. **Quality-focused**: Built-in assessment and visualization\n",
    "5. **Reproducible**: Consistent methodology across studies\n",
    "\n",
    "This approach should provide much better temporal matching for your All of Us research while maintaining the sophisticated quality control you've already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
