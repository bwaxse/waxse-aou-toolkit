{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "src_bucket = '{bucket}'  # Reference files (Huan Mo)\n",
    "henry_bucket = '{bucket}' # Original SAIGE implementation (Henry) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "henry_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries_considered = ['eur', 'afr', 'amr', 'eas', 'sas']\n",
    "\n",
    "# GWAS info\n",
    "traits = {\n",
    "    'condition__hpv': 'binary'\n",
    "}\n",
    "covariates = ['imputed_sex', 'age_at_last_ehr'] + ['ancPC{}'.format(str(x)) for x in range(1, 21)]\n",
    "covariates_discrete = []\n",
    "\n",
    "# Columns to manipulate\n",
    "covariates_binarize = ['imputed_sex::F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output folders\n",
    "output_folder = f'{my_bucket}/saige_gwas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Henry's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil cp {henry_bucket}/data/cohort_metadata__pcs__phenotypes.tsv.gz {src_bucket}/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For stg303 data, Henry's folders contain many files/results that we don't need\n",
    "# ! gsutil ls {henry_bucket}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## my other bucket with useful data from Henry\n",
    "# ! gsutil -m cp -r {bucket}/data/stg303 {src_bucket}/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {src_bucket}/data/stg303/eur/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {henry_bucket}/data/stg105/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil ls {henry_bucket}/data/stg105/eur/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil -m cp -r {henry_bucket}/data/stg105/ {src_bucket}/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This folders are the reference folders that we need to run\n",
    "! gsutil ls {src_bucket}/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpv_df = pd.read_csv('hpv_gwas_cohort.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(query_dir):\n",
    "    tmp = subprocess.run(\n",
    "        f'gsutil ls {query_dir}',\n",
    "        shell=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    files = tmp.stdout.decode('utf-8').split('\\n')\n",
    "    return(files)\n",
    "\n",
    "def dsub_script(\n",
    "    machine_type,\n",
    "    envs,\n",
    "    in_params,\n",
    "    out_params,\n",
    "    boot_disk = 100,\n",
    "    disk_size = 150,\n",
    "    image = 'us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "    script = 'run_impute.sh',\n",
    "    preemptible = True\n",
    "):\n",
    "    \n",
    "    # get useful info\n",
    "    dsub_user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0].replace('.','-')\n",
    "\n",
    "    \n",
    "    dsub_cmd = 'dsub '\n",
    "    dsub_cmd += '--provider google-cls-v2 '\n",
    "    dsub_cmd += '--machine-type \"{}\" '.format(machine_type)\n",
    "    \n",
    "    if preemptible:\n",
    "        dsub_cmd += '--preemptible '\n",
    "        \n",
    "    if 'c4' in machine_type:\n",
    "        # c4 doesn't use pd-ssd\n",
    "        dsub_cmd += '--disk-type \"hyperdisk-balanced\" '\n",
    "    else:\n",
    "        dsub_cmd += '--disk-type \"pd-ssd\" '\n",
    "        \n",
    "    dsub_cmd += '--boot-disk-size {} '.format(boot_disk)\n",
    "    dsub_cmd += '--disk-size {} '.format(disk_size)\n",
    "    dsub_cmd += '--user-project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--image \"{}\" '.format(image)\n",
    "    dsub_cmd += '--network \"network\" '\n",
    "    dsub_cmd += '--subnetwork \"subnetwork\" '\n",
    "    dsub_cmd += '--service-account \"$(gcloud config get-value account)\" '\n",
    "    dsub_cmd += '--user \"{}\" '.format(dsub_user_name)\n",
    "    dsub_cmd += '--logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +\\'%Y%m%d\\')/{job-id}-{task-id}-{task-attempt}.log\" '\n",
    "    dsub_cmd += ' \"$@\" '\n",
    "    dsub_cmd += '--name \"{}\" '.format(machine_type)\n",
    "    dsub_cmd += '--env GOOGLE_PROJECT=\"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--script \"{}\" '.format(script)\n",
    "    \n",
    "    # Assign any environmental conditions\n",
    "    for env_key in envs.keys():\n",
    "        dsub_cmd += '--env {}=\"{}\" '.format(env_key, envs[env_key])\n",
    "        \n",
    "    # Assign any inputs\n",
    "    for in_key in in_params.keys():\n",
    "        dsub_cmd += '--input {}=\"{}\" '.format(in_key, in_params[in_key])\n",
    "        \n",
    "    # Assign any outputs\n",
    "    for out_key in out_params.keys():\n",
    "        dsub_cmd += '--output {}=\"{}\" '.format(out_key, out_params[out_key])\n",
    "        \n",
    "    os.system(dsub_cmd)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dsub_status(user=None, full=False):\n",
    "    \"\"\"Check status of dsub jobs for the specified user\"\"\"\n",
    "    if user is None:\n",
    "        # Get current user if not specified\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "\n",
    "    if full:\n",
    "        make_full = ' --full'\n",
    "    else:\n",
    "        make_full = ''\n",
    "    \n",
    "    cmd = f\"dstat --provider google-cls-v2 --user {user} --status '*' --project {project}{make_full}\"\n",
    "    # cmd = f\"ddel --provider google-cls-v2 --project terra-vpc-sc-840afe1e --location us-central1 --jobs 'transances--bwaxse--250319-022343-75' --users 'bwaxse'\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge pruned variants for step1 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile merge_pruned_genotypes.sh\n",
    "\n",
    "# #!/bin/bash\n",
    "\n",
    "# in_base=$(echo $INPUT_CHR1_BED | sed 's/chr1.bed//g');\n",
    "# out_base=$(echo $OUTPUT_BED | sed 's/.bed//g');\n",
    "\n",
    "# merge_lst='merge_input_beds.txt';\n",
    "# touch $merge_lst;\n",
    "# for i in ${in_base}*.bed; do\n",
    "#     echo $i | sed 's/.bed//g' >> $merge_lst; \n",
    "# done;\n",
    "\n",
    "# plink2 \\\n",
    "#     --pmerge-list merge_input_beds.txt bfile \\\n",
    "#     --indiv-sort none \\\n",
    "#     --delete-pmerge-result \\\n",
    "#     --remove $REF_SAMPLES \\\n",
    "#     --make-bed \\\n",
    "#     --out $out_base;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_bed_files(\n",
    "#     anc,\n",
    "#     script,\n",
    "#     chroms = list(range(1,23))\n",
    "# ):\n",
    "    \n",
    "#     # get base files\n",
    "#     mb = os.getenv('WORKSPACE_BUCKET')\n",
    "#     in_base = '{}/data/stg201/pruned_genotypes/{}/genotypes_chr{{}}'.format(mb, anc)\n",
    "    \n",
    "#     # base out\n",
    "#     out_dir = '{}/data/stg303/{}'.format(mb, anc)\n",
    "    \n",
    "#     in_dict = {'REF_SAMPLES': '{}/data/stg105/{}/reference_samples.txt'.format(mb, anc)}\n",
    "#     for chrom in chroms:\n",
    "#         in_dict['INPUT_CHR{}_BED'.format(chrom)] = in_base.format(chrom) + '.bed'\n",
    "#         in_dict['INPUT_CHR{}_BIM'.format(chrom)] = in_base.format(chrom) + '.bim'\n",
    "#         in_dict['INPUT_CHR{}_FAM'.format(chrom)] = in_base.format(chrom) + '.fam'\n",
    "    \n",
    "#     env_dict = {}\n",
    "#     out_dict = {\n",
    "#         'OUTPUT_BED': '{}/pruned_genotypes.bed'.format(out_dir),\n",
    "#         'OUTPUT_BIM': '{}/pruned_genotypes.bim'.format(out_dir),\n",
    "#         'OUTPUT_PED': '{}/pruned_genotypes.fam'.format(out_dir)\n",
    "#     }\n",
    "            \n",
    "#     dsub_script(\n",
    "#         machine_type = 'c4-standard-8',\n",
    "#         envs = env_dict,\n",
    "#         in_params = in_dict,\n",
    "#         out_params = out_dict,\n",
    "#         boot_disk = 100,\n",
    "#         disk_size = 150,\n",
    "#         image = 'us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "#         script = script,\n",
    "#         preemptible = True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for anc in ancestries_considered:\n",
    "#     _ = merge_bed_files(\n",
    "#         anc,\n",
    "#         'merge_pruned_genotypes.sh',\n",
    "#         chroms = list(range(1,23))\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_columns(df, col_refs):\n",
    "    # Iterate through each column, binarize it. Col_ref = trait1::ref\n",
    "    for col_ref in col_refs:\n",
    "        col = col_ref.split('::')[0]\n",
    "        ref = col_ref.split('::')[1]\n",
    "\n",
    "        col_vals = df[col].dropna().unique()\n",
    "        if ((len(col_vals) > 2) and (ref in col_vals)):\n",
    "            print('The term {} contains >2 levels. Skipping binarize...'.format(\n",
    "                col\n",
    "            ))\n",
    "            continue\n",
    "        elif (ref not in col_vals):\n",
    "            raise Exception('The reference {} is not a level for term {}. Please fix!'.format(\n",
    "                ref, col\n",
    "            ))\n",
    "\n",
    "        col_alt = np.setdiff1d(col_vals, [ref]).tolist()[0]\n",
    "        val_map = {ref: 0, col_alt: 1}\n",
    "        df[col] = [ val_map[x] if not pd.isna(x) else x for x in df[col] ]\n",
    "    return df\n",
    "\n",
    "def _rank_inverse_normalize(data_ser, c=3.0/8):\n",
    "    '''\n",
    "    Inverse rank-normalize phenotype. Takes Series.\n",
    "    '''\n",
    "    # Shuffle by index\n",
    "    orig_idx = data_ser.index\n",
    "\n",
    "    data_ser = data_ser.loc[~pd.isnull(data_ser)]\n",
    "    alg_input = data_ser.loc[np.random.permutation(data_ser.index.tolist())].copy()\n",
    "\n",
    "    # Get rank, ties are determined by their position in the series (hence\n",
    "    # why we randomised the series)\n",
    "    rank = ss.rankdata(alg_input, method='ordinal')\n",
    "    rank = pd.Series(rank, index=alg_input.index)\n",
    "\n",
    "    # Convert rank to normal distribution\n",
    "    norm_ser = rank.apply(\n",
    "        lambda x, c, n: ss.norm.ppf((x - c) / (n - 2*c + 1)),\n",
    "        n=len(rank),\n",
    "        c=c\n",
    "    )\n",
    "    final = pd.Series(\n",
    "        [ norm_ser[x] if x in norm_ser.index else pd.NA for x in orig_idx ],\n",
    "        index=orig_idx\n",
    "    )\n",
    "    return(final)\n",
    "\n",
    "def _zscore_standardize(data_ser):\n",
    "    '''\n",
    "    Standardize the phenotype using Z-score standardization. Takes Series.\n",
    "    '''\n",
    "    data_ser -= data_ser.mean(skipna=True)\n",
    "    data_ser /= data_ser.std(skipna=True)\n",
    "    return data_ser\n",
    "\n",
    "\n",
    "def normalize_columns(df, col_method_dict):\n",
    "    # Iterate through each column, normalize\n",
    "    for col in col_method_dict.keys():\n",
    "        df['{}__untransformed'.format(col)] = df[col]\n",
    "\n",
    "        col_vals = df[col]\n",
    "        method = col_method_dict[col]\n",
    "        \n",
    "        if method == 'inv_rank_norm':\n",
    "            col_vals = _rank_inverse_normalize(col_vals, 3.0/8)\n",
    "        elif method == 'standardize':\n",
    "            col_vals = _zscore_standardize(col_vals)\n",
    "        else:\n",
    "            print('Incorrect normalization method `{}`. Skipping...'.format(method))\n",
    "\n",
    "        df[col] = col_vals\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This metadata file contains both demographic covariates \n",
    "## and examples of phenotypes of interest provided by Henry\n",
    "## You can join this dataframe to your phenotype tables\n",
    "\n",
    "metadata_file = f'{src_bucket}/data/cohort_metadata__pcs__phenotypes.tsv.gz'\n",
    "\n",
    "metadata = pd.read_csv(\n",
    "    metadata_file,\n",
    "    sep='\\t',\n",
    "    header=0,\n",
    "    dtype={'person_id' : str }\n",
    ")\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpv_df[['person_id', 'case']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an explicit copy of the filtered dataframe\n",
    "hpv_df_copy = hpv_df.copy()\n",
    "\n",
    "# Convert person_id to string\n",
    "hpv_df_copy['person_id'] = hpv_df_copy['person_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.merge(\n",
    "    hpv_df_copy[['person_id', 'case']].rename(columns={'case': 'condition__hpv'}),\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby('ancestry_pred', as_index = False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby('condition__hpv', as_index = False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metadata[metadata['condition__hpv'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anc in ancestries_considered:\n",
    "    anc_metadata = metadata[metadata['ancestry_pred_other'] == anc]\n",
    "    \n",
    "    # Manipulate any columns\n",
    "    # some abnormal imputed sex -- limit to M or F\n",
    "    # this will make n of metadata < genotypes but saige can handle just fine\n",
    "    anc_metadata = anc_metadata[anc_metadata.imputed_sex.isin(['M', 'F'])]\n",
    "    \n",
    "    anc_metadata = binarize_columns(anc_metadata, covariates_binarize)\n",
    "#     anc_metadata = normalize_columns(anc_metadata, covariates_normalize)\n",
    "    \n",
    "    # write\n",
    "    anc_metadata.to_csv(\n",
    "        f'{output_folder}/{anc}/gwas_metadata.tsv',\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        header=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {output_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_df = pd.read_csv(f'{output_folder}/amr/gwas_metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_df.groupby('condition__hpv', as_index=False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_df = pd.read_csv(f'{output_folder}/eur/gwas_metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_df.groupby('condition__hpv', as_index=False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afr_df = pd.read_csv(f'{output_folder}/afr/gwas_metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afr_df.groupby('condition__hpv', as_index=False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_df = pd.read_csv(f'{output_folder}/sas/gwas_metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_df.groupby('condition__hpv', as_index=False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eas_df = pd.read_csv(f'{output_folder}/eas/gwas_metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eas_df.groupby('condition__hpv', as_index=False).agg({'person_id' : 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_saige_null_model.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "in_base=$(echo $INPUT_BED | sed 's/.bed//g');\n",
    "out_base=$(echo $OUTPUT_NULL_RDA | sed 's/.rda//g');\n",
    "\n",
    "step1_fitNULLGLMM.R \\\n",
    "    --plinkFile=${in_base} \\\n",
    "    --phenoFile=${INPUT_METADATA} \\\n",
    "    --phenoCol=${TRAIT} \\\n",
    "    --covarColList=${COVARIATES} \\\n",
    "    --qCovarColList=${COVARIATES_DISCRETE} \\\n",
    "    --sampleIDColinphenoFile=person_id \\\n",
    "    --traitType=${TRAIT_TYPE} \\\n",
    "    --invNormalize=FALSE \\\n",
    "    --nThreads=${THREADS} \\\n",
    "    --IsOverwriteVarianceRatioFile=TRUE \\\n",
    "    --skipVarianceRatioEstimation=FALSE \\\n",
    "    --outputPrefix=${out_base} \\\n",
    "    --useSparseGRMtoFitNULL=FALSE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_saige_null(\n",
    "    anc,\n",
    "    trait,\n",
    "    trait_type,\n",
    "    covs,\n",
    "    covs_discrete,\n",
    "    script\n",
    "):\n",
    "    # get base files\n",
    "\n",
    "    in_base = f'{output_folder}/{anc}'\n",
    "    ref_base = f'{src_bucket}/data/stg303/{anc}'\n",
    "    out_dir = f'{output_folder}/{anc}/{trait}'\n",
    "    \n",
    "    env_dict = {\n",
    "        'TRAIT': trait,\n",
    "        'TRAIT_TYPE': trait_type,\n",
    "        'COVARIATES': ','.join(covs),\n",
    "        'COVARIATES_DISCRETE': ','.join(covs_discrete),\n",
    "        'THREADS': 8\n",
    "    }\n",
    "    \n",
    "    in_dict = {\n",
    "        'INPUT_BED': f'{ref_base}/pruned_genotypes.bed',\n",
    "        'INPUT_BIM': f'{ref_base}/pruned_genotypes.bim',\n",
    "        'INPUT_FAM': f'{ref_base}/pruned_genotypes.fam',\n",
    "        'INPUT_METADATA': f'{in_base}/gwas_metadata.tsv'\n",
    "    }\n",
    "    \n",
    "    out_dict = {\n",
    "        'OUTPUT_NULL_RDA': f'{out_dir}/saige_null_model.rda',\n",
    "        'OUTPUT_NULL_VARRAT': f'{out_dir}/saige_null_model.varianceRatio.txt'\n",
    "    }\n",
    "            \n",
    "    dsub_script(\n",
    "        machine_type = 'c4-standard-8',\n",
    "        envs = env_dict,\n",
    "        in_params = in_dict,\n",
    "        out_params = out_dict,\n",
    "        boot_disk = 100,\n",
    "        disk_size = 150,\n",
    "        image = 'wzhou88/saige:1.3.6',\n",
    "        script = script,\n",
    "        preemptible = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anc in ['eur', 'afr', 'amr', 'eas', 'sas']:\n",
    "    for trait in traits.keys():\n",
    "        _ = run_saige_null(\n",
    "            anc,\n",
    "            trait,\n",
    "            traits[trait],\n",
    "            covariates,\n",
    "            covariates_discrete,\n",
    "            'run_saige_null_model.sh'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsub_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {my_bucket}/dsub/logs/c4-standard-8/bwaxse/20250514/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SAIGE hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_saige_chrom.sh\n",
    "\n",
    "#!/bin/bash\n",
    "step2_SPAtests.R \\\n",
    "    --vcfFile=\"${INPUT_VCF}\" \\\n",
    "    --vcfFileIndex=\"${INPUT_VCF_IX}\" \\\n",
    "    --vcfField=\"DS\" \\\n",
    "    --chrom=\"${CHR}\" \\\n",
    "    --is_imputed_data=\"TRUE\" \\\n",
    "    --AlleleOrder=\"alt-first\" \\\n",
    "    --GMMATmodelFile=\"${INPUT_NULL_RDA}\" \\\n",
    "    --varianceRatioFile=\"${INPUT_NULL_VARRAT}\" \\\n",
    "    --is_Firth_beta=\"TRUE\" \\\n",
    "    --pCutoffforFirth=\"0.05\" \\\n",
    "    --minMAC=20 \\\n",
    "    --is_output_moreDetails=\"TRUE\" \\\n",
    "    --SAIGEOutputFile=\"${OUTPUT_FILE}\" \\\n",
    "    --LOCO=\"TRUE\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_saige_test(\n",
    "    anc,\n",
    "    trait,\n",
    "    script,\n",
    "    chroms = range(1, 23)\n",
    "):\n",
    "    # get base files\n",
    "    mb = os.getenv('WORKSPACE_BUCKET')\n",
    "    vcf_in_base = f'{src_bucket}/data/stg105/{anc}/targimp_genotypes_chr{{}}.vcf.gz'\n",
    "    nm_in_base = f'{output_folder}/{anc}/{trait}'\n",
    "    out_dir = f'{output_folder}/{anc}/{trait}/swarm_gwas'\n",
    "    \n",
    "    # get existing files to avoid repeat work\n",
    "    efs = [ x.split('/')[-1].replace('.txt', '') for x in get_file_list(out_dir) if x.endswith('.txt') ]\n",
    "    \n",
    "    for chrom in chroms:\n",
    "        if 'gwas_results_chr{}'.format(chrom) not in efs:\n",
    "            env_dict = {'CHR': chrom}\n",
    "\n",
    "            in_dict = {\n",
    "                'INPUT_VCF': vcf_in_base.format(str(chrom)),\n",
    "                'INPUT_VCF_IX': vcf_in_base.format(str(chrom)) + '.csi',\n",
    "                'INPUT_NULL_RDA': f'{nm_in_base}/saige_null_model.rda',\n",
    "                'INPUT_NULL_VARRAT': f'{nm_in_base}/saige_null_model.varianceRatio.txt'\n",
    "            }\n",
    "\n",
    "            out_dict = {\n",
    "                'OUTPUT_FILE': f'{out_dir}/gwas_results_chr{chrom}.txt',\n",
    "                'OUTPUT_FILE_INDEX': f'{out_dir}/gwas_results_chr{chrom}.txt.index'\n",
    "            }\n",
    "\n",
    "            dsub_script(\n",
    "                machine_type = 'c4-standard-8',\n",
    "                envs = env_dict,\n",
    "                in_params = in_dict,\n",
    "                out_params = out_dict,\n",
    "                boot_disk = 100,\n",
    "                disk_size = 150,\n",
    "                image = 'wzhou88/saige:1.3.6',\n",
    "                script = script,\n",
    "                preemptible = True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anc in ['eur', 'afr', 'amr', 'sas', 'eas']:\n",
    "    for trait in traits.keys():\n",
    "        _ = run_saige_test(\n",
    "            anc,\n",
    "            trait,\n",
    "            'run_saige_chrom.sh',\n",
    "            chroms = range(1, 23)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check All Statuses\n",
    "check_dsub_status(full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dstat --provider google-cls-v2 --project terra-vpc-sc-e05b4e1b --location us-central1 \\\n",
    "--jobs 'c4-standar--bwaxse--250506-190742-82' --users 'bwaxse' --status '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {my_bucket}/saige_gwas/afr/condition__hpv/swarm_gwas/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
