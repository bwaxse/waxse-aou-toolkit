{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy, scipy\n",
    "import plotnine as plt9\n",
    "import copy\n",
    "import re\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(query_dir):\n",
    "    tmp = subprocess.run(\n",
    "        f'gsutil ls {query_dir}',\n",
    "        shell=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    files = tmp.stdout.decode('utf-8').split('\\n')\n",
    "    return(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_age_format(age):\n",
    "    \"\"\"\n",
    "    Validate age format for dsub dstat command\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    age : str\n",
    "        Age string to validate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if format is valid, False otherwise\n",
    "    \"\"\"\n",
    "    # Pattern: one or more digits followed by exactly one valid unit\n",
    "    pattern = r'^\\d+[smhdw]$'\n",
    "    return bool(re.match(pattern, age.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsub_script(\n",
    "    label,\n",
    "    machine_type,\n",
    "    envs,\n",
    "    in_params,\n",
    "    out_params,\n",
    "    boot_disk = 100,\n",
    "    disk_size = 150,\n",
    "    image = 'us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "    script = 'pgen_subset_multiancestry.sh',\n",
    "    preemptible = True\n",
    "):\n",
    "    \n",
    "    # get useful info\n",
    "    dsub_user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0].replace('.','-')\n",
    "\n",
    "    job_name = f'{label}'\n",
    "    \n",
    "    dsub_cmd = 'dsub '\n",
    "    dsub_cmd += '--provider google-cls-v2 '\n",
    "    dsub_cmd += '--machine-type \"{}\" '.format(machine_type)\n",
    "    \n",
    "    if preemptible:\n",
    "        dsub_cmd += '--preemptible '\n",
    "        \n",
    "    if 'c4' in machine_type:\n",
    "        dsub_cmd += '--disk-type \"hyperdisk-balanced\" '\n",
    "    else:\n",
    "        dsub_cmd += '--disk-type \"pd-ssd\" '\n",
    "        \n",
    "    dsub_cmd += '--boot-disk-size {} '.format(boot_disk)\n",
    "    dsub_cmd += '--disk-size {} '.format(disk_size)\n",
    "    dsub_cmd += '--user-project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--project \"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--image \"{}\" '.format(image)\n",
    "    dsub_cmd += '--network \"network\" '\n",
    "    dsub_cmd += '--subnetwork \"subnetwork\" '\n",
    "    dsub_cmd += '--service-account \"$(gcloud config get-value account)\" '\n",
    "    dsub_cmd += '--user \"{}\" '.format(dsub_user_name)\n",
    "    dsub_cmd += '--logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +\\'%Y%m%d\\')/{job-id}-{task-id}-{task-attempt}.log\" '\n",
    "    dsub_cmd += ' \"$@\" '\n",
    "    dsub_cmd += '--name \"{}\" '.format(job_name)\n",
    "    dsub_cmd += '--env GOOGLE_PROJECT=\"${GOOGLE_PROJECT}\" '\n",
    "    dsub_cmd += '--script \"{}\" '.format(script)\n",
    "    \n",
    "    # Assign any environmental conditions\n",
    "    for env_key in envs.keys():\n",
    "        dsub_cmd += '--env {}=\"{}\" '.format(env_key, envs[env_key])\n",
    "        \n",
    "    # Assign any inputs\n",
    "    for in_key in in_params.keys():\n",
    "        dsub_cmd += '--input {}=\"{}\" '.format(in_key, in_params[in_key])\n",
    "        \n",
    "    # Assign any outputs\n",
    "    for out_key in out_params.keys():\n",
    "        dsub_cmd += '--output {}=\"{}\" '.format(out_key, out_params[out_key])\n",
    "        \n",
    "    os.system(dsub_cmd)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dsub_status(user=None, full=False, age='1d'):\n",
    "    \"\"\"\n",
    "    Check status of dsub jobs for the specified user\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user : str, optional\n",
    "        Username to check jobs for. Defaults to current user from OWNER_EMAIL\n",
    "    full : bool, default False\n",
    "        Include full job details in output\n",
    "    age : str, optional\n",
    "        Maximum age of jobs to display. Format: <integer><unit>\n",
    "        Units: s (seconds), m (minutes), h (hours), d (days), w (weeks)\n",
    "        Examples: '3d', '12h', '30m', '7w'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    subprocess.CompletedProcess\n",
    "        Result of the dstat command\n",
    "\n",
    "    \"\"\"\n",
    "    if user is None:\n",
    "        # Get current user if not specified\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "\n",
    "    # Validate age parameter if provided\n",
    "    if age is not None:\n",
    "        if not _validate_age_format(age):\n",
    "            raise ValueError(\n",
    "                f\"Invalid age format: '{age}'. \"\n",
    "                \"Expected format: <integer><unit> where unit is one of: s, m, h, d, w. \"\n",
    "                \"Examples: '3d', '12h', '30m', '7w'\"\n",
    "            )\n",
    "    # Build command\n",
    "    cmd_parts = [\n",
    "        \"dstat\",\n",
    "        \"--provider google-cls-v2\",\n",
    "        f\"--user {user}\",\n",
    "        \"--status '*'\",\n",
    "        f\"--project {project}\"\n",
    "    ]\n",
    "\n",
    "    if full:\n",
    "        cmd_parts.append(\"--full\")\n",
    "    \n",
    "    if age:\n",
    "        cmd_parts.append(f\"--age {age}\")\n",
    "    \n",
    "    cmd = \" \".join(cmd_parts)\n",
    "    print(f\"Running: {cmd}\")\n",
    "\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_details(user=None, job=None):\n",
    "    \"\"\"List all jobs for the user, including failed ones\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    if user is None:\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "        \n",
    "    if job is None:\n",
    "        job = \"'*' \"\n",
    "    else:\n",
    "        job = f'--jobs {job} '\n",
    "    \n",
    "    cmd = f\"dstat --provider google-cls-v2 --project {project} --user {user} --status {job}--full\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_job(job_id):\n",
    "    \"\"\"Cancel a specific job\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    cmd = f\"ddel --provider google-cls-v2 --project {project} --jobs {job_id}\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_running_jobs():\n",
    "    \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Cancel only running jobs\n",
    "    cancel_cmd = f\"ddel --provider google-cls-v2 --project {project} --users 'bwaxse' --jobs '*'\"\n",
    "    print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "    \n",
    "    return subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chromosome_list():\n",
    "    \"\"\"Return list of all chromosomes including X and Y\"\"\"\n",
    "    return list(range(1, 23)) + ['X', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dsub for pgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgen_subset_multiancestry.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Subset whole-cohort pgen by multiple ancestries\n",
    "# Input: whole cohort pgen files + sample files for each ancestry\n",
    "# Output: ancestry-specific pgen files for SAIGE input\n",
    "\n",
    "INPUT_PGEN_BASE=\"${INPUT_PGEN_PGEN%.*}\"  # Remove .pgen extension\n",
    "echo \"Derived INPUT_PGEN_BASE: $INPUT_PGEN_BASE\"\n",
    "\n",
    "nthread=$(python -c \"import os; print(len(os.sched_getaffinity(0)))\");\n",
    "echo \"Running with $nthread threads\";\n",
    "\n",
    "# Parse comma-separated ancestries\n",
    "IFS=',' read -ra ANCS <<< \"$ANCESTRIES\"\n",
    "\n",
    "echo \"=== Processing ancestry-specific subsets ===\"\n",
    "for anc in \"${ANCS[@]}\"; do\n",
    "    echo \"Processing ancestry: $anc\"\n",
    "    \n",
    "    # Get ancestry-specific inputs using variable indirection\n",
    "    sample_var=\"SAMPLE_FILE_${anc}\"\n",
    "    output_var=\"OUTPUT_PREFIX_${anc}\"\n",
    "    output_with_star=\"${!output_var}\"\n",
    "    output_prefix=\"${output_with_star%\\*}\"\n",
    "    \n",
    "    echo \"Subsetting pgen for ancestry $anc with samples: ${!sample_var}\"\n",
    "    echo \"Output prefix: $output_prefix\"\n",
    "    \n",
    "    # Ancestry-specific filtering\n",
    "    # FILTER='PASS' & HWE>1e-10 & F_MISSING<0.05\n",
    "    # biallelic snps only\n",
    "    plink2 \\\n",
    "        --pfile $INPUT_PGEN_BASE \\\n",
    "        --keep ${!sample_var} \\\n",
    "        --min-af ${MAF}:minor \\\n",
    "        --var-filter \\\n",
    "        --hwe ${HWE_PVAL} \\\n",
    "        --geno ${MISSING_RATE} \\\n",
    "        --min-alleles 2 \\\n",
    "        --max-alleles 2 \\\n",
    "        --make-bed \\ #--make-pgen \\\n",
    "        --memory 100000 \\\n",
    "        --threads $nthread \\\n",
    "        --out $output_prefix\n",
    "    \n",
    "    echo \"Ancestry-specific pgen complete for $anc\"\n",
    "    echo \"Files created with prefix: $output_prefix\"\n",
    "    ls -la ${output_prefix}*\n",
    "    echo \"---\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pgen_whole_cohort.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Subset whole-cohort pgen for entire cohort\n",
    "# Input: whole cohort pgen files + sample file for all individuals\n",
    "# Output: whole-cohort pgen files for trans-ancestry analyses\n",
    "\n",
    "INPUT_PGEN_BASE=\"${INPUT_PGEN_PGEN%.*}\"  # Remove .pgen extension\n",
    "echo \"Derived INPUT_PGEN_BASE: $INPUT_PGEN_BASE\"\n",
    "\n",
    "nthread=$(python -c \"import os; print(len(os.sched_getaffinity(0)))\");\n",
    "echo \"Running with $nthread threads\";\n",
    "\n",
    "echo \"=== Processing whole-cohort subset ===\"\n",
    "echo \"Subsetting whole cohort with samples: $SAMPLE_FILE_WHOLE_COHORT\"\n",
    "\n",
    "OUTPUT_PREFIX=\"${OUTPUT_PREFIX_WHOLE_COHORT%\\*}\"\n",
    "echo \"Output prefix: $OUTPUT_PREFIX\"\n",
    "\n",
    "# Whole-cohort filtering\n",
    "# FILTER='PASS' & F_MISSING<0.05\n",
    "# biallelic snps only\n",
    "plink2 \\\n",
    "    --pfile $INPUT_PGEN_BASE \\\n",
    "    --keep $SAMPLE_FILE_WHOLE_COHORT \\\n",
    "    --min-af ${MAF}:minor \\\n",
    "    --var-filter \\\n",
    "    --geno ${MISSING_RATE} \\\n",
    "    --min-alleles 2 \\\n",
    "    --max-alleles 2 \\\n",
    "    --make-bed \\ #--make-pgen \\\n",
    "    --memory 100000 \\\n",
    "    --threads $nthread \\\n",
    "    --out $OUTPUT_PREFIX\n",
    "    \n",
    "echo \"Whole-cohort pgen complete\"\n",
    "echo \"Files created with prefix: $OUTPUT_PREFIX\"\n",
    "ls -la ${OUTPUT_PREFIX}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pgen_ancestry_pipeline(\n",
    "    ancestries,\n",
    "    whole_cohort_pgen_base,\n",
    "    base_out_dir,\n",
    "    maf=0.01,\n",
    "    hwe_pval=1e-10,\n",
    "    missing_rate=0.05,\n",
    "    script='pgen_subset_multiancestry.sh',\n",
    "    chroms=None,\n",
    "    machine_type='c3-highmem-22',\n",
    "    preemptible=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate ancestry-specific pgen files\n",
    "    \"\"\"\n",
    "    if chroms is None:\n",
    "        chroms = get_chromosome_list()\n",
    "    \n",
    "    # Filepaths for ancestry-specific sample files\n",
    "    sample_files = {}\n",
    "    for anc in ancestries:\n",
    "        sample_files[anc] = f'{my_bucket}/data/stg001/{anc}/samples.txt'\n",
    "\n",
    "    # Process each chromosome\n",
    "    for chrom in chroms:\n",
    "        # Check if any ancestry is missing for this chromosome\n",
    "        all_exist = True\n",
    "        for anc in ancestries:\n",
    "            out_dir = f'{base_out_dir}/{anc}'\n",
    "            existing_files = [x.split('/')[-1] for x in get_file_list(out_dir) if x.endswith('.pgen')]\n",
    "            if f'genotypes_chr{chrom}.pgen' not in existing_files:\n",
    "                all_exist = False\n",
    "                break\n",
    "        \n",
    "        if not all_exist:\n",
    "            # Inline dsub submission instead of separate function\n",
    "            env_dict = {\n",
    "                'ANCESTRIES': ','.join(ancestries),\n",
    "                'MAF': str(maf),\n",
    "                'HWE_PVAL': str(hwe_pval),\n",
    "                'MISSING_RATE': str(missing_rate)\n",
    "            }\n",
    "            \n",
    "            in_dict = {\n",
    "                'INPUT_PGEN_PGEN': f'{whole_cohort_pgen_base.format(chrom)}.pgen',\n",
    "                'INPUT_PGEN_PSAM': f'{whole_cohort_pgen_base.format(chrom)}.psam', \n",
    "                'INPUT_PGEN_PVAR': f'{whole_cohort_pgen_base.format(chrom)}.pvar'\n",
    "            }\n",
    "            \n",
    "            # Add sample files for each ancestry\n",
    "            for anc in ancestries:\n",
    "                in_dict[f'SAMPLE_FILE_{anc}'] = sample_files[anc]\n",
    "            \n",
    "            # Output files for each ancestry\n",
    "            out_dict = {}\n",
    "            for anc in ancestries:\n",
    "                out_prefix = f'{base_out_dir}/{anc}/genotypes_chr{chrom}'\n",
    "                out_dict[f'OUTPUT_PREFIX_{anc}'] = out_prefix + '*'\n",
    "            \n",
    "            dsub_script(\n",
    "                label=f'chr{chrom}pgen_ancestry_',\n",
    "                machine_type=machine_type,\n",
    "                envs=env_dict,\n",
    "                in_params=in_dict,\n",
    "                out_params=out_dict,\n",
    "                boot_disk=100,\n",
    "                disk_size=1200,\n",
    "                script=script,\n",
    "                preemptible=preemptible\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pgen_whole_cohort_pipeline(\n",
    "    df,\n",
    "    whole_cohort_pgen_base,\n",
    "    base_out_dir,\n",
    "    maf=0.01,\n",
    "    missing_rate=0.05,\n",
    "    script='pgen_whole_cohort.sh',\n",
    "    chroms=None,\n",
    "    machine_type='c3-highmem-22',\n",
    "    preemptible=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate whole-cohort pgen files (no HWE filtering)\n",
    "    \"\"\"\n",
    "    if chroms is None:\n",
    "        chroms = get_chromosome_list()\n",
    "    \n",
    "    # Prepare whole-cohort sample file\n",
    "    all_sample_ids = df['research_id'].to_list()\n",
    "    whole_cohort_file = 'whole_cohort_samples.txt'\n",
    "    with open(whole_cohort_file, 'w') as f:\n",
    "        f.writelines([str(x) + '\\n' for x in all_sample_ids])\n",
    "    \n",
    "    whole_cohort_dir = f'{my_bucket}/data/stg001/whole_cohort'\n",
    "    os.system(f'gsutil cp {whole_cohort_file} {whole_cohort_dir}/samples.txt')\n",
    "    sample_file = f'{whole_cohort_dir}/samples.txt'\n",
    "    \n",
    "    # Process each chromosome\n",
    "    for chrom in chroms:\n",
    "        # Check if output exists\n",
    "        existing_files = [x.split('/')[-1] for x in get_file_list(f'{my_bucket}/data/stg009/whole_cohort') if x.endswith('.pgen')]\n",
    "        if f'genotypes_chr{chrom}.pgen' not in existing_files:\n",
    "            # Inline dsub submission\n",
    "            env_dict = {\n",
    "                'MAF': str(maf),\n",
    "                'MISSING_RATE': str(missing_rate)\n",
    "            }\n",
    "            \n",
    "            in_dict = {\n",
    "                'INPUT_PGEN_PGEN': f'{whole_cohort_pgen_base.format(chrom)}.pgen',\n",
    "                'INPUT_PGEN_PSAM': f'{whole_cohort_pgen_base.format(chrom)}.psam', \n",
    "                'INPUT_PGEN_PVAR': f'{whole_cohort_pgen_base.format(chrom)}.pvar',\n",
    "                'SAMPLE_FILE_WHOLE_COHORT': sample_file\n",
    "            }\n",
    "            \n",
    "            out_prefix = f'{base_out_dir}/whole_cohort/genotypes_chr{chrom}'\n",
    "            out_dict = {\n",
    "                'OUTPUT_PREFIX_WHOLE_COHORT': out_prefix + '*'\n",
    "            }\n",
    "            \n",
    "            dsub_script(\n",
    "                label=f'pgen_whole_cohort_chr{chrom}',\n",
    "                machine_type=machine_type,\n",
    "                envs=env_dict,\n",
    "                in_params=in_dict,\n",
    "                out_params=out_dict,\n",
    "                boot_disk=100,\n",
    "                disk_size=1200,\n",
    "                script=script,\n",
    "                preemptible=preemptible\n",
    "            )\n",
    "        else:\n",
    "            print(f'genotypes_chr{chrom}.pgen already present in {my_bucket}/data/stg009/whole_cohort/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and Ancestry Preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AoU Ancestry Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_file = 'gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/ancestry/ancestry_preds.tsv'\n",
    "\n",
    "# # Copy the file\n",
    "# os.system(f\"gsutil -u $GOOGLE_PROJECT cp {source_file} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_df = pl.read_csv('ancestry_preds.tsv',\n",
    "                          separator='\\t',\n",
    "                          schema_overrides={ 'research_id' : pl.Utf8 })\n",
    "print(f'{ancestry_df.height} research_id in ancestry_preds.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Flagged Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs_file = \"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/qc/flagged_samples.tsv\"\n",
    "# !gsutil -u $$GOOGLE_PROJECT cp {fs_file} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = pl.read_csv(\n",
    "    'flagged_samples.tsv',\n",
    "    separator='\\t',\n",
    "    schema_overrides={ 's' : pl.Utf8 }\n",
    ")\n",
    "fs_samps = fs['s'].to_list()\n",
    "print(f'{fs.height} s in flagged_samples.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~pl.col('research_id').is_in(fs_samps)\n",
    "ancestry_df = ancestry_df.filter(mask)\n",
    "print(f'{ancestry_df.height} research_id in ancestry_preds.tsv after removing flagged samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Related Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rel_file = \"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv\"\n",
    "# !gsutil -u $$GOOGLE_PROJECT cp {rel_file} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = pl.read_csv(\n",
    "    'relatedness_flagged_samples.tsv',\n",
    "    separator='\\t',\n",
    "    schema_overrides={ 'sample_id' : pl.Utf8 }\n",
    ")\n",
    "rel_samps = rel['sample_id'].to_list()\n",
    "print(f'{rel.height} sample_id in relatedness_flagged_samples.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_mask = ~pl.col('research_id').is_in(rel_samps)\n",
    "ancestry_df = ancestry_df.filter(rel_mask)\n",
    "print(f'{ancestry_df.height} research_id in ancestry_preds.tsv after also removing related samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_df.filter(pl.col('ancestry_pred_other')=='eur').height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ancestries\n",
    "Need massive memory to handle these files (c3-highmem-22, 176 GiB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries_considered = ['eur', 'afr', 'amr', 'eas', 'sas', 'mid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v8 whole-cohort pgen files\n",
    "whole_cohort_base = 'gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/acaf_threshold/pgen/acaf_threshold.chr{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pgen_ancestry_pipeline(\n",
    "    ancestries=ancestries_considered,\n",
    "    whole_cohort_pgen_base=whole_cohort_base,\n",
    "    base_out_dir=f'{my_bucket}/data/stg009',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pgen_whole_cohort_pipeline(\n",
    "    df=ancestry_df,\n",
    "    whole_cohort_pgen_base=whole_cohort_base,\n",
    "    base_out_dir=f'{my_bucket}/data/stg009',\n",
    "    chroms=[18]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check All Statuses\n",
    "check_dsub_status(full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel_running_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 'pgen-ances--bwaxse--250624-213359-29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details(job=job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {bucket or my_bucket}/dsub/logs/pgen-ancestry-chr1/bwaxse/20250624/pgen-ances--bwaxse--250624-213359-29-task-None.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 'pgen-whole--bwaxse--250624-213515-00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details(job=job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {bucket or my_bucket}/dsub/logs/pgen-whole-cohort-chr1/bwaxse/20250624/pgen-whole--bwaxse--250624-213515-00-task-None.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil du -h {bucket or my_bucket}/data/stg009/eur/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dsub: bed from pgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_pgen_to_bed.sh\n",
    "\n",
    "#!/bin/bash\n",
    "# Convert pgen files to legacy plink bed/bim/fam format for SAIGE\n",
    "# Input: pgen files for single ancestry/whole cohort\n",
    "# Output: bed/bim/fam files for SAIGE input\n",
    "\n",
    "nthread=$(python -c \"import os; print(len(os.sched_getaffinity(0)))\");\n",
    "echo \"Running with $nthread threads\";\n",
    "\n",
    "# Single target per job\n",
    "target=\"$TARGETS\"\n",
    "echo \"=== Converting pgen to bed/bim/fam format for SAIGE ===\"\n",
    "echo \"Processing target: $target\"\n",
    "\n",
    "# Get target-specific inputs using variable indirection (now specific files)\n",
    "pgen_var=\"INPUT_PGEN_${target}\"\n",
    "bed_var=\"OUTPUT_BED_${target}\"\n",
    "\n",
    "# Get the file paths\n",
    "pgen_file=\"${!pgen_var}\"\n",
    "bed_file=\"${!bed_var}\"\n",
    "\n",
    "# Extract base name for plink2 (remove extension)\n",
    "input_base=\"${pgen_file%.pgen}\"\n",
    "output_base=\"${bed_file%.bed}\"\n",
    "\n",
    "echo \"Converting pgen for $target:\"\n",
    "echo \"  Input:  $input_base.{pgen,psam,pvar}\"\n",
    "echo \"  Output: $output_base.{bed,bim,fam}\"\n",
    "\n",
    "# Convert pgen to bed/bim/fam\n",
    "plink2 \\\n",
    "    --pfile $input_base \\\n",
    "    --make-bed \\\n",
    "    --memory 50000 \\\n",
    "    --threads $nthread \\\n",
    "    --out $output_base\n",
    "\n",
    "echo \"Conversion complete for $target\"\n",
    "echo \"Files created:\"\n",
    "ls -la ${output_base}.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pgen_to_bed_pipeline(\n",
    "    ancestries,\n",
    "    include_whole_cohort=True,\n",
    "    script='run_pgen_to_bed.sh',\n",
    "    chroms=None,\n",
    "    machine_type='c3-standard-4',\n",
    "    preemptible=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert pgen files to bed/bim/fam format for SAIGE\n",
    "    Uses simplified path structure: {base_bucket}/data/stg009/{ancestry}/\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ancestries : list\n",
    "        List of ancestry codes (e.g., ['eas', 'eur', 'afr', 'amr', 'mid', 'sas'])\n",
    "    include_whole_cohort : bool, default True\n",
    "        Whether to also convert whole_cohort pgen files\n",
    "    chroms : list, optional\n",
    "        Chromosomes to process. Defaults to 1-22, X, Y\n",
    "    \"\"\"\n",
    "    if chroms is None:\n",
    "        chroms = get_chromosome_list()\n",
    "    \n",
    "    # Build list of targets (ancestries + optionally whole_cohort)\n",
    "    targets = ancestries.copy()\n",
    "    if include_whole_cohort:\n",
    "        targets.append('whole_cohort')\n",
    "    \n",
    "    # Submit separate job for each target-chromosome combination\n",
    "    for target in targets:\n",
    "        for chrom in chroms:\n",
    "            target_dir = f'{my_bucket}/data/stg009/{target}'\n",
    "            \n",
    "            # Check if bed files exist\n",
    "            existing_files = [x.split('/')[-1] for x in get_file_list(target_dir) if x.endswith('.bed')]\n",
    "            if f'genotypes_chr{chrom}.bed' not in existing_files:\n",
    "                \n",
    "                # Submit individual job for this target-chromosome\n",
    "                env_dict = {\n",
    "                    'TARGETS': target  # Single target per job\n",
    "                }\n",
    "                \n",
    "                # Input and output in same directory - just different formats\n",
    "                in_dict = {\n",
    "                    f'INPUT_PGEN_{target}': f'{target_dir}/genotypes_chr{chrom}.pgen',\n",
    "                    f'INPUT_PSAM_{target}': f'{target_dir}/genotypes_chr{chrom}.psam',\n",
    "                    f'INPUT_PVAR_{target}': f'{target_dir}/genotypes_chr{chrom}.pvar'\n",
    "                }\n",
    "                out_dict = {\n",
    "                    f'OUTPUT_BED_{target}': f'{target_dir}/genotypes_chr{chrom}.bed',\n",
    "                    f'OUTPUT_BIM_{target}': f'{target_dir}/genotypes_chr{chrom}.bim', \n",
    "                    f'OUTPUT_FAM_{target}': f'{target_dir}/genotypes_chr{chrom}.fam'\n",
    "                }\n",
    "                \n",
    "                dsub_script(\n",
    "                    label=f'{target}_chr{chrom}_to_bed',\n",
    "                    machine_type=machine_type,\n",
    "                    envs=env_dict,\n",
    "                    in_params=in_dict,\n",
    "                    out_params=out_dict,\n",
    "                    boot_disk=100,\n",
    "                    disk_size=100,\n",
    "                    script=script,\n",
    "                    preemptible=preemptible\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pgen_to_bed_pipeline(\n",
    "    ancestries=['eas', 'eur', 'afr', 'amr', 'mid', 'sas'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pgen_to_bed_pipeline(\n",
    "    ancestries=['eur'],\n",
    "    chroms=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check All Statuses\n",
    "check_dsub_status(full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel_running_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 'eur-chr1-t--bwaxse--250625-144512-99'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details(job=job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {bucket or my_bucket}/dsub/logs/eur-chr1-to-bed/bwaxse/20250625/eur-chr1-t--bwaxse--250625-144512-99-task-None.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil du -h {bucket or my_bucket}/data/stg009/eur/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
