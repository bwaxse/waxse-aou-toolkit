{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy, scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def check_dsub_status(user=None, full=False):\n",
    "    \"\"\"Check status of dsub jobs for the specified user\"\"\"\n",
    "    if user is None:\n",
    "        # Get current user if not specified\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "\n",
    "    if full:\n",
    "        make_full = ' --full'\n",
    "    else:\n",
    "        make_full = ''\n",
    "    \n",
    "    cmd = f\"dstat --provider google-cls-v2 --user {user} --status '*' --project {project}{make_full}\"\n",
    "    # cmd = f\"ddel --provider google-cls-v2 --project terra-vpc-sc-840afe1e --location us-central1 --jobs 'transances--bwaxse--250319-022343-75' --users 'bwaxse'\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_details(user=None, job=None):\n",
    "    \"\"\"List all jobs for the user, including failed ones\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    if user is None:\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "        \n",
    "    if job is None:\n",
    "        job = \"'*' \"\n",
    "    else:\n",
    "        job = f'--jobs {job} '\n",
    "    \n",
    "    cmd = f\"dstat --provider google-cls-v2 --project {project} --user {user} --status {job}--full\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_running_jobs():\n",
    "    \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Cancel only running jobs\n",
    "    cancel_cmd = f\"ddel --provider google-cls-v2 --project {project} --users 'bwaxse' --jobs '*'\"\n",
    "    print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "    \n",
    "    return subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsub_script(\n",
    "    machine_type,\n",
    "    out_dir,\n",
    "    anc,\n",
    "    in_dict=None,\n",
    "    out_dict=None,\n",
    "    memory=None,\n",
    "    threads=None,\n",
    "    chr_num=None,\n",
    "    boot_disk=100,\n",
    "    disk_size=150,\n",
    "    preemptible=True,\n",
    "    image='us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "    script='pc_genotype_correlations.sh'\n",
    "):\n",
    "    dsub_user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    job_name = f'{anc}_chr{chr_num}_{script.replace(\".sh\", \"\")}'\n",
    "    \n",
    "    cmd = [\n",
    "        'dsub',\n",
    "        '--provider', 'google-cls-v2',\n",
    "        '--machine-type', machine_type,\n",
    "        '--disk-type', 'pd-ssd', \n",
    "        '--boot-disk-size', str(boot_disk),\n",
    "        '--disk-size', str(disk_size),\n",
    "        '--user-project', os.environ['GOOGLE_PROJECT'],\n",
    "        '--project', os.environ['GOOGLE_PROJECT'],\n",
    "        '--image', image,\n",
    "        '--network', 'network',\n",
    "        '--subnetwork', 'subnetwork',\n",
    "        '--service-account', subprocess.check_output(['gcloud', 'config', 'get-value', 'account']).decode().strip(),\n",
    "        '--user', dsub_user_name,\n",
    "        '--logging', f\"{os.environ['WORKSPACE_BUCKET']}/dsub/logs/{{job-name}}/{{user-id}}/{{job-id}}-{{task-id}}-{{task-attempt}}.log\",\n",
    "        '--name', job_name,\n",
    "        '--env', f'GOOGLE_PROJECT={os.environ[\"GOOGLE_PROJECT\"]}',\n",
    "        '--env', f'ANCESTRY={anc}',\n",
    "        '--env', f'CHR={chr_num}'\n",
    "    ]\n",
    "    if preemptible:\n",
    "        cmd.append('--preemptible')\n",
    "        \n",
    "    # Add optional environment variables\n",
    "    if memory:\n",
    "        cmd.extend(['--env', f'MEMORY={memory}'])\n",
    "    if threads:\n",
    "        cmd.extend(['--env', f'THREADS={threads}'])\n",
    "    \n",
    "    # Add input files\n",
    "    if in_dict:\n",
    "        for key, value in in_dict.items():\n",
    "            cmd.extend(['--input', f'{key}={value}'])\n",
    "    \n",
    "    # Add output files\n",
    "    if out_dict:\n",
    "        for key, value in out_dict.items():\n",
    "            cmd.extend(['--output', f'{key}={value}'])\n",
    "    \n",
    "    cmd.extend(['--script', script])\n",
    "    subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Ancestry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_df = pl.read_csv(\n",
    "    f'{my_bucket}/data/ancestry_metadata.tsv',\n",
    "    separator='\\t',\n",
    "    schema_overrides={'person_id' : str },\n",
    ").with_columns(\n",
    "    pl.col('pca_features')\n",
    "    .str.strip_chars(\"[]\")\n",
    "    .str.replace_all(\"'\", \"\")\n",
    "    .str.split(\", \")\n",
    "    .list.eval(pl.element().cast(pl.Float64))\n",
    ").with_columns(\n",
    "    pl.col('probabilities')\n",
    "    .str.strip_chars(\"[]\")\n",
    "    .str.replace_all(\"'\", \"\")\n",
    "    .str.split(\", \")\n",
    "    .list.eval(pl.element().cast(pl.Float64))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot PCs and Save Ancestry Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_colors = {\n",
    "    'eur': '#1f77b4',  # blue\n",
    "    'afr': '#ff7f0e',  # orange\n",
    "    'amr': '#2ca02c',  # green\n",
    "    'eas': '#d62728',  # red\n",
    "    'mid': '#9467bd',  # purple\n",
    "    'sas': '#8c564b'   # brown\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_ancestries = ancestry_df['ancestry_pred_other'].unique().to_list()\n",
    "ancestry_list = [anc for anc in ['eur', 'afr', 'amr', 'eas', 'sas', 'mid'] if anc in available_ancestries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC pairs to plot\n",
    "pc_pairs = [(0,1), (0,2), (1,2), (2,3)]\n",
    "pc_labels = ['PC1 vs PC2', 'PC1 vs PC3', 'PC2 vs PC3', 'PC3 vs PC4']\n",
    "\n",
    "# Prepare whole cohort data\n",
    "all_ancestry_data = ancestry_df.select(['research_id', 'ancestry_pred_other', 'pca_features'])\n",
    "all_whole_pcs = np.array(all_ancestry_data['pca_features'].to_list())\n",
    "all_ancestry_labels = all_ancestry_data['ancestry_pred_other'].to_list()\n",
    "\n",
    "# Collect all PC data and calculate consistent ranges\n",
    "print(\"Collecting PC data and calculating ranges...\")\n",
    "all_pc_data = {}\n",
    "\n",
    "# Store whole cohort data\n",
    "all_pc_data['whole_cohort'] = {\n",
    "    'data': all_whole_pcs,\n",
    "    'labels': all_ancestry_labels\n",
    "}\n",
    "\n",
    "# Read and store ancestry-specific data\n",
    "for ancestry in ancestry_list:\n",
    "    anc_eigen_path = f\"{my_bucket}/data/stg005/pca_results/{ancestry}_eigenvectors.txt\"\n",
    "    try:\n",
    "        anc_pcs = pl.read_csv(anc_eigen_path, separator='\\t', has_header=False)\n",
    "        pc_cols = [f\"anc_PC{i}\" for i in range(1, 31)]\n",
    "        anc_pcs.columns = ['research_id'] + pc_cols\n",
    "\n",
    "        ancestry_data = (\n",
    "            ancestry_df\n",
    "            .filter(pl.col('ancestry_pred_other') == ancestry)\n",
    "            .join(anc_pcs.select(['research_id'] + [f'anc_PC{i}' for i in range(1, 5)]), \n",
    "                  on='research_id', how='inner')\n",
    "        )\n",
    "        \n",
    "        if len(ancestry_data) > 0:\n",
    "            anc_pc_data = ancestry_data.select([f'anc_PC{i}' for i in range(1, 5)]).to_numpy()\n",
    "            all_pc_data[ancestry] = anc_pc_data\n",
    "        else:\n",
    "            all_pc_data[ancestry] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"☓ Could not read {ancestry} eigenvectors: {e}\")\n",
    "        all_pc_data[ancestry] = None\n",
    "\n",
    "# Calculate consistent ranges with 5% padding\n",
    "pc_ranges = {}\n",
    "for pc_idx in range(4):\n",
    "    all_values = []\n",
    "    all_values.extend(all_pc_data['whole_cohort']['data'][:, pc_idx])\n",
    "    \n",
    "    for ancestry in ancestry_list:\n",
    "        if all_pc_data[ancestry] is not None:\n",
    "            all_values.extend(all_pc_data[ancestry][:, pc_idx])\n",
    "    \n",
    "    if all_values:\n",
    "        min_val, max_val = np.min(all_values), np.max(all_values)\n",
    "        range_size = max_val - min_val\n",
    "        padding = range_size * 0.05\n",
    "        pc_ranges[pc_idx] = (min_val - padding, max_val + padding)\n",
    "    else:\n",
    "        pc_ranges[pc_idx] = (-1, 1)\n",
    "\n",
    "print(f\"PC ranges: {pc_ranges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reference_lines(ax):\n",
    "    \"\"\"Add light grey lines at x=0 and y=0\"\"\"\n",
    "    ax.axhline(y=0, color='lightgrey', linestyle='-', alpha=0.5, linewidth=0.8)\n",
    "    ax.axvline(x=0, color='lightgrey', linestyle='-', alpha=0.5, linewidth=0.8)\n",
    "\n",
    "def plot_ancestry_comparison(use_consistent_limits=True):\n",
    "    \"\"\"Create a single ancestry comparison figure\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 27))\n",
    "    gs = fig.add_gridspec(len(ancestry_list) + 2, 4, \n",
    "                         height_ratios=[1] * (len(ancestry_list) + 1) + [0.3],\n",
    "                         hspace=0.5, wspace=0.3, top=0.95, bottom=0.05)\n",
    "\n",
    "    title_suffix = \"Consistent Axis Limits\" if use_consistent_limits else \"Individual Axis Limits\"\n",
    "    fig.suptitle(f'Ancestry Comparison: {title_suffix}\\n(\"other\" ancestry excluded)', \n",
    "                 fontsize=18, y=0.98)\n",
    "\n",
    "    # TOP ROW: Whole cohort PCs showing all ancestries\n",
    "    for col, (pc1, pc2) in enumerate(pc_pairs):\n",
    "        ax = fig.add_subplot(gs[0, col])\n",
    "\n",
    "        for ancestry in ancestry_list:\n",
    "            if ancestry in ancestry_colors:\n",
    "                mask = np.array(all_ancestry_labels) == ancestry\n",
    "                if np.any(mask):\n",
    "                    ax.scatter(all_whole_pcs[mask, pc1], all_whole_pcs[mask, pc2],\n",
    "                             c=ancestry_colors[ancestry], alpha=0.1, s=8, \n",
    "                             rasterized=True)\n",
    "\n",
    "        add_reference_lines(ax)\n",
    "        \n",
    "        if use_consistent_limits:\n",
    "            ax.set_xlim(pc_ranges[pc1])\n",
    "            ax.set_ylim(pc_ranges[pc2])\n",
    "            \n",
    "        ax.set_xlabel(f'Whole Cohort PC{pc1+1}', fontsize=12)\n",
    "        ax.set_ylabel(f'Whole Cohort PC{pc2+1}', fontsize=12)\n",
    "        ax.set_title(f'Whole Cohort: {pc_labels[col]}', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # SUBSEQUENT ROWS: Ancestry-specific PCs for each ancestry\n",
    "    for row_idx, ancestry in enumerate(ancestry_list):\n",
    "        if all_pc_data[ancestry] is None:\n",
    "            # Fill row with empty plots\n",
    "            for col in range(4):\n",
    "                ax = fig.add_subplot(gs[row_idx + 1, col])\n",
    "                ax.text(0.5, 0.5, f'No data\\nfor {ancestry}', \n",
    "                       ha='center', va='center', transform=ax.transAxes,\n",
    "                       fontsize=14, color='red')\n",
    "                ax.set_title(f'{ancestry}: {pc_labels[col]}', \n",
    "                           fontsize=14, fontweight='bold')\n",
    "            continue\n",
    "\n",
    "        anc_pc_data = all_pc_data[ancestry]\n",
    "\n",
    "        for col, (pc1, pc2) in enumerate(pc_pairs):\n",
    "            ax = fig.add_subplot(gs[row_idx + 1, col])\n",
    "\n",
    "            color = ancestry_colors.get(ancestry, '#333333')\n",
    "            ax.scatter(anc_pc_data[:, pc1], anc_pc_data[:, pc2],\n",
    "                       c=color, alpha=0.2, s=8, \n",
    "                       edgecolor='white', linewidth=0.1, rasterized=True)\n",
    "\n",
    "            add_reference_lines(ax)\n",
    "            \n",
    "            if use_consistent_limits:\n",
    "                ax.set_xlim(pc_ranges[pc1])\n",
    "                ax.set_ylim(pc_ranges[pc2])\n",
    "                \n",
    "            ax.set_xlabel(f'{ancestry}-Specific PC{pc1+1}', fontsize=12)\n",
    "            ax.set_ylabel(f'{ancestry}-Specific PC{pc2+1}', fontsize=12)\n",
    "            ax.set_title(f'{ancestry}: {pc_labels[col]}', fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "            if col == 0:\n",
    "                ax.text(0.02, 0.98, f'n={len(anc_pc_data):,}', \n",
    "                       transform=ax.transAxes, fontsize=10, \n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    # LEGEND ROW\n",
    "    legend_ax = fig.add_subplot(gs[-1, :])\n",
    "    legend_ax.axis('off')\n",
    "\n",
    "    # Create legend handles\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    for ancestry in ancestry_list:\n",
    "        if ancestry in ancestry_colors:\n",
    "            handle = plt.Line2D([0], [0], marker='o', color='w', \n",
    "                              markerfacecolor=ancestry_colors[ancestry], \n",
    "                              markersize=12, alpha=0.8, markeredgecolor='black', markeredgewidth=0.5)\n",
    "            legend_handles.append(handle)\n",
    "            legend_labels.append(ancestry)\n",
    "\n",
    "    legend_ax.legend(legend_handles, legend_labels, \n",
    "                    loc='center', ncol=len(ancestry_list), \n",
    "                    fontsize=14, title='Ancestry Groups', title_fontsize=16)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plot_ancestry_comparison(use_consistent_limits=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plot_ancestry_comparison(use_consistent_limits=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Ancestry Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the base ancestry dataframe\n",
    "comprehensive_pc_df = ancestry_df.clone()\n",
    "\n",
    "# Initialize shared ancestry-specific PC columns (all null initially)\n",
    "comprehensive_pc_df = comprehensive_pc_df.with_columns(\n",
    "    pl.lit(None, dtype=pl.List(pl.Float64)).alias('anc_pca_features')\n",
    ")\n",
    "\n",
    "all_anc_pcs = []\n",
    "\n",
    "# For each ancestry, load PCs and merge\n",
    "for ancestry in ancestry_list:\n",
    "    print(f\"Loading {ancestry} PCs...\")\n",
    "    \n",
    "    anc_eigen_path = f\"{my_bucket}/data/stg005/pca_results/{ancestry}_eigenvectors.txt\"\n",
    "    try:          \n",
    "        anc_pcs = pl.read_csv(anc_eigen_path, separator='\\t', has_header=False)\n",
    "        # Rename to shared column names\n",
    "        pc_cols = [f\"ancPC{i}\" for i in range(1, 31)]\n",
    "        anc_pcs.columns = ['research_id'] + pc_cols\n",
    "        \n",
    "        # Add ancestry identifier\n",
    "        anc_pcs = anc_pcs.with_columns(pl.lit(ancestry).alias('pc_ancestry'))\n",
    "        all_anc_pcs.append(anc_pcs)\n",
    "        \n",
    "        print(f\"✓ Loaded {ancestry} PCs: {len(anc_pcs):,} individuals\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"☓ Could not read {ancestry} eigenvectors: {e}\")\n",
    "\n",
    "combined_anc_pcs = []\n",
    "        \n",
    "# Concatenate all ancestry-specific PCs\n",
    "if all_anc_pcs:\n",
    "    combined_anc_pcs = pl.concat(all_anc_pcs, how='vertical')\n",
    "    \n",
    "    # Merge with main dataset\n",
    "    comprehensive_pc_df = ancestry_df.join(\n",
    "        combined_anc_pcs.drop('pc_ancestry'), \n",
    "        on='research_id', \n",
    "        how='left'\n",
    "    )\n",
    "else:\n",
    "    comprehensive_pc_df = ancestry_df\n",
    "\n",
    "# Convert PC columns to a list\n",
    "comprehensive_pc_df = comprehensive_pc_df.with_columns(\n",
    "    pl.concat_list([pl.col(f\"ancPC{i}\") for i in range(1, 31)]).alias('anc_pca_features')\n",
    ").select(['research_id', 'ancestry_pred', 'probabilities', 'pca_features', 'ancestry_pred_other',\n",
    "          'anc_pca_features'])\n",
    "    \n",
    "# Save the comprehensive dataset\n",
    "output_path = f'{my_bucket}/data/ancestry_specific_pcs.parquet'\n",
    "try:\n",
    "    comprehensive_pc_df.write_parquet(output_path)\n",
    "    print(f\"✓ Saved comprehensive PC dataset to: {output_path}\")\n",
    "    print(f\"  Dataset shape: {comprehensive_pc_df.shape}\")\n",
    "    print(f\"  Columns: {comprehensive_pc_df.columns}\")\n",
    "except Exception as e:\n",
    "    print(f\"☓ Error saving comprehensive dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensive_pc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-Genotype Correlation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%writefile pc_genotype_correlations.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Starting PC-genotype correlation analysis for ${ANCESTRY} chromosome ${CHR}\"\n",
    "\n",
    "# Extract base name from the .pgen file\n",
    "GENOTYPE_BASE=${INPUT_GENOTYPES_PGEN%.pgen}\n",
    "\n",
    "# Set output prefix\n",
    "OUTPUT_PREFIX=\"${OUTPUT_RESULTS%\\*}\"\n",
    "\n",
    "# Run PLINK2 correlation analysis\n",
    "echo \"Running PLINK2 correlation analysis...\"\n",
    "plink2 \\\n",
    "    --pfile $GENOTYPE_BASE \\\n",
    "    --pheno ${INPUT_EIGENVECTORS} \\\n",
    "    --pheno-col-nums 2,3,4,5 \\\n",
    "    --pheno-name PC1,PC2,PC3,PC4 \\\n",
    "    --corr \\\n",
    "    --memory ${MEMORY:-8000} \\\n",
    "    --threads ${THREADS:-2} \\\n",
    "    --out $OUTPUT_PREFIX\n",
    "\n",
    "echo \"Correlation analysis completed for ${ANCESTRY} chr${CHR}\"\n",
    "echo \"Generated files:\"\n",
    "ls -la ${OUTPUT_PREFIX}*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def run_pc_correlations(\n",
    "    my_bucket,\n",
    "    anc,\n",
    "    chr_num,\n",
    "    script='pc_genotype_correlations.sh'\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate PC-genotype correlations for one chromosome\n",
    "    \"\"\"\n",
    "    # Define paths\n",
    "    genotype_dir = f'{my_bucket}/data/stg001/{anc}'\n",
    "    pca_dir = f'{my_bucket}/data/stg005/pca_results'\n",
    "    out_dir = f'{my_bucket}/data/stg007/correlation_results/{anc}'\n",
    "    \n",
    "    print(f\"Running PC correlation analysis for {anc} chr{chr_num}...\")\n",
    "    \n",
    "    # Input files\n",
    "    in_dict = {\n",
    "        'INPUT_GENOTYPES_PGEN': f'{genotype_dir}/genotypes_chr{chr_num}.pgen',\n",
    "        'INPUT_GENOTYPES_PSAM': f'{genotype_dir}/genotypes_chr{chr_num}.psam', \n",
    "        'INPUT_GENOTYPES_PVAR': f'{genotype_dir}/genotypes_chr{chr_num}.pvar',\n",
    "        'INPUT_EIGENVECTORS': f'{pca_dir}/{anc}_eigenvectors.txt'\n",
    "    }\n",
    "    \n",
    "    dsub_script(\n",
    "        machine_type='c3-standard-8',\n",
    "        out_dir=out_dir,\n",
    "        anc=anc,\n",
    "        chr_num=chr_num,\n",
    "        memory=25000,\n",
    "        threads=8,\n",
    "        in_dict=in_dict,\n",
    "        out_dict={\n",
    "            'OUTPUT_RESULTS': f'{out_dir}/{anc}_chr{chr_num}*'\n",
    "        },\n",
    "        boot_disk=100,\n",
    "        disk_size=150,\n",
    "        preemptible=True,\n",
    "        image='us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "        script=script\n",
    "    )\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def run_correlations_for_all_ancestries_and_chrs(my_bucket, ancestries):\n",
    "    \"\"\"\n",
    "    Run PC-genotype correlation analysis for all ancestries and chromosomes\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== STARTING PC-GENOTYPE CORRELATION JOBS ===\")\n",
    "    \n",
    "    for anc in ancestries:\n",
    "        print(f\"\\nSubmitting correlation jobs for {anc}...\")\n",
    "        results[anc] = {}\n",
    "        \n",
    "        for chr_num in range(22, 23):\n",
    "            out_dir = run_pc_correlations(my_bucket, anc, chr_num)\n",
    "            results[anc][chr_num] = out_dir\n",
    "            print(f\"  Submitted chr{chr_num}\")\n",
    "            \n",
    "    total_jobs = len(ancestries) * 22\n",
    "    print(f\"\\n=== SUBMITTED {total_jobs} CORRELATION JOBS ===\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PC-GT Correlation\n",
    "This was an attempt to make sure that there were no PC-genotype associations like this paper introduced: https://pubmed.ncbi.nlm.nih.gov/39680601/ \n",
    "Plink2 does not have correlation alone, so did not run GLM, although that would be an alternative."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ancestries_considered = ['mid'] #['eur', 'afr', 'amr', 'eas', 'sas', 'mid']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# results = run_correlations_for_all_ancestries_and_chrs(my_bucket, ancestries_considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {bucket or my_bucket}/data/stg003/t2dggi_clusters__formatted.tsv | head -5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
