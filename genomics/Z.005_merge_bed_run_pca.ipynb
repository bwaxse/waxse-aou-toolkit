{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy, scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dsub_status(user=None, full=False):\n",
    "    \"\"\"Check status of dsub jobs for the specified user\"\"\"\n",
    "    if user is None:\n",
    "        # Get current user if not specified\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "\n",
    "    if full:\n",
    "        make_full = ' --full'\n",
    "    else:\n",
    "        make_full = ''\n",
    "    \n",
    "    cmd = f\"dstat --provider google-cls-v2 --user {user} --status '*' --project {project}{make_full}\"\n",
    "    # cmd = f\"ddel --provider google-cls-v2 --project terra-vpc-sc-840afe1e --location us-central1 --jobs 'transances--bwaxse--250319-022343-75' --users 'bwaxse'\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_details(user=None, job=None):\n",
    "    \"\"\"List all jobs for the user, including failed ones\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    if user is None:\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "        \n",
    "    if job is None:\n",
    "        job = \"'*' \"\n",
    "    else:\n",
    "        job = f'--jobs {job} '\n",
    "    \n",
    "    cmd = f\"dstat --provider google-cls-v2 --project {project} --user {user} --status {job}--full\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_running_jobs():\n",
    "    \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Cancel only running jobs\n",
    "    cancel_cmd = f\"ddel --provider google-cls-v2 --project {project} --users 'bwaxse' --jobs '*'\"\n",
    "    print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "    \n",
    "    return subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_job(job_id):\n",
    "    \"\"\"Cancel a specific job\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    cmd = f\"ddel --provider google-cls-v2 --project {project} --jobs {job_id}\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(query_dir):\n",
    "    tmp = subprocess.run(\n",
    "        f'gsutil ls {query_dir}',\n",
    "        shell=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    files = tmp.stdout.decode('utf-8').split('\\n')\n",
    "    return(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsub_script(\n",
    "    machine_type,\n",
    "    out_dir,\n",
    "    anc,\n",
    "    in_dict=None,\n",
    "    out_dict=None,\n",
    "    memory=None,\n",
    "    threads=None,\n",
    "    num_pcs=None,\n",
    "    boot_disk=100,\n",
    "    disk_size=150,\n",
    "    preemptible=True,\n",
    "    image='us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "    script='merge_pruned_genotypes.sh'\n",
    "):\n",
    "\n",
    "    dsub_user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    job_name = f'{anc}_{script.replace(\".sh\", \"\")}'\n",
    "    \n",
    "    cmd = [\n",
    "        'dsub',\n",
    "        '--provider', 'google-cls-v2',\n",
    "        '--machine-type', machine_type,\n",
    "        '--disk-type', 'pd-ssd', # 'hyperdisk-balanced' for c4-highmem-16\n",
    "        '--boot-disk-size', str(boot_disk),\n",
    "        '--disk-size', str(disk_size),\n",
    "        '--user-project', os.environ['GOOGLE_PROJECT'],\n",
    "        '--project', os.environ['GOOGLE_PROJECT'],\n",
    "        '--image', image,\n",
    "        '--network', 'network',\n",
    "        '--subnetwork', 'subnetwork',\n",
    "        '--service-account', subprocess.check_output(['gcloud', 'config', 'get-value', 'account']).decode().strip(),\n",
    "        '--user', dsub_user_name,\n",
    "        '--logging', f\"{os.environ['WORKSPACE_BUCKET']}/dsub/logs/{{job-name}}/{{user-id}}/{{job-id}}-{{task-id}}-{{task-attempt}}.log\",\n",
    "        '--name', job_name,\n",
    "        '--env', f'GOOGLE_PROJECT={os.environ[\"GOOGLE_PROJECT\"]}',\n",
    "        '--env', f'ANCESTRY={anc}',\n",
    "        '--script', script\n",
    "    ]\n",
    "\n",
    "    if preemptible:\n",
    "        cmd.append('--preemptible')\n",
    "\n",
    "    # Add optional environment variables\n",
    "    if memory:\n",
    "        cmd.extend(['--env', f'MEMORY={memory}'])\n",
    "    if threads:\n",
    "        cmd.extend(['--env', f'THREADS={threads}'])\n",
    "    if num_pcs:\n",
    "        cmd.extend(['--env', f'NUM_PCS={num_pcs}'])\n",
    "    \n",
    "    # Add input files\n",
    "    if in_dict:\n",
    "        for key, value in in_dict.items():\n",
    "            cmd.extend(['--input', f'{key}={value}'])\n",
    "    \n",
    "    # Add output files\n",
    "    if out_dict:\n",
    "        for key, value in out_dict.items():\n",
    "            cmd.extend(['--output', f'{key}={value}'])\n",
    "    \n",
    "    subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge genotypes function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil du -h {my_bucket}/data/stg003/pruned_genotypes/all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile merge_pruned_genotypes.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"=== MERGING CHROMOSOME FILES ===\"\n",
    "\n",
    "# Get base path from first chromosome file - handle double chr issue\n",
    "in_base=$(echo $INPUT_CHR1_BED | sed 's/genotypes_chr1_pruned.bed//g');\n",
    "OUTPUT_PREFIX=\"${OUTPUT_RESULTS%\\*}\"\n",
    "\n",
    "echo \"Input base: $in_base\"\n",
    "echo \"Output prefix: $OUTPUT_PREFIX\"\n",
    "echo \"Ancestry: $ANCESTRY\"\n",
    "echo \"Detected pattern: genotypes_chr{N}_pruned.*\"\n",
    "\n",
    "# Create merge list file\n",
    "merge_lst='merge_input_beds.txt';\n",
    "rm -f $merge_lst;  # Clean up any existing file\n",
    "\n",
    "# Add all chromosome files to merge list - handle double chr pattern\n",
    "for chrom in {1..22}; do\n",
    "    bed_file=\"${in_base}genotypes_chr${chrom}_pruned\"\n",
    "    if [[ -f \"${bed_file}.bed\" ]]; then\n",
    "        echo \"$bed_file\" >> $merge_lst;\n",
    "        echo \"Added chromosome $chrom to merge list\"\n",
    "    else\n",
    "        echo \"Warning: Missing chromosome $chrom files (expected: ${bed_file}.bed)\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Check if we have files to merge\n",
    "if [[ ! -s $merge_lst ]]; then\n",
    "    echo \"Error: No chromosome files found for merging\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"Files to merge:\"\n",
    "cat $merge_lst\n",
    "\n",
    "# Merge all chromosome files\n",
    "echo \"Starting merge...\"\n",
    "plink2 \\\n",
    "    --pmerge-list $merge_lst bfile \\\n",
    "    --indiv-sort none \\\n",
    "    --delete-pmerge-result \\\n",
    "    --make-bed \\\n",
    "    --memory ${MEMORY:-15000} \\\n",
    "    --threads ${THREADS:-4} \\\n",
    "    --out $OUTPUT_PREFIX\n",
    "\n",
    "# Create PED file for smartpca (copy of FAM)\n",
    "cp ${OUTPUT_PREFIX}.fam ${OUTPUT_PREFIX}.ped\n",
    "\n",
    "echo \"Merge completed successfully\"\n",
    "echo \"Generated files:\"\n",
    "ls -la ${OUTPUT_PREFIX}.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bed_files(\n",
    "    my_bucket,\n",
    "    anc,\n",
    "    script='merge_pruned_genotypes.sh'\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge LD-pruned chromosome files into single dataset\n",
    "    Uses Terra image with PLINK2\n",
    "    \"\"\"\n",
    "    # Define paths\n",
    "    in_dir = f'{my_bucket}/data/stg003/pruned_genotypes/{anc}'\n",
    "    out_dir = f'{my_bucket}/data/stg005/merged_genotypes'\n",
    "    \n",
    "    # Check if already exists\n",
    "    existing_files = get_file_list(out_dir)\n",
    "    if any(f'{anc}_merged_genotypes.bed' in f for f in existing_files):\n",
    "        print(f\"Merged bed files already exist for {anc}\")\n",
    "        return out_dir\n",
    "    \n",
    "    print(f\"Merging chromosome files for {anc}...\")\n",
    "    \n",
    "    # Input parameters - all chromosome files\n",
    "    in_dict = {\n",
    "        'INPUT_CHR1_BED': f'{in_dir}/genotypes_chr1_pruned.bed'\n",
    "    }\n",
    "    \n",
    "    # Add all chromosome files as inputs\n",
    "    for chrom in range(1, 23):\n",
    "        for ext in ['bed', 'bim', 'fam']:\n",
    "            key = f'INPUT_CHR{chrom}_{ext.upper()}'\n",
    "            in_dict[key] = f'{in_dir}/genotypes_chr{chrom}_pruned.{ext}'\n",
    "    \n",
    "    dsub_script(\n",
    "        machine_type='c3-standard-8',\n",
    "        out_dir=out_dir,\n",
    "        anc=anc,\n",
    "        memory=15000,\n",
    "        threads=6,\n",
    "        in_dict=in_dict,\n",
    "        out_dict={\n",
    "            'OUTPUT_RESULTS': f'{out_dir}/{anc}_merged_genotypes*'\n",
    "        },\n",
    "        boot_disk=100,\n",
    "        disk_size=200,\n",
    "        image='us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "        script=script\n",
    "    )\n",
    "\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_merge_for_ancestries(my_bucket, ancestries):\n",
    "    \"\"\"\n",
    "    Run merge step for multiple ancestries\n",
    "    Step 1: Merge chromosomes using Terra image + PLINK2\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== STARTING MERGE JOBS FOR ALL ANCESTRIES ===\")\n",
    "    \n",
    "    for anc in ancestries:\n",
    "        print(f\"\\nSubmitting merge job for {anc}...\")\n",
    "        merged_dir = merge_bed_files(my_bucket, anc)\n",
    "        results[anc] = merged_dir\n",
    "        print(f\"  Merged genotypes will be in: {merged_dir}\")\n",
    "    \n",
    "    print(f\"\\n=== SUBMITTED {len(ancestries)} MERGE JOBS ===\")\n",
    "    print(\"Wait for all merge jobs to complete before running PCA step.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile smartpca.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"=== RUNNING SMARTPCA ===\"\n",
    "\n",
    "# Use input files\n",
    "input_bed=\"$INPUT_BED\"\n",
    "input_bim=\"$INPUT_BIM\"\n",
    "input_ped=\"$INPUT_PED\"\n",
    "\n",
    "echo \"PCA input files:\"\n",
    "echo \"  BED: $input_bed\"\n",
    "echo \"  BIM: $input_bim\" \n",
    "echo \"  PED: $input_ped\"\n",
    "\n",
    "# Verify input files exist\n",
    "for file in \"$input_bed\" \"$input_bim\" \"$input_ped\"; do\n",
    "    if [[ ! -f \"$file\" ]]; then\n",
    "        echo \"Error: Input file $file not found\"\n",
    "        exit 1\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Build smartpca configuration file\n",
    "smpca_config='smartpca.config'\n",
    "\n",
    "echo -e \"genotypename:\\t${input_bed}\" > $smpca_config\n",
    "echo -e \"snpname:\\t${input_bim}\" >> $smpca_config\n",
    "echo -e \"indivname:\\t${input_ped}\" >> $smpca_config\n",
    "echo -e \"evecoutname:\\t${SMARTPCA_OUT}\" >> $smpca_config\n",
    "echo -e \"numoutevec:\\t${NUM_PCS:-30}\" >> $smpca_config\n",
    "echo -e \"fastmode:\\tYES\" >> $smpca_config\n",
    "\n",
    "echo \"SmartPCA configuration:\"\n",
    "cat $smpca_config\n",
    "\n",
    "# Add after the configuration section\n",
    "echo \"=== SYSTEM RESOURCES ===\"\n",
    "echo \"Input file sizes:\"\n",
    "ls -lh \"$input_bed\" \"$input_bim\" \"$input_ped\"\n",
    "wc -l \"$input_ped\"  # Count samples\n",
    "wc -l \"$input_bim\"  # Count SNPs\n",
    "\n",
    "# Run smartpca\n",
    "echo \"Running SmartPCA...\"\n",
    "/usr/lib/eigensoft/smartpca -p $smpca_config > $SMARTPCA_LOG 2>&1\n",
    "\n",
    "# Check if smartpca completed successfully\n",
    "if [[ $? -ne 0 ]]; then\n",
    "    echo \"Error: SmartPCA failed\"\n",
    "    echo \"Log contents:\"\n",
    "    cat $SMARTPCA_LOG\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"SmartPCA completed successfully\"\n",
    "\n",
    "# Extract eigenvalues (first row, handle multiple spaces)\n",
    "echo \"Extracting eigenvalues...\"\n",
    "cat ${SMARTPCA_OUT} | head -n 1 | awk \" {{gsub(/ +/, \\\"\\n\\\")}}1 \" | grep . | tail -n +2 > $EIGENVALUES\n",
    "\n",
    "# Extract eigenvectors (remove first row and format)\n",
    "echo \"Extracting eigenvectors...\"\n",
    "cat ${SMARTPCA_OUT} | \\\n",
    "    tail -n +2 | \\\n",
    "    awk \" {{gsub(/^ *0:/, \\\"\\\")}}1 \" | \\\n",
    "    awk \" {{gsub(/ +/, \\\"\\t\\\")}}1 \" | \\\n",
    "    awk -F\"\\t\" \"{{OFS = FS}} NF{{NF-=1}};1\" \\\n",
    "    > $EIGENVECTORS;\n",
    "\n",
    "echo \"=== PCA ANALYSIS COMPLETED SUCCESSFULLY ===\"\n",
    "echo \"Generated files:\"\n",
    "ls -la \"$EIGENVALUES\" \"$EIGENVECTORS\" \"$SMARTPCA_OUT\" \"$SMARTPCA_LOG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_smartpca(\n",
    "    my_bucket,\n",
    "    anc,\n",
    "    num_pcs=30,\n",
    "    script='smartpca.sh'\n",
    "):\n",
    "    \"\"\"\n",
    "    Run PCA on merged genotype data\n",
    "    Uses eigensoft image with smartpca\n",
    "    \"\"\"\n",
    "    # Input from merge step\n",
    "    merged_dir = f'{my_bucket}/data/stg005/merged_genotypes'\n",
    "    out_dir = f'{my_bucket}/data/stg005/pca_results'\n",
    "\n",
    "    artifact_registry = os.getenv('ARTIFACT_REGISTRY_DOCKER_REPO', '')\n",
    "    \n",
    "    # Check if already exists\n",
    "    existing_files = get_file_list(out_dir)\n",
    "    if any(f'{anc}_eigenvectors.txt' in f for f in existing_files):\n",
    "        print(f\"PCA results already exist for {anc}\")\n",
    "        return out_dir\n",
    "        \n",
    "    print(f\"Running SmartPCA for {anc} ancestry...\")\n",
    "    \n",
    "    dsub_script(\n",
    "        machine_type='c4-highmem-16',\n",
    "        out_dir=out_dir,\n",
    "        anc=anc,\n",
    "        num_pcs=num_pcs,\n",
    "        preemptible=(anc != 'eur'),  # Non-preemptible only for EUR\n",
    "        in_dict={\n",
    "            'INPUT_BED': f'{merged_dir}/{anc}_merged_genotypes.bed',\n",
    "            'INPUT_BIM': f'{merged_dir}/{anc}_merged_genotypes.bim',\n",
    "            'INPUT_PED': f'{merged_dir}/{anc}_merged_genotypes.ped'\n",
    "        },\n",
    "        out_dict={\n",
    "            'SMARTPCA_OUT': f'{out_dir}/{anc}_smartpca.txt',\n",
    "            'SMARTPCA_LOG': f'{out_dir}/{anc}_smartpca.log', \n",
    "            'EIGENVALUES': f'{out_dir}/{anc}_eigenvalues.txt',\n",
    "            'EIGENVECTORS': f'{out_dir}/{anc}_eigenvectors.txt'\n",
    "        },\n",
    "        boot_disk=100,\n",
    "        disk_size=300,\n",
    "        image=f'{artifact_registry}/biocontainers/eigensoft:v7.2.1dfsg-1-deb_cv1',\n",
    "        script=script\n",
    "    )\n",
    "    \n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_smartpca_for_ancestries(my_bucket, ancestries, num_pcs=30):\n",
    "    \"\"\"\n",
    "    Run SmartPCA step for multiple ancestries \n",
    "    Step 2: PCA using Eigensoft image + smartpca\n",
    "    Assumes merge step is already completed\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== STARTING SMARTPCA JOBS FOR ALL ANCESTRIES ===\")\n",
    "    \n",
    "    for anc in ancestries:\n",
    "        print(f\"\\nSubmitting SmartPCA job for {anc}...\")\n",
    "        pca_dir = run_smartpca(my_bucket, anc, num_pcs=num_pcs)\n",
    "        results[anc] = pca_dir\n",
    "        print(f\"  PCA results will be in: {pca_dir}\")\n",
    "    \n",
    "    print(f\"\\n=== SUBMITTED {len(ancestries)} SMARTPCA JOBS ===\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['eur', 'afr', 'amr', 'eas', 'sas']#, 'mid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = run_merge_for_ancestries(my_bucket, ['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results = run_smartpca_for_ancestries(my_bucket, ['eur'], num_pcs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsub_status(full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = 'eur-smartp--bwaxse--250618-165307-66'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 'all-merge---bwaxse--250619-003255-67'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details(job=job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {bucket or my_bucket}/dsub/logs/all-merge-pruned-genotypes/bwaxse/all-merge---bwaxse--250619-003255-67-task-None.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cat {bucket or my_bucket}/dsub/logs/eur-smartpca/bwaxse/eur-smartp--bwaxse--250618-165307-66-task-None.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {bucket or my_bucket}/data/stg005/merged_genotypes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {bucket or my_bucket}/data/stg003/pruned_genotypes/all/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
