{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct and Calculate PheRSs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np, scipy as sps\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "from datetime import timedelta, datetime as dt\n",
    "# import pytz\n",
    "from typing import Dict, List, Optional, Union\n",
    "from jinja2 import Template, Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phetk.phecode import Phecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line allows for the plots to be displayed inline in the Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"ticks\",font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the version of the Curated Data Repository (CDR).\n",
    "version = %env WORKSPACE_CDR\n",
    "print(\"version: \" + version)\n",
    "\n",
    "# get the bucket name\n",
    "bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "print(\"bucket: \" + bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all columns in pandas\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# show full column width\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Polars string length to 100\n",
    "pl.Config.set_fmt_str_lengths(100)\n",
    "\n",
    "# Set the row limit to a higher value\n",
    "pl.Config.set_tbl_rows(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['#0173b2', '#de8f05', '#8de5a1', '#d55e00', '#029e73', '#cc78bc', '#ece133', \n",
    "           '#56b4e9', '#949494', '#fbafe4', '#ca9161']\n",
    "\n",
    "color_dict = {\n",
    "    'SARS-CoV-2': '#0173b2',\n",
    "    'Flu': '#de8f05',\n",
    "    'RSV': '#8de5a1',\n",
    "    'RV': '#d55e00',\n",
    "    'hCoV': '#029e73',\n",
    "    'hMPV': '#cc78bc',\n",
    "    'PIV': '#ece133',\n",
    "    'ADV': '#56b4e9',\n",
    "    'pert': '#949494',\n",
    "    'M_pna': '#fbafe4',\n",
    "    'C_pna': '#ca9161'\n",
    "}\n",
    "\n",
    "sns.set_palette(sns.color_palette(palette))\n",
    "sns.color_palette(palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_gbq(query):\n",
    "    \"\"\"\n",
    "    Take a SQL query and return result as polars dataframe\n",
    "    :param query: BigQuery SQL query\n",
    "    :return: polars dataframe\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query)\n",
    "    rows = query_job.result()\n",
    "    df = pl.from_arrow(rows.to_arrow())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PASC Code Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy csv file to the bucket\n",
    "# args = [\"gsutil\", \"cp\", f\"./Zhang_long_covid_min_1_icd.tsv\", f'{bucket}/data/pasc/phers/']\n",
    "# output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# # print output from gsutil\n",
    "# output.stderr\n",
    "\n",
    "# # copy csv file to the bucket\n",
    "# args = [\"gsutil\", \"cp\", f\"./Zhang_long_covid_min_1_lists.tsv\", f'{bucket}/data/pasc/phers/']\n",
    "# output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# # print output from gsutil\n",
    "# output.stderr\n",
    "\n",
    "# # copy csv file to the bucket\n",
    "# args = [\"gsutil\", \"cp\", f\"./phecodeX_unrolled_ICD_CM.csv\", f'{bucket}/data/pasc/phers/']\n",
    "# output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# # print output from gsutil\n",
    "# output.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phecodeX_unrolled_icd_df = pl.read_csv(f'{bucket}/data/pasc/phers/phecodeX_unrolled_ICD_CM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_1_phecodes_df = pl.read_csv(f'{bucket}/data/long_covid/phers/cox_sig_phewas_long_covid_min_1_phecodes.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_1_phecodes_df = min_1_phecodes_df.select(['phecode', 'phecode_string', \n",
    "                                                          'phecode_category',  \n",
    "                                                          'long_covid_min_1_hazard_ratio_low',\n",
    "                                                          'long_covid_min_1_hazard_ratio_high',\n",
    "                                                          'long_covid_min_1_standard_error',\n",
    "                                                          'long_covid_min_1_hazard_ratio',\n",
    "                                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique phecodes from each dataframe\n",
    "min_1 = set(min_1_phecodes_df['phecode'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(min_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_pasc_icd_df = pl.read_csv(f'{bucket}/data/pasc/phers/Zhang_PASC_icd.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_pasc_list_df = pl.read_csv(f'{bucket}/data/pasc/phers/Zhang_PASC_lists.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pl.read_csv(f'{bucket}/data/long_covid/combined_cohort.csv', separator='\\t',\n",
    "                         try_parse_dates=True)\n",
    "test_df = pl.read_csv(f'{bucket}/data/long_covid/test_cohort.csv', separator='\\t',\n",
    "                         try_parse_dates=True)\n",
    "train_df = pl.read_csv(f'{bucket}/data/long_covid/train_cohort.csv', separator='\\t',\n",
    "                         try_parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Demographics and Residualization Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_q = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    p.person_id,\n",
    "    CAST(p.birth_datetime AS DATE) AS dob,\n",
    "    p_race_concept.concept_name as race,\n",
    "    p_ethnicity_concept.concept_name as ethnicity,\n",
    "    p_sex_at_birth_concept.concept_name as sex_at_birth,\n",
    "FROM\n",
    "    {version}.person p\n",
    "LEFT JOIN\n",
    "    {version}.concept p_race_concept \n",
    "        ON p.race_concept_id = p_race_concept.concept_id \n",
    "LEFT JOIN\n",
    "    {version}.concept p_ethnicity_concept \n",
    "        ON p.ethnicity_concept_id = p_ethnicity_concept.concept_id \n",
    "LEFT JOIN\n",
    "    {version}.concept p_sex_at_birth_concept \n",
    "        ON p.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id  \n",
    "\"\"\"\n",
    "\n",
    "demographics_df = polars_gbq(demographics_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"None of these\": \"None\",\n",
    "    \"I prefer not to answer\": \"PNA\",\n",
    "    \"Black or African American\": \"Black\",\n",
    "    \"PMI: Skip\": \"Skip\",\n",
    "    \"More than one population\": \"Multip\",\n",
    "    \"Asian\": \"Asian\",\n",
    "    \"Native Hawaiian or Other Pacific Islander\": \"NHPI\",\n",
    "    \"None Indicated\": \"NoInd\",\n",
    "    \"White\": \"White\",\n",
    "    \"Middle Eastern or North African\": \"MENA\",\n",
    "    \"American Indian or Alaska Native\": \"AIAN\"\n",
    "}\n",
    "demographics_df = demographics_df.with_columns(\n",
    "    pl.col(\"race\").map_elements(lambda x: mapping.get(x, x), return_dtype=pl.Utf8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"I prefer not to answer\": \"PNA\",\n",
    "    \"No matching concept\": \"NoMatch\",\n",
    "    \"Male\": \"Male\",\n",
    "    \"Intersex\": \"Intersex\",\n",
    "    \"Female\": \"Female\",\n",
    "    \"PMI: Skip\": \"Skip\",\n",
    "    \"None\": \"None\"\n",
    "}\n",
    "demographics_df = demographics_df.with_columns(\n",
    "    pl.col(\"sex_at_birth\").map_elements(lambda x: mapping.get(x, x), return_dtype=pl.Utf8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"PMI: Prefer Not To Answer\": \"PNA\",\n",
    "    \"No matching concept\": \"NoMatch\",\n",
    "    \"What Race Ethnicity: Race Ethnicity None Of These\": \"None\",\n",
    "    \"Not Hispanic or Latino\": \"NotHisp\",\n",
    "    \"PMI: Skip\": \"Skip\",\n",
    "    \"Hispanic or Latino\": \"HispLat\",\n",
    "}\n",
    "demographics_df = demographics_df.with_columns(\n",
    "    pl.col(\"ethnicity\").map_elements(lambda x: mapping.get(x, x), return_dtype=pl.Utf8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demographics_dummies(df):\n",
    "    \"\"\"Create race/ethnicity categories for PheWAS\"\"\"\n",
    "    \n",
    "    # Create the race_ethnicity column using nested when-then logic\n",
    "    df_with_categories = df.with_columns(\n",
    "        pl.when(pl.col(\"ethnicity\") == \"HispLat\")\n",
    "        .then(pl.lit(\"Hispanic_Latino\"))\n",
    "        .when((pl.col(\"ethnicity\") == \"NotHisp\") & (pl.col(\"race\") == \"White\"))\n",
    "        .then(pl.lit(\"White_NonHispanic\"))\n",
    "        .when((pl.col(\"ethnicity\") == \"NotHisp\") & (pl.col(\"race\") == \"Black\"))\n",
    "        .then(pl.lit(\"Black_NonHispanic\"))\n",
    "        .when((pl.col(\"ethnicity\") == \"NotHisp\") & (pl.col(\"race\") == \"Asian\"))\n",
    "        .then(pl.lit(\"Asian_NonHispanic\"))\n",
    "        .when((pl.col(\"ethnicity\") == \"NotHisp\") & (pl.col(\"race\") == \"Multip\"))\n",
    "        .then(pl.lit(\"Multiracial_NonHispanic\"))\n",
    "        .when((pl.col(\"ethnicity\") == \"NotHisp\") & (pl.col(\"race\") == \"AIAN\"))\n",
    "        .then(pl.lit(\"AIAN_NonHispanic\"))\n",
    "        .when((pl.col(\"ethnicity\") == \"NotHisp\") & (pl.col(\"race\") == \"MENA\"))\n",
    "        .then(pl.lit(\"MENA_NonHispanic\"))\n",
    "        .when((pl.col(\"ethnicity\") == \"NotHisp\") & (pl.col(\"race\") == \"NHPI\"))\n",
    "        .then(pl.lit(\"NHPI_NonHispanic\"))\n",
    "        .otherwise(pl.lit(\"Other_Unknown\"))\n",
    "        .alias(\"race_ethnicity\")\n",
    "    )\n",
    "    \n",
    "    # Create dummy variables (omit White_NonHispanic as reference)\n",
    "    demographics_final = df_with_categories.with_columns([\n",
    "        pl.when(pl.col(\"sex_at_birth\") == \"Male\")\n",
    "          .then(1)\n",
    "          .when(pl.col(\"sex_at_birth\") == \"Female\") \n",
    "          .then(0)\n",
    "          .otherwise(None)  # Exclude ambiguous cases\n",
    "          .cast(pl.Int8)\n",
    "          .alias(\"sex_male\"),\n",
    "        (pl.col(\"race_ethnicity\") == \"Hispanic_Latino\").cast(pl.Int8).alias(\"Hispanic_Latino\"),\n",
    "        (pl.col(\"race_ethnicity\") == \"Black_NonHispanic\").cast(pl.Int8).alias(\"Black_NonHispanic\"),\n",
    "        (pl.col(\"race_ethnicity\") == \"Asian_NonHispanic\").cast(pl.Int8).alias(\"Asian_NonHispanic\"),\n",
    "        (pl.col(\"race_ethnicity\") == \"Multiracial_NonHispanic\").cast(pl.Int8).alias(\"Multiracial_NonHispanic\"),\n",
    "        (pl.col(\"race_ethnicity\") == \"Other_Unknown\").cast(pl.Int8).alias(\"Other_Race_NonHispanic\")\n",
    "        # Note: White_NonHispanic omitted as reference category\n",
    "    ])\n",
    "\n",
    "    return demographics_final.drop('race_ethnicity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_with_dummies = create_demographics_dummies(demographics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify ICD Codes for PheRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_10_df = phecodeX_unrolled_icd_df.filter(pl.col('vocabulary_id')=='ICD10CM').select(['ICD']).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_137_pasc_df = zhang_pasc_list_df.filter(pl.col('137_pasc_category').is_not_null()).select(['137_pasc_category', '137_pasc_domain'])\n",
    "zhang_44_pasc_df = zhang_pasc_list_df.filter(pl.col('44_pasc_category').is_not_null()).select(['44_pasc_category', '44_pasc_domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_137_pasc_df = zhang_137_pasc_df.join(\n",
    "    zhang_pasc_icd_df.select(['ccsr_description', 'icd_10', 'icd_10_string', 'cluster_category']),\n",
    "    left_on='137_pasc_category',\n",
    "    right_on='ccsr_description',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_44_pasc_df = zhang_44_pasc_df.join(\n",
    "    zhang_pasc_icd_df.select(['ccsr_description', 'icd_10', 'icd_10_string', 'cluster_category']),\n",
    "    left_on='44_pasc_category',\n",
    "    right_on='ccsr_description',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_period_to_icd10(icd_code):\n",
    "    \"\"\"\n",
    "    Adds a period to an ICD-10 code in the correct position.\n",
    "    Standard format is a letter followed by 2 digits (or letter-digit-letter), \n",
    "    then a period, then additional digits.\n",
    "    \"\"\"\n",
    "    if not icd_code or len(icd_code) < 3:\n",
    "        return icd_code\n",
    "    \n",
    "    # Insert period after the third character (e.g., M16.12)\n",
    "    return f\"{icd_code[:3]}.{icd_code[3:]}\"\n",
    "\n",
    "# Create a mapping from no-period to with-period ICD-10 codes\n",
    "icd_mapping = {}\n",
    "\n",
    "# Option 1: Use a regex pattern to identify where periods should be in icd_10_df\n",
    "with_period_codes = icd_10_df.select(\"ICD\").to_series().to_list()\n",
    "\n",
    "# Option 2: If icd_10_df already has the periods, create a mapping by removing periods\n",
    "for code in with_period_codes:\n",
    "    # Create a version with period removed\n",
    "    no_period_code = code.replace(\".\", \"\")\n",
    "    icd_mapping[no_period_code] = code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update zhang_137_pasc_df\n",
    "zhang_137_pasc_df = zhang_137_pasc_df.with_columns([\n",
    "    pl.col(\"icd_10\").map_elements(\n",
    "        lambda x: icd_mapping.get(x, add_period_to_icd10(x)),\n",
    "        return_dtype = pl.String\n",
    "    ).alias(\"icd_10_with_period\")\n",
    "])\n",
    "\n",
    "# replace the original column\n",
    "zhang_137_pasc_df = zhang_137_pasc_df.drop(\"icd_10\").rename({\"icd_10_with_period\": \"icd_10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_137_pasc_df = zhang_137_pasc_df.with_columns(\n",
    "    pl.lit('ICD10CM').alias('vocabulary_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update zhang_44_pasc_df\n",
    "zhang_44_pasc_df = zhang_44_pasc_df.with_columns([\n",
    "    pl.col(\"icd_10\").map_elements(\n",
    "        lambda x: icd_mapping.get(x, add_period_to_icd10(x)),\n",
    "        return_dtype = pl.String\n",
    "    ).alias(\"icd_10_with_period\")\n",
    "])\n",
    "\n",
    "# replace the original column\n",
    "zhang_44_pasc_df = zhang_44_pasc_df.drop(\"icd_10\").rename({\"icd_10_with_period\": \"icd_10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_44_pasc_df = zhang_44_pasc_df.with_columns(\n",
    "    pl.lit('ICD10CM').alias('vocabulary_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable HR Threshold for Cox PheWAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(min_1_phecodes_df['long_covid_min_1_hazard_ratio'], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_min_1_phecodes_df = min_1_phecodes_df.filter(pl.col('long_covid_min_1_hazard_ratio')<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(filtered_min_1_phecodes_df['long_covid_min_1_hazard_ratio'], bins=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_serial_hr_threshold(significant_phecodes_df, hr_col):\n",
    "    \"\"\"\n",
    "    Create PheRS with incrementally stricter HR thresholds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort phecodes by HR (ascending)\n",
    "    sorted_phecodes = significant_phecodes_df.sort(hr_col)\n",
    "    \n",
    "    # Define removal percentiles\n",
    "    removal_percentiles = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for percentile in removal_percentiles:\n",
    "        n_total = len(sorted_phecodes)\n",
    "        n_keep = int(n_total * (100 - percentile) / 100)\n",
    "        \n",
    "        filtered_phecodes = sorted_phecodes.tail(n_keep) if n_keep > 0 else sorted_phecodes.head(0)\n",
    "        \n",
    "        if len(filtered_phecodes) == 0:\n",
    "            print(f\"Percentile {percentile}: No phecodes remaining\")\n",
    "            continue\n",
    "            \n",
    "        min_hr = filtered_phecodes[hr_col].min()\n",
    "        max_hr = filtered_phecodes[hr_col].max()\n",
    "        \n",
    "        print(f\"Percentile {percentile}: Keeping {n_keep}/{n_total} phecodes (HR range: {min_hr:.2f} - {max_hr:.2f})\")\n",
    "        \n",
    "        results[percentile] = filtered_phecodes\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_serial_hr_thresholds(significant_phecodes_df, hr_col, \n",
    "                               focus_range=(24, 46), step_by_one=True):\n",
    "    \"\"\"\n",
    "    Create PheRS with incrementally stricter HR thresholds.\n",
    "    Focus on fine-grained steps in the critical range where performance peaks.\n",
    "    \n",
    "    Parameters:\n",
    "    - significant_phecodes_df: DataFrame with phecodes and HR values\n",
    "    - hr_col: Column name containing hazard ratios\n",
    "    - focus_range: Tuple of (min_keep, max_keep) for fine-grained stepping\n",
    "    - step_by_one: If True, step by 1 phecode in focus range, else use percentiles\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort phecodes by HR (ascending - lowest HR first)\n",
    "    sorted_phecodes = significant_phecodes_df.sort(hr_col)\n",
    "    n_total = len(sorted_phecodes)\n",
    "    \n",
    "    # Define thresholds\n",
    "    thresholds_to_keep = []\n",
    "    \n",
    "    # Coarse steps outside the focus range\n",
    "    coarse_steps = [n_total, int(0.9*n_total), int(0.8*n_total), int(0.7*n_total), int(0.6*n_total)]\n",
    "    \n",
    "    # Add steps above focus range\n",
    "    for step in coarse_steps:\n",
    "        if step > focus_range[1]:\n",
    "            thresholds_to_keep.append(step)\n",
    "    \n",
    "    # Fine-grained steps in focus range\n",
    "    if step_by_one:\n",
    "        # Step by 1 phecode in critical range\n",
    "        for n_keep in range(focus_range[1], focus_range[0] - 1, -1):\n",
    "            thresholds_to_keep.append(n_keep)\n",
    "    else:\n",
    "        # Use percentile-based steps in focus range\n",
    "        focus_steps = list(range(focus_range[0], focus_range[1] + 1, 2))\n",
    "        thresholds_to_keep.extend(reversed(focus_steps))\n",
    "    \n",
    "    # Add steps below focus range\n",
    "    coarse_steps_low = [int(0.4*n_total), int(0.3*n_total), int(0.2*n_total), int(0.1*n_total)]\n",
    "    for step in coarse_steps_low:\n",
    "        if step < focus_range[0]:\n",
    "            thresholds_to_keep.append(step)\n",
    "    \n",
    "    # Remove duplicates and sort descending (most inclusive first)\n",
    "    thresholds_to_keep = sorted(list(set(thresholds_to_keep)), reverse=True)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for n_keep in thresholds_to_keep:\n",
    "        if n_keep <= 0 or n_keep > n_total:\n",
    "            continue\n",
    "            \n",
    "        # Take the top n_keep phecodes (highest HRs)\n",
    "        filtered_phecodes = sorted_phecodes.tail(n_keep)\n",
    "        \n",
    "        if len(filtered_phecodes) == 0:\n",
    "            print(f\"Keeping {n_keep}: No phecodes remaining\")\n",
    "            continue\n",
    "            \n",
    "        min_hr = filtered_phecodes[hr_col].min()\n",
    "        max_hr = filtered_phecodes[hr_col].max()\n",
    "        \n",
    "        print(f\"Keeping {n_keep}/{n_total} phecodes (HR range: {min_hr:.2f} - {max_hr:.2f})\")\n",
    "        \n",
    "        # Use n_keep as key for easier interpretation\n",
    "        results[n_keep] = filtered_phecodes\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phecode_thresholds = create_serial_hr_thresholds(min_1_phecodes_df, 'long_covid_min_1_hazard_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phecode_thresholds[37].sort('long_covid_min_1_hazard_ratio', descending=True)['phecode', 'phecode_string', 'phecode_category', 'long_covid_min_1_hazard_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in ICD Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percentile in phecode_thresholds.keys():\n",
    "    phecode_thresholds[percentile] = phecode_thresholds[percentile].join(\n",
    "        phecodeX_unrolled_icd_df,\n",
    "        on='phecode',\n",
    "        how='left'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Phecode Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phecode = Phecode(platform=\"aou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phecode.icd_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add ICD 9 and 10 flags to demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary indicators for ICD-9/10 presence and calculate EHR length\n",
    "icd_indicators = (\n",
    "    phecode.icd_events\n",
    "    .group_by('person_id')\n",
    "    .agg([\n",
    "        pl.col('flag').eq(9).any().cast(pl.Int8).alias('has_icd9'),\n",
    "        pl.col('flag').eq(10).any().cast(pl.Int8).alias('has_icd10'),\n",
    "        (pl.col('date').max() - pl.col('date').min()).dt.total_days().truediv(365.25).alias('ehr_length_years')\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_with_dummies = demographics_with_dummies.join(\n",
    "    icd_indicators,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phecode Prevalence Weights (Inverse Log Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_new_post_covid_icds(icd_events_df, split_df, post_covid_start=28, post_covid_end=365):\n",
    "    \"\"\"\n",
    "    Identify ICD codes that are NEW in the post-COVID period\n",
    "    Uses ALL available pre-COVID history\n",
    "    \"\"\"\n",
    "    # Join to get time_zero for each person\n",
    "    joined_df = icd_events_df.join(\n",
    "        split_df.select(['person_id', 'time_zero']),\n",
    "        on='person_id',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Calculate days since time_zero\n",
    "    with_days = joined_df.with_columns([\n",
    "        (pl.col('date') - pl.col('time_zero')).dt.total_days().alias('days_since_time_zero')\n",
    "    ])\n",
    "    \n",
    "    # Split into pre-COVID and post-COVID periods\n",
    "    pre_covid_events = with_days.filter(\n",
    "        pl.col('days_since_time_zero') < 0  # all history before COVID\n",
    "    )\n",
    "    \n",
    "    post_covid_events = with_days.filter(\n",
    "        (pl.col('days_since_time_zero') >= post_covid_start) & \n",
    "        (pl.col('days_since_time_zero') <= post_covid_end)  # 28-365 days post-COVID\n",
    "    )\n",
    "    \n",
    "    # Get unique person-ICD combinations from pre-COVID period\n",
    "    pre_covid_icds = pre_covid_events.select(['person_id', 'ICD', 'vocabulary_id']).unique()\n",
    "    \n",
    "    # Mark pre-COVID ICD codes\n",
    "    pre_covid_person_icds = pre_covid_icds.with_columns(\n",
    "        pl.lit(True).alias('had_pre_covid')\n",
    "    )\n",
    "    \n",
    "    # Find post-COVID ICD codes that are new (not present pre-COVID)\n",
    "    new_post_covid_icds = post_covid_events.join(\n",
    "        pre_covid_person_icds,\n",
    "        on=['person_id', 'ICD', 'vocabulary_id'],\n",
    "        how='left'\n",
    "    ).filter(\n",
    "        pl.col('had_pre_covid').is_null()  # Only keep ICD codes not present pre-COVID\n",
    "    ).drop('had_pre_covid')\n",
    "    \n",
    "    print(f\"Pre-COVID events: {len(pre_covid_events):,}\")\n",
    "    print(f\"Post-COVID events: {len(post_covid_events):,}\")\n",
    "    print(f\"New post-COVID ICD events: {len(new_post_covid_icds):,}\")\n",
    "    print(f\"Patients with new ICD codes: {new_post_covid_icds.select(pl.n_unique('person_id')).item():,}\")\n",
    "    \n",
    "    return new_post_covid_icds\n",
    "\n",
    "# Apply to both cohorts\n",
    "print('Training set:')\n",
    "train_new_icd_events = identify_new_post_covid_icds(phecode.icd_events, train_df)\n",
    "print('\\nTest set:')\n",
    "test_new_icd_events = identify_new_post_covid_icds(phecode.icd_events, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long COVID PheWAS Events and Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_phewas_test_phecode_events = {}\n",
    "cox_phewas_test_N = {}\n",
    "cox_phewas_test_phecode_counts = {}\n",
    "\n",
    "# Test set new events\n",
    "for percentile in phecode_thresholds.keys():\n",
    "    # Join ICD events with phecode mapping\n",
    "    cox_phewas_test_phecode_events[percentile] = test_new_icd_events.join(\n",
    "        phecode_thresholds[percentile].select(['ICD', 'vocabulary_id', 'phecode']),\n",
    "        on=['ICD', 'vocabulary_id'],\n",
    "        how='inner'  # Only keep events that map to a phecode\n",
    "    )\n",
    "\n",
    "    # Get total number of patients in the cohort\n",
    "    cox_phewas_test_N[percentile] = test_new_icd_events.select(pl.n_unique('person_id')).item()\n",
    "\n",
    "    # Count unique patients for each phecode\n",
    "    cox_phewas_test_phecode_counts[percentile] = (cox_phewas_test_phecode_events[percentile]\n",
    "        .group_by('phecode')\n",
    "        .agg(\n",
    "            pl.n_unique('person_id').alias('patient_count')\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_phewas_train_phecode_events = {}\n",
    "cox_phewas_train_N = {}\n",
    "cox_phewas_train_phecode_counts = {}\n",
    "cox_phewas_train_weights = {}\n",
    "\n",
    "# Training set new events and weights\n",
    "for percentile in phecode_thresholds.keys():\n",
    "    # Join ICD events with phecode mapping\n",
    "    cox_phewas_train_phecode_events[percentile] = train_new_icd_events.join(\n",
    "        phecode_thresholds[percentile].select(['ICD', 'vocabulary_id', 'phecode']),\n",
    "        on=['ICD', 'vocabulary_id'],\n",
    "        how='inner'  # Only keep events that map to a phecode\n",
    "    )\n",
    "\n",
    "    # Get total number of patients in the cohort\n",
    "    cox_phewas_train_N[percentile] = train_new_icd_events.select(pl.n_unique('person_id')).item()\n",
    "\n",
    "    # Count unique patients for each phecode\n",
    "    cox_phewas_train_phecode_counts[percentile] = (cox_phewas_train_phecode_events[percentile]\n",
    "        .group_by('phecode')\n",
    "        .agg(\n",
    "            pl.n_unique('person_id').alias('patient_count')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Calculate weights as log(N/np)\n",
    "    cox_phewas_train_weights[percentile] = cox_phewas_train_phecode_counts[percentile].with_columns([\n",
    "        pl.lit(cox_phewas_train_N[percentile]).alias('total_patients'),\n",
    "        (pl.lit(cox_phewas_train_N[percentile]) / pl.col('patient_count')).log10().alias('weight')\n",
    "    ])\n",
    "\n",
    "    # Join with phecode descriptions for better readability\n",
    "    cox_phewas_train_weights[percentile] = cox_phewas_train_weights[percentile].join(\n",
    "        phecode_thresholds[percentile].select(['phecode', 'phecode_string', 'phecode_category',\n",
    "                                               'long_covid_min_1_hazard_ratio_low',\n",
    "                                               'long_covid_min_1_hazard_ratio_high', \n",
    "                                               'long_covid_min_1_standard_error',\n",
    "                                               'long_covid_min_1_hazard_ratio',]).unique(),\n",
    "        on='phecode',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    cox_phewas_train_weights[percentile].write_csv(f'{bucket}/data/long_covid/phers/variable_hr/long_covid_min_1_pct_{percentile}_phewas_train_phecode_weights.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhang Events and Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang test events\n",
    "# Join ICD events with phecode mapping\n",
    "zhang_137_test_phecode_events_df = test_new_icd_events.join(\n",
    "    zhang_137_pasc_df.select(['icd_10', 'vocabulary_id', '137_pasc_category']),\n",
    "    left_on=['ICD', 'vocabulary_id'],\n",
    "    right_on=['icd_10', 'vocabulary_id'],\n",
    "    how='inner'  # Only keep events that map to a phecode\n",
    ")\n",
    "\n",
    "# Get total number of patients in the cohort\n",
    "zhang_137_test_N = test_new_icd_events.select(pl.n_unique('person_id')).item()\n",
    "\n",
    "# Count unique patients for each phecode\n",
    "zhang_137_test_phecode_counts = (zhang_137_test_phecode_events_df\n",
    "    .group_by('137_pasc_category')\n",
    "    .agg(\n",
    "        pl.n_unique('person_id').alias('patient_count')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Zhang training events and weights\n",
    "# Join ICD events with phecode mapping\n",
    "zhang_137_train_phecode_events_df = train_new_icd_events.join(\n",
    "    zhang_137_pasc_df.select(['icd_10', 'vocabulary_id', '137_pasc_category']),\n",
    "    left_on=['ICD', 'vocabulary_id'],\n",
    "    right_on=['icd_10', 'vocabulary_id'],\n",
    "    how='inner'  # Only keep events that map to a phecode\n",
    ")\n",
    "\n",
    "# Get total number of patients in the cohort\n",
    "zhang_137_train_N = train_new_icd_events.select(pl.n_unique('person_id')).item()\n",
    "\n",
    "# Count unique patients for each phecode\n",
    "zhang_137_train_phecode_counts = (zhang_137_train_phecode_events_df\n",
    "    .group_by('137_pasc_category')\n",
    "    .agg(\n",
    "        pl.n_unique('person_id').alias('patient_count')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate weights as log(N/np)\n",
    "zhang_137_train_weights_df = zhang_137_train_phecode_counts.with_columns([\n",
    "    pl.lit(zhang_137_train_N).alias('total_patients'),\n",
    "    (pl.lit(zhang_137_train_N) / pl.col('patient_count')).log10().alias('weight')\n",
    "])\n",
    "\n",
    "# Join with phecode descriptions for better readability\n",
    "zhang_137_train_weights_df = zhang_137_train_weights_df.join(\n",
    "    zhang_137_pasc_df.select(['137_pasc_category', '137_pasc_domain', 'cluster_category']).unique(),\n",
    "    on='137_pasc_category',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "zhang_137_train_weights_df.write_csv(f'{bucket}/data/long_covid/phers/zhang_137_train_PASC_phecode_weights.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang test events\n",
    "# Join ICD events with phecode mapping\n",
    "zhang_44_test_phecode_events_df = test_new_icd_events.join(\n",
    "    zhang_44_pasc_df.select(['icd_10', 'vocabulary_id', '44_pasc_category']),\n",
    "    left_on=['ICD', 'vocabulary_id'],\n",
    "    right_on=['icd_10', 'vocabulary_id'],\n",
    "    how='inner'  # Only keep events that map to a phecode\n",
    ")\n",
    "\n",
    "# Get total number of patients in the cohort\n",
    "zhang_44_test_N = test_new_icd_events.select(pl.n_unique('person_id')).item()\n",
    "\n",
    "# Count unique patients for each phecode\n",
    "zhang_44_test_phecode_counts = (zhang_44_test_phecode_events_df\n",
    "    .group_by('44_pasc_category')\n",
    "    .agg(\n",
    "        pl.n_unique('person_id').alias('patient_count')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Zhang training events and weights\n",
    "# Join ICD events with phecode mapping\n",
    "zhang_44_train_phecode_events_df = train_new_icd_events.join(\n",
    "    zhang_44_pasc_df.select(['icd_10', 'vocabulary_id', '44_pasc_category']),\n",
    "    left_on=['ICD', 'vocabulary_id'],\n",
    "    right_on=['icd_10', 'vocabulary_id'],\n",
    "    how='inner'  # Only keep events that map to a phecode\n",
    ")\n",
    "\n",
    "# Get total number of patients in the cohort\n",
    "zhang_44_train_N = train_new_icd_events.select(pl.n_unique('person_id')).item()\n",
    "\n",
    "# Count unique patients for each phecode\n",
    "zhang_44_train_phecode_counts = (zhang_44_train_phecode_events_df\n",
    "    .group_by('44_pasc_category')\n",
    "    .agg(\n",
    "        pl.n_unique('person_id').alias('patient_count')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate weights as log(N/np)\n",
    "zhang_44_train_weights_df = zhang_44_train_phecode_counts.with_columns([\n",
    "    pl.lit(zhang_44_train_N).alias('total_patients'),\n",
    "    (pl.lit(zhang_44_train_N) / pl.col('patient_count')).log10().alias('weight')\n",
    "])\n",
    "\n",
    "# Join with phecode descriptions for better readability\n",
    "zhang_44_train_weights_df = zhang_44_train_weights_df.join(\n",
    "    zhang_44_pasc_df.select(['44_pasc_category', '44_pasc_domain', 'cluster_category']).unique(),\n",
    "    on='44_pasc_category',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "zhang_44_train_weights_df.write_csv(f'{bucket}/data/long_covid/phers/zhang_44_train_PASC_phecode_weights.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate PheRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calculate_phers_for_patients(phecode_events, phecode_weights, phenotype, label):\n",
    "    # Create a patient-phenotype presence matrix (1 if patient has phenotype, 0 otherwise)\n",
    "    # We only need to know if a patient has a phecode at least once\n",
    "    patient_phecode_matrix = (phecode_events\n",
    "        .select(['person_id', phenotype])\n",
    "        .unique()\n",
    "        .with_columns(pl.lit(1).alias('has_phecode'))\n",
    "    )\n",
    "    \n",
    "    # Join with weights\n",
    "    weighted_matrix = patient_phecode_matrix.join(\n",
    "        phecode_weights.select([phenotype, 'weight']),\n",
    "        on=phenotype,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Sum weights for each patient to get PheRS\n",
    "    phers_scores = weighted_matrix.group_by('person_id').agg(\n",
    "        pl.sum('weight').alias(label)\n",
    "    )\n",
    "    \n",
    "    return phers_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weight_variants(phewas_train_weights_df):    \n",
    "    weights_df = phewas_train_weights_df.with_columns([\n",
    "        # prevalence-based weight\n",
    "        pl.col('weight').alias('prevalence_weight'),\n",
    "               \n",
    "        # z-statistic\n",
    "        (pl.col('long_covid_min_1_hazard_ratio') / pl.col('long_covid_min_1_standard_error')).alias('z_stat_weight'),\n",
    "    ])\n",
    "    \n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the weight variants for PheRS approach\n",
    "cox_phewas_train_weights_variants = {}\n",
    "\n",
    "for percentile in phecode_thresholds.keys():\n",
    "    cox_phewas_train_weights_variants[percentile] = create_weight_variants(cox_phewas_train_weights[percentile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weight_conditions = []\n",
    "\n",
    "# Calculate PheRS for each weighting approach\n",
    "for percentile in phecode_thresholds.keys():\n",
    "    weight_approaches = [\n",
    "        ('prevalence_weight', f'cox_phewas_{percentile}_prevalence_phers'),\n",
    "        ('z_stat_weight', f'cox_phewas_{percentile}_z_stat_phers'), \n",
    "    ]\n",
    "    all_weight_conditions.extend([f'cox_phewas_{percentile}_prevalence_phers', f'cox_phewas_{percentile}_z_stat_phers'])\n",
    "\n",
    "    for weight_col, phers_label in weight_approaches:\n",
    "        # Create temporary weights dataframe for this approach\n",
    "        temp_weights = cox_phewas_train_weights_variants[percentile].select(['phecode', weight_col]).rename({weight_col: 'weight'})\n",
    "\n",
    "        # Calculate training PheRS\n",
    "        temp_train_phers = calculate_phers_for_patients(cox_phewas_train_phecode_events[percentile], \n",
    "                                                 temp_weights, \n",
    "                                                 phenotype='phecode',\n",
    "                                                 label=phers_label)\n",
    "\n",
    "        # Calculate test PheRS  \n",
    "        temp_test_phers = calculate_phers_for_patients(cox_phewas_test_phecode_events[percentile], \n",
    "                                                temp_weights, \n",
    "                                                phenotype='phecode',\n",
    "                                                label=phers_label)\n",
    "\n",
    "        # Join to main dataframes\n",
    "        train_df = train_df.join(temp_train_phers, on='person_id', how='left').with_columns(\n",
    "            pl.col(phers_label).fill_null(0)\n",
    "        )\n",
    "\n",
    "        test_df = test_df.join(temp_test_phers, on='person_id', how='left').with_columns(\n",
    "            pl.col(phers_label).fill_null(0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PheRS for each patient\n",
    "zhang_137_train_phers_df = calculate_phers_for_patients(zhang_137_train_phecode_events_df, \n",
    "                                               zhang_137_train_weights_df, \n",
    "                                               phenotype='137_pasc_category',\n",
    "                                               label='zhang_137_phers')\n",
    "\n",
    "zhang_137_test_phers_df = calculate_phers_for_patients(zhang_137_test_phecode_events_df, \n",
    "                                               zhang_137_train_weights_df, \n",
    "                                               phenotype='137_pasc_category',\n",
    "                                               label='zhang_137_phers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PheRS for each patient\n",
    "zhang_44_train_phers_df = calculate_phers_for_patients(zhang_44_train_phecode_events_df, \n",
    "                                               zhang_44_train_weights_df, \n",
    "                                               phenotype='44_pasc_category',\n",
    "                                               label='zhang_44_phers')\n",
    "\n",
    "zhang_44_test_phers_df = calculate_phers_for_patients(zhang_44_test_phecode_events_df, \n",
    "                                               zhang_44_train_weights_df, \n",
    "                                               phenotype='44_pasc_category',\n",
    "                                               label='zhang_44_phers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.join(\n",
    "    zhang_137_train_phers_df,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ").with_columns(\n",
    "    pl.col('zhang_137_phers').fill_null(0)\n",
    ")\n",
    "train_df = train_df.join(\n",
    "    zhang_44_train_phers_df,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ").with_columns(\n",
    "    pl.col('zhang_44_phers').fill_null(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.join(\n",
    "    zhang_137_test_phers_df,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ").with_columns(\n",
    "    pl.col('zhang_137_phers').fill_null(0)\n",
    ")\n",
    "test_df = test_df.join(\n",
    "    zhang_44_test_phers_df,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ").with_columns(\n",
    "    pl.col('zhang_44_phers').fill_null(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weight_conditions.extend(['zhang_137_phers', 'zhang_44_phers'])\n",
    "\n",
    "for col in all_weight_conditions:\n",
    "    zero_pct = (train_df.filter(pl.col(col) == 0).height / train_df.height) * 100\n",
    "    print(f\"{col}: {zero_pct:.1f}% have score = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residualize PheRS Using Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Demographics and Residualization Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.join(\n",
    "    demographics_with_dummies.select(['person_id', 'dob', 'sex_male', 'Hispanic_Latino',\n",
    "                                      'Black_NonHispanic', 'Asian_NonHispanic', 'Multiracial_NonHispanic', \n",
    "                                      'Other_Race_NonHispanic', 'has_icd9', 'has_icd10', 'ehr_length_years']),\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ")\n",
    "test_df = test_df.join(\n",
    "    demographics_with_dummies.select(['person_id', 'dob', 'sex_male', 'Hispanic_Latino',\n",
    "                                      'Black_NonHispanic', 'Asian_NonHispanic', 'Multiracial_NonHispanic', \n",
    "                                      'Other_Race_NonHispanic', 'has_icd9', 'has_icd10', 'ehr_length_years']),\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.with_columns([\n",
    "    (pl.col(\"time_zero\").dt.date() - pl.col(\"dob\")).dt.total_days()\n",
    "    .truediv(365.25)\n",
    "    .alias(\"age_years\")\n",
    "])\n",
    "test_df = test_df.with_columns([\n",
    "    (pl.col(\"time_zero\").dt.date() - pl.col(\"dob\")).dt.total_days()\n",
    "    .truediv(365.25)\n",
    "    .alias(\"age_years\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns have NaNs\n",
    "nan_check = train_df.select([\n",
    "    pl.col('age_years').is_null().sum().alias('age_years_nulls'),\n",
    "    pl.col('sex_male').is_null().sum().alias('sex_male_nulls'),\n",
    "    pl.col('Hispanic_Latino').is_null().sum().alias('Hispanic_Latino_nulls'),\n",
    "    pl.col('Black_NonHispanic').is_null().sum().alias('Black_NonHispanic_nulls'),\n",
    "    pl.col('Asian_NonHispanic').is_null().sum().alias('Asian_NonHispanic_nulls'),\n",
    "    pl.col('Multiracial_NonHispanic').is_null().sum().alias('Multiracial_NonHispanic_nulls'),\n",
    "    pl.col('Other_Race_NonHispanic').is_null().sum().alias('Other_Race_NonHispanic_nulls'),\n",
    "])\n",
    "\n",
    "nan_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing missing values for residualization...\")\n",
    "original_len = len(train_df)\n",
    "\n",
    "print(f\"Train N: {original_len}\")\n",
    "\n",
    "# Filter out people with missing sex\n",
    "train_df = train_df.filter(pl.col('sex_male').is_not_null())\n",
    "\n",
    "print(f\"After excluding {original_len-len(train_df)} with missing sex: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing missing values for residualization...\")\n",
    "original_len = len(test_df)\n",
    "\n",
    "print(f\"Test N: {original_len}\")\n",
    "\n",
    "# Filter out people with missing sex\n",
    "test_df = test_df.filter(pl.col('sex_male').is_not_null())\n",
    "\n",
    "print(f\"After excluding {original_len-len(test_df)} with missing sex: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weight_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residualize_phers_all_cases(train_df, test_df, phers_cols=all_weight_conditions):\n",
    "    \"\"\"\n",
    "    Residualization including cases with PheRS of 0\n",
    "    \"\"\"\n",
    "\n",
    "    residualized_scores = {}\n",
    "    models = {}\n",
    "\n",
    "    for phers_col in phers_cols:\n",
    "        print(f\"Residualizing {phers_col}...\")\n",
    "\n",
    "        # 1. FIT MODEL ON TRAINING SET ONLY\n",
    "        X_train, age_spline, feature_cols = prepare_residualization_features(train_df)\n",
    "        train_pd = train_df.to_pandas()\n",
    "        train_mask = train_pd[phers_col].notna()\n",
    "\n",
    "        y_train = train_pd[phers_col][train_mask].values\n",
    "        X_train_clean = X_train[train_mask]\n",
    "\n",
    "        # Fit residualization model on training data\n",
    "        resid_model = LinearRegression()\n",
    "        resid_model.fit(X_train_clean, y_train)\n",
    "\n",
    "        # Calculate training residuals\n",
    "        y_train_pred = resid_model.predict(X_train_clean)\n",
    "        train_residuals = y_train - y_train_pred\n",
    "        train_mse = np.mean(train_residuals**2)  # Use training MSE for standardization\n",
    "\n",
    "        # 2. APPLY MODEL TO TRAINING SET\n",
    "        train_studentized = train_residuals / np.sqrt(train_mse)\n",
    "\n",
    "        # 3. APPLY SAME MODEL TO TEST SET\n",
    "        X_test, _, _ = prepare_residualization_features_with_fitted_spline(test_df, age_spline, feature_cols)\n",
    "        test_pd = test_df.to_pandas()\n",
    "        test_mask = test_pd[phers_col].notna()\n",
    "\n",
    "        y_test = test_pd[phers_col][test_mask].values\n",
    "        X_test_clean = X_test[test_mask]\n",
    "\n",
    "        # Apply training model to test data\n",
    "        y_test_pred = resid_model.predict(X_test_clean)\n",
    "        test_residuals = y_test - y_test_pred\n",
    "        test_studentized = test_residuals / np.sqrt(train_mse)  # Use training MSE for standardization\n",
    "\n",
    "        # Store results\n",
    "        residualized_scores[f'train_{phers_col}'] = train_studentized\n",
    "        residualized_scores[f'test_{phers_col}'] = test_studentized\n",
    "        models[phers_col] = (resid_model, age_spline, train_mse)\n",
    "\n",
    "        print(f\"  Training - Original: mean={y_train.mean():.3f}, Residualized: mean={train_studentized.mean():.3f}\")\n",
    "        print(f\"  Test - Original: mean={y_test.mean():.3f}, Residualized: mean={test_studentized.mean():.3f}\")\n",
    "\n",
    "    return residualized_scores, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_residualization_features(df):\n",
    "    \"\"\"\n",
    "    Prepare features for the residualization model\n",
    "    \"\"\"\n",
    "    # Handle both pandas and polars input\n",
    "    if hasattr(df, 'to_pandas'):  # It's a polars DataFrame\n",
    "        df_pd = df.to_pandas()\n",
    "    else:  # It's already pandas\n",
    "        df_pd = df\n",
    "    \n",
    "    # Age spline (cubic with 3 knots)\n",
    "    age_spline = SplineTransformer(n_knots=3, degree=3, include_bias=False)\n",
    "    age_features = age_spline.fit_transform(df_pd[['age_years']].values)\n",
    "    \n",
    "    # Collect all features\n",
    "    feature_cols = [\n",
    "        'sex_male',\n",
    "        'Hispanic_Latino', 'Black_NonHispanic', 'Asian_NonHispanic', \n",
    "        'Multiracial_NonHispanic', 'Other_Race_NonHispanic', \n",
    "        'has_icd9', 'has_icd10', 'ehr_length_years'\n",
    "    ]\n",
    "        \n",
    "    # Stack features\n",
    "    demographic_features = df_pd[feature_cols].values\n",
    "    X = np.hstack([age_features, demographic_features])\n",
    "    \n",
    "    return X, age_spline, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_residualization_features_with_fitted_spline(df, fitted_age_spline, feature_cols):\n",
    "    \"\"\"\n",
    "    Apply already-fitted age spline to new data\n",
    "    \"\"\"\n",
    "    df_pd = df.to_pandas()\n",
    "    \n",
    "    # Use the already-fitted spline\n",
    "    age_features = fitted_age_spline.transform(df_pd[['age_years']].values)\n",
    "    \n",
    "    # Get demographic features\n",
    "    demographic_features = df_pd[feature_cols].values\n",
    "    X = np.hstack([age_features, demographic_features])\n",
    "    \n",
    "    return X, fitted_age_spline, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residualized scores back to your dataframes\n",
    "def add_residualized_scores_to_dfs(train_df, test_df, residualized_scores):\n",
    "    \"\"\"\n",
    "    Add residualized scores back to train and test dataframes\n",
    "    \"\"\"\n",
    "    # Convert to pandas for easier manipulation\n",
    "    train_pd = train_df.to_pandas()\n",
    "    test_pd = test_df.to_pandas()\n",
    "    \n",
    "    # Add training residualized scores\n",
    "    for score_name, scores in residualized_scores.items():\n",
    "        if score_name.startswith('train_'):\n",
    "            col_name = score_name.replace('train_', 'r')  # e.g., 'train_phewas_phers' -> 'rphewas_phers'\n",
    "            \n",
    "            # Create full array with NaNs\n",
    "            full_scores = np.full(len(train_pd), np.nan)\n",
    "            \n",
    "            # Find which rows had valid scores\n",
    "            original_col = score_name.replace('train_', '')  # e.g., 'train_phewas_phers' -> 'phewas_phers'\n",
    "            mask = train_pd[original_col].notna()\n",
    "            full_scores[mask] = scores\n",
    "            \n",
    "            train_pd[col_name] = full_scores\n",
    "    \n",
    "    # Add test residualized scores  \n",
    "    for score_name, scores in residualized_scores.items():\n",
    "        if score_name.startswith('test_'):\n",
    "            col_name = score_name.replace('test_', 'r')  # e.g., 'test_phewas_phers' -> 'rphewas_phers'\n",
    "            \n",
    "            # Create full array with NaNs\n",
    "            full_scores = np.full(len(test_pd), np.nan)\n",
    "            \n",
    "            # Find which rows had valid scores\n",
    "            original_col = score_name.replace('test_', '')  # e.g., 'test_phewas_phers' -> 'phewas_phers'\n",
    "            mask = test_pd[original_col].notna()\n",
    "            full_scores[mask] = scores\n",
    "            \n",
    "            test_pd[col_name] = full_scores\n",
    "    \n",
    "    # Convert back to polars\n",
    "    train_final = pl.from_pandas(train_pd)\n",
    "    test_final = pl.from_pandas(test_pd)\n",
    "    \n",
    "    return train_final, test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero PheRS = \"No qualifying high-risk phenotypes\"\n",
    "- Negative rPheRS = \"Even healthier than demographics would predict\"\n",
    "- Positive rPheRS = \"Higher burden than expected for demographic group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fix the function and apply residualization\n",
    "residualized_scores, resid_models = residualize_phers_all_cases(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add scores back to dataframes\n",
    "train_final, test_final = add_residualized_scores_to_dfs(train_df, test_df, residualized_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.write_csv(f'{bucket}/data/long_covid/phers/variable_hr/residualized_phers_train_plus_set.tsv', \n",
    "                      separator='\\t')\n",
    "test_final.write_csv(f'{bucket}/data/long_covid/phers/variable_hr/residualized_phers_test_plus_set.tsv', \n",
    "                      separator='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
