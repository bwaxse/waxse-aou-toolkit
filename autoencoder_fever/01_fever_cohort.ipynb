{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65ae45a-3c23-4b27-ba9b-e0c5b877fc1f",
   "metadata": {},
   "source": [
    "# Define Fever Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14309c25-22db-40b7-a1f9-7bc5d944f622",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ae923-8c1f-4aa8-bfff-6c498b2b9d84",
   "metadata": {},
   "source": [
    "### Bucket and CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f657dac0-0f76-45cd-8fb2-9f2c6ebfabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4c6607-5d37-4169-8cf8-3fbec4597c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26289f59-0833-4cfc-84da-981a27076271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551c7c92-21d9-4117-b3a3-0da2b8e7051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omop_unifier import Explorer, Mapper, Unifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588027a9-9575-4893-a9dc-53844989ae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully:\n",
      "  dataset: wb-silky-artichoke-2408.C2024Q3R8\n",
      "  bucket: gs://workspace-bucket-wb-jaunty-date-2788\n",
      "  temp_bucket: gs://temporary-workspace-bucket-wb-jaunty-date-2788\n"
     ]
    }
   ],
   "source": [
    "from aou_helpers import load_aou_env\n",
    "\n",
    "# Load config (this also sets os.environ)\n",
    "env = load_aou_env()\n",
    "\n",
    "dataset = os.environ['WORKSPACE_CDR']\n",
    "bucket = os.environ['WORKSPACE_BUCKET']\n",
    "temp_bucket = os.environ['WORKSPACE_TEMP_BUCKET']\n",
    "gcproject = os.environ['GOOGLE_CLOUD_PROJECT']\n",
    "\n",
    "# Verify variables are accessible via os.environ\n",
    "print(\"Environment variables loaded successfully:\")\n",
    "print(f\"  dataset: {dataset}\")\n",
    "print(f\"  bucket: {bucket}\")\n",
    "print(f\"  temp_bucket: {temp_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0894a1-cf4d-4bad-b396-70d9dcc2fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line allows for the plots to be displayed inline in the Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"ticks\",font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aa640ee-47fa-4e69-8235-fac02a1cddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_fmt_str_lengths(128)\n",
    "\n",
    "# Set the row limit to a higher value\n",
    "pl.Config.set_tbl_rows(50)\n",
    "\n",
    "# show all columns in pandas\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# show full column width\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7e710-6f1e-4232-8cca-be5b3279b6b1",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00fc2d7-17e7-49d6-9648-f4e760fb3323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_resource_usage():\n",
    "  \"\"\"Monitor memory and CPU usage\"\"\"\n",
    "  memory = psutil.virtual_memory()\n",
    "  cpu_percent = psutil.cpu_percent(interval=1)\n",
    "\n",
    "  print(\"\\n\")\n",
    "  print(\"=\"*50)\n",
    "  print(f\"Memory: {memory.used / 1e9:.1f}GB / {memory.total / 1e9:.1f}GB \"\n",
    "        f\"({memory.percent:.1f}% used.) CPU: {cpu_percent:.1f}% used across {os.cpu_count()} cores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9036f016-15a5-46b8-a703-b399a46dfde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_gbq(query):\n",
    "    \"\"\"Execute BigQuery SQL and return polars DataFrame\"\"\"\n",
    "    client = bigquery.Client()\n",
    "    return pl.from_arrow(client.query(query).result().to_arrow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4fe729-5edc-482b-95e8-0296f1ea396a",
   "metadata": {},
   "source": [
    "# Get Total Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ade6e3a8-00a7-4c5f-bf6c-0bcc70a59f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vitals exclusion concept IDs (from sarcoid project)\n",
    "# These are measurements collected on all AoU participants (not indicative of actual clinical measurements)\n",
    "# Heart rate rhythm, Heart rate, Blood pressure panel, Adult Waist Circumference Protocol,\n",
    "# PhenX - hip circumference protocol, Body height, Body weight, Diastolic BP, Systolic BP, BMI, Pre-pregnancy weight\n",
    "VITALS_EXCLUSION = [3022318, 3027018, 3031203, 40759207, 40765148, 3036277,\n",
    "                    3025315, 3012888, 3004249, 3038553, 3022281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5649bcd-6079-4f95-bf9a-e0e2d17fbe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participants with medications: 334,643\n",
      "Participants with ICD codes: 356,363\n",
      "Participants with lab measurements: 348,126\n",
      "\n",
      "N = 315,945 participants with data from ALL modalities\n",
      "  (vitals AND medications AND ICD codes AND labs)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Memory: 3.2GB / 135.2GB (2.4% used.) CPU: 0.0% used across 32 cores.\n"
     ]
    }
   ],
   "source": [
    "# Count Participants with Relevant EHR Data\n",
    "# Requirement: Must have data from ALL modalities (vitals AND meds AND ICD AND labs)\n",
    "\n",
    "# Participants with medications\n",
    "meds_query = f\"\"\"\n",
    "SELECT DISTINCT person_id\n",
    "FROM `{dataset}`.drug_exposure AS de\n",
    "JOIN `{dataset}`.concept AS c\n",
    "    ON de.drug_concept_id = c.concept_id\n",
    "WHERE c.domain_id = 'Drug'\n",
    "\"\"\"\n",
    "meds_persons = polars_gbq(meds_query)\n",
    "print(f\"Participants with medications: {len(meds_persons):,}\")\n",
    "\n",
    "# Participants with ICD codes\n",
    "# Checks both observation and condition_occurrence tables\n",
    "# Checks both source_value and source_concept_id paths\n",
    "icd_query = f\"\"\"\n",
    "SELECT DISTINCT person_id FROM (\n",
    "  -- Codes from observation (source_value)\n",
    "  SELECT o.person_id\n",
    "  FROM `{dataset}`.observation AS o\n",
    "  JOIN `{dataset}`.concept AS c ON o.observation_source_value = c.concept_code\n",
    "  WHERE c.vocabulary_id IN ('ICD9CM', 'ICD10CM', 'SNOMED')\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- Codes from observation (source_concept_id)\n",
    "  SELECT o.person_id\n",
    "  FROM `{dataset}`.observation AS o\n",
    "  JOIN `{dataset}`.concept AS c ON o.observation_source_concept_id = c.concept_id\n",
    "  WHERE c.vocabulary_id IN ('ICD9CM', 'ICD10CM', 'SNOMED')\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- Codes from condition_occurrence (source_value)\n",
    "  SELECT co.person_id\n",
    "  FROM `{dataset}`.condition_occurrence AS co\n",
    "  JOIN `{dataset}`.concept AS c ON co.condition_source_value = c.concept_code\n",
    "  WHERE c.vocabulary_id IN ('ICD9CM', 'ICD10CM', 'SNOMED')\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- Codes from condition_occurrence (source_concept_id)\n",
    "  SELECT co.person_id\n",
    "  FROM `{dataset}`.condition_occurrence AS co\n",
    "  JOIN `{dataset}`.concept AS c ON co.condition_source_concept_id = c.concept_id\n",
    "  WHERE c.vocabulary_id IN ('ICD9CM', 'ICD10CM', 'SNOMED')\n",
    ")\n",
    "\"\"\"\n",
    "icd_persons = polars_gbq(icd_query)\n",
    "print(f\"Participants with ICD codes: {len(icd_persons):,}\")\n",
    "\n",
    "# Participants with lab measurements (LOINC codes excluding registration vitals)\n",
    "vitals_exclusion_str = ', '.join(map(str, VITALS_EXCLUSION))\n",
    "labs_query = f\"\"\"\n",
    "SELECT DISTINCT m.person_id\n",
    "FROM `{dataset}`.measurement AS m\n",
    "JOIN `{dataset}`.concept AS c ON m.measurement_concept_id = c.concept_id\n",
    "WHERE c.vocabulary_id = 'LOINC'\n",
    "  AND m.measurement_concept_id NOT IN ({vitals_exclusion_str})\n",
    "\"\"\"\n",
    "labs_persons = polars_gbq(labs_query)\n",
    "print(f\"Participants with lab measurements: {len(labs_persons):,}\")\n",
    "print()\n",
    "\n",
    "# Intersection: Participants with ALL modalities\n",
    "all_modalities = meds_persons.join(icd_persons, on='person_id', how='inner')\n",
    "all_modalities = all_modalities.join(labs_persons, on='person_id', how='inner')\n",
    "\n",
    "n_with_all_ehr = len(all_modalities)\n",
    "def format_count(n):\n",
    "    return '<20' if n < 20 else f\"{n:,}\"\n",
    "\n",
    "print(f\"N = {format_count(n_with_all_ehr)} participants with data from ALL modalities\")\n",
    "print(f\"  (vitals AND medications AND ICD codes AND labs)\")\n",
    "\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c992514-978d-46ee-9339-52d7a0130072",
   "metadata": {},
   "source": [
    "## Step 3: Construct Macrovisits Using N3C Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cfe20b4-44f6-4e1f-9e86-812d25871dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit concept IDs for macrovisit construction\n",
    "# IP, Inpatient Hospital, ER+Inpatient, ICU, Inpatient Critical Care\n",
    "INPATIENT_CONCEPTS = [9201, 8717, 262, 32037, 581379]\n",
    "OUTPATIENT_CONCEPT = 9202\n",
    "ER_CONCEPT = 9203\n",
    "\n",
    "visit_concepts_query = f\"\"\"\n",
    "      SELECT\n",
    "          v.visit_concept_id,\n",
    "          c.concept_name AS visit_type,\n",
    "          COUNT(DISTINCT v.person_id) AS person_count,\n",
    "          COUNT(*) AS visit_count,\n",
    "          CASE\n",
    "              WHEN ca_9201.ancestor_concept_id IS NOT NULL THEN '9201 (IP)'\n",
    "              WHEN ca_9202.ancestor_concept_id IS NOT NULL THEN '9202 (OP)'\n",
    "              WHEN ca_9203.ancestor_concept_id IS NOT NULL THEN '9203 (ER)'\n",
    "              ELSE 'Other'\n",
    "          END AS visit_ancestor\n",
    "      FROM `{dataset}.visit_occurrence` v\n",
    "      JOIN `{dataset}.concept` c\n",
    "          ON v.visit_concept_id = c.concept_id\n",
    "      LEFT JOIN `{dataset}.concept_ancestor` ca_9201\n",
    "          ON v.visit_concept_id = ca_9201.descendant_concept_id\n",
    "          AND ca_9201.ancestor_concept_id IN (9201, 8717, 262, 32037, 581379)\n",
    "      LEFT JOIN `{dataset}.concept_ancestor` ca_9202\n",
    "          ON v.visit_concept_id = ca_9202.descendant_concept_id\n",
    "          AND ca_9202.ancestor_concept_id = 9202\n",
    "      LEFT JOIN `{dataset}.concept_ancestor` ca_9203\n",
    "          ON v.visit_concept_id = ca_9203.descendant_concept_id\n",
    "          AND ca_9203.ancestor_concept_id = 9203\n",
    "      GROUP BY v.visit_concept_id, c.concept_name, visit_ancestor\n",
    "      ORDER BY visit_count DESC\n",
    "      \"\"\"\n",
    "\n",
    "visit_descendents_df = polars_gbq(visit_concepts_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b396f3c7-770b-4a84-b5ec-d5115fcdd16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>visit_concept_id</th><th>visit_type</th><th>person_count</th><th>visit_count</th><th>visit_ancestor</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>9202</td><td>&quot;Outpatient Visit&quot;</td><td>514097</td><td>38997910</td><td>&quot;9202 (OP)&quot;</td></tr><tr><td>581477</td><td>&quot;Office Visit&quot;</td><td>136227</td><td>7550613</td><td>&quot;9202 (OP)&quot;</td></tr><tr><td>32036</td><td>&quot;Laboratory Visit&quot;</td><td>115638</td><td>3900001</td><td>&quot;Other&quot;</td></tr><tr><td>5083</td><td>&quot;Telehealth&quot;</td><td>73101</td><td>2430598</td><td>&quot;Other&quot;</td></tr><tr><td>9201</td><td>&quot;Inpatient Visit&quot;</td><td>135018</td><td>2090555</td><td>&quot;9201 (IP)&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────────────┬──────────────────┬──────────────┬─────────────┬────────────────┐\n",
       "│ visit_concept_id ┆ visit_type       ┆ person_count ┆ visit_count ┆ visit_ancestor │\n",
       "│ ---              ┆ ---              ┆ ---          ┆ ---         ┆ ---            │\n",
       "│ i64              ┆ str              ┆ i64          ┆ i64         ┆ str            │\n",
       "╞══════════════════╪══════════════════╪══════════════╪═════════════╪════════════════╡\n",
       "│ 9202             ┆ Outpatient Visit ┆ 514097       ┆ 38997910    ┆ 9202 (OP)      │\n",
       "│ 581477           ┆ Office Visit     ┆ 136227       ┆ 7550613     ┆ 9202 (OP)      │\n",
       "│ 32036            ┆ Laboratory Visit ┆ 115638       ┆ 3900001     ┆ Other          │\n",
       "│ 5083             ┆ Telehealth       ┆ 73101        ┆ 2430598     ┆ Other          │\n",
       "│ 9201             ┆ Inpatient Visit  ┆ 135018       ┆ 2090555     ┆ 9201 (IP)      │\n",
       "└──────────────────┴──────────────────┴──────────────┴─────────────┴────────────────┘"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visit_descendents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704d51d-922f-4497-8ee3-eab9e0f986d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instead of one massive SQL query, break into smaller steps\n",
    "# Step 1: Get visits for people with all modalities (in batches to avoid query size limits)\n",
    "\n",
    "print(\"Step 1: Loading visit_occurrence records for relevant participants...\")\n",
    "print(f\"  - Total participants to query: {all_modalities['person_id'].n_unique():,}\")\n",
    "\n",
    "# Batch the person_ids to avoid BigQuery query size limit (1MB)\n",
    "# Each person_id is ~8 chars, so 50K IDs = ~450KB per query (safe margin)\n",
    "batch_size = 50000\n",
    "person_ids = all_modalities['person_id'].to_list()\n",
    "\n",
    "# Create batches\n",
    "batches = []\n",
    "for i in range(0, len(person_ids), batch_size):\n",
    "    batch_ids = person_ids[i:i+batch_size]\n",
    "    batches.append(batch_ids)\n",
    "\n",
    "total_batches = len(batches)\n",
    "print(f\"  - Created {total_batches} batches of ~{batch_size:,} IDs each\")\n",
    "\n",
    "# Function to execute a single batch query\n",
    "def fetch_batch(batch_idx, batch_ids):\n",
    "    \"\"\"Fetch visits for a batch of person_ids\"\"\"\n",
    "    person_ids_str = ','.join(map(str, batch_ids))\n",
    "    \n",
    "    batch_query = f\"\"\"\n",
    "    SELECT \n",
    "        person_id,\n",
    "        CAST(visit_occurrence_id AS STRING) as visit_occurrence_id,\n",
    "        visit_concept_id,\n",
    "        visit_start_date,\n",
    "        visit_end_date\n",
    "    FROM `{dataset}`.visit_occurrence\n",
    "    WHERE person_id IN ({person_ids_str})\n",
    "      AND visit_start_date IS NOT NULL \n",
    "      AND visit_end_date IS NOT NULL\n",
    "      AND visit_start_date <= visit_end_date\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_visits = polars_gbq(batch_query)\n",
    "    return batch_idx, batch_visits, len(batch_ids)\n",
    "\n",
    "# Execute batches in parallel using ThreadPoolExecutor\n",
    "# Use max_workers based on available CPUs (limit to reasonable number for BigQuery)\n",
    "max_workers = min(16, os.cpu_count())  # Limit to 16 parallel queries to avoid overwhelming BigQuery\n",
    "print(f\"  - Running {total_batches} batches in parallel with {max_workers} workers...\")\n",
    "\n",
    "all_visits_batches = [None] * total_batches  # Pre-allocate list to maintain order\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all batch queries\n",
    "    future_to_batch = {\n",
    "        executor.submit(fetch_batch, i, batch): i \n",
    "        for i, batch in enumerate(batches)\n",
    "    }\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_batch):\n",
    "        batch_idx, batch_visits, batch_size_actual = future.result()\n",
    "        all_visits_batches[batch_idx] = batch_visits\n",
    "        completed += 1\n",
    "        print(f\"  - Completed {completed}/{total_batches}: Batch {batch_idx+1} - {len(batch_visits):,} visits for {batch_size_actual:,} participants\")\n",
    "\n",
    "# Concatenate all batches\n",
    "all_visits = pl.concat(all_visits_batches)\n",
    "print(f\"\\n✓ Total visits loaded: {len(all_visits):,} for {all_modalities['person_id'].n_unique():,} participants\")\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a171d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get concept_ancestor relationships for visit types\n",
    "print(\"Step 2: Getting visit type hierarchies...\")\n",
    "\n",
    "# Create comma-separated string of inpatient concept IDs\n",
    "inpatient_concepts_str = ', '.join(map(str, INPATIENT_CONCEPTS))\n",
    "\n",
    "visit_ancestor_query = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    ca.descendant_concept_id,\n",
    "    ca.ancestor_concept_id\n",
    "FROM `{dataset}`.concept_ancestor ca\n",
    "WHERE ca.ancestor_concept_id IN ({inpatient_concepts_str}, {OUTPATIENT_CONCEPT}, {ER_CONCEPT})\n",
    "\"\"\"\n",
    "\n",
    "visit_ancestors = polars_gbq(visit_ancestor_query)\n",
    "print(f\"Loaded {len(visit_ancestors):,} concept relationships\")\n",
    "\n",
    "# Create lookup sets for each visit type\n",
    "inpatient_descendants = set(\n",
    "    visit_ancestors.filter(pl.col('ancestor_concept_id').is_in(INPATIENT_CONCEPTS))['descendant_concept_id'].to_list()\n",
    ")\n",
    "outpatient_descendants = set(\n",
    "    visit_ancestors.filter(pl.col('ancestor_concept_id') == OUTPATIENT_CONCEPT)['descendant_concept_id'].to_list()\n",
    ")\n",
    "er_descendants = set(\n",
    "    visit_ancestors.filter(pl.col('ancestor_concept_id') == ER_CONCEPT)['descendant_concept_id'].to_list()\n",
    ")\n",
    "\n",
    "print(f\"  - Inpatient descendants: {len(inpatient_descendants)}\")\n",
    "print(f\"  - Outpatient descendants: {len(outpatient_descendants)}\")\n",
    "print(f\"  - ER descendants: {len(er_descendants)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Identify qualifying microvisits in Polars\n",
    "print(\"Step 3: Filtering to qualifying microvisits...\")\n",
    "\n",
    "all_visits = all_visits.with_columns([\n",
    "    ((pl.col('visit_end_date') - pl.col('visit_start_date')).dt.total_days()).alias('los_days')\n",
    "])\n",
    "\n",
    "inpatient_microvisits = all_visits.filter(\n",
    "    # IP concepts\n",
    "    pl.col('visit_concept_id').is_in(list(inpatient_descendants))\n",
    "    # OR ER visits with LOS >= 1 day\n",
    "    | (pl.col('visit_concept_id').is_in(list(er_descendants)) & (pl.col('los_days') >= 1))\n",
    "    # OR OP visits with LOS = 1 day\n",
    "    | (pl.col('visit_concept_id').is_in(list(outpatient_descendants)) & (pl.col('los_days') == 1))\n",
    ")\n",
    "\n",
    "print(f\"Qualifying microvisits: {len(inpatient_microvisits):,} ({len(inpatient_microvisits)/len(all_visits):.2%})\")\n",
    "print(f\"  - Unique persons: {inpatient_microvisits['person_id'].n_unique():,} ({inpatient_microvisits['person_id'].n_unique()/all_visits['person_id'].n_unique():.2%})\")\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6pwys5at53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: N3C Merging Intervals Algorithm (SQL CTEs adapted in Polars)\n",
    "\n",
    "print(\"Step 4: Running N3C merging intervals algorithm...\")\n",
    "\n",
    "# Step 4a: Add max_end_date and rank\n",
    "merging_a = inpatient_microvisits.sort(['person_id', 'visit_start_date']).with_columns([\n",
    "    pl.col('visit_end_date').cum_max().over('person_id').alias('max_end_date'),\n",
    "    pl.col('visit_start_date').rank().over('person_id').alias('rank_value')\n",
    "])\n",
    "\n",
    "print(f\"\\n  - Added max_end_date and rank:\")\n",
    "display(merging_a.head())\n",
    "\n",
    "# Step 4b: Identify gaps (new macrovisit starts)\n",
    "merging_b = merging_a.with_columns([\n",
    "    pl.when(\n",
    "        pl.col('visit_start_date') <= pl.col('max_end_date').shift(1).over('person_id')\n",
    "    ).then(0).otherwise(1).alias('gap')\n",
    "])\n",
    "\n",
    "print(f\"\\n  - Identified gaps between visits:\")\n",
    "display(merging_b.head())\n",
    "\n",
    "# Step 4c: Create group numbers (cumulative sum of gaps)\n",
    "merging_c = merging_b.with_columns([\n",
    "    pl.col('gap').cum_sum().over('person_id').alias('group_number')\n",
    "])\n",
    "\n",
    "print(f\"\\n  - Assigned group numbers:\")\n",
    "display(merging_c.head())\n",
    "\n",
    "# Step 4d: Create macrovisit_id and date ranges\n",
    "macrovisits = merging_c.group_by(['person_id', 'group_number']).agg([\n",
    "    pl.col('visit_start_date').min().alias('macrovisit_start_date'),\n",
    "    pl.col('visit_end_date').max().alias('macrovisit_end_date')\n",
    "]).with_columns([\n",
    "    # Create macrovisit_id matching N3C format\n",
    "    (\n",
    "        pl.col('person_id').cast(pl.Utf8) + '_' +\n",
    "        pl.col('group_number').cast(pl.Utf8) + '_' +\n",
    "        (pl.col('macrovisit_start_date').hash() % 10).abs().cast(pl.Utf8)\n",
    "    ).alias('macrovisit_id')\n",
    "])\n",
    "\n",
    "print(f\"\\n  - Created {len(macrovisits):,} macrovisits from {len(inpatient_microvisits):,} microvisits ({len(macrovisits)/len(inpatient_microvisits):.2%})\")\n",
    "print(f\"  - Unique persons with macrovisits: {macrovisits['person_id'].n_unique():,}\\n\")\n",
    "display(macrovisits.filter(pl.col('person_id')==1000039).head())\n",
    "\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ghwelzqdckj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Map all visits to macrovisits\n",
    "print(\"Step 5: Mapping all visits to macrovisits...\")\n",
    "\n",
    "# Join all visits to macrovisits where visit_start_date falls within macrovisit window\n",
    "microvisits_to_macrovisits = all_visits.join(\n",
    "    macrovisits,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ").filter(\n",
    "    # Keep visits that fall within a macrovisit OR have no macrovisit\n",
    "    pl.col('macrovisit_id').is_null() |\n",
    "    (\n",
    "        (pl.col('visit_start_date') >= pl.col('macrovisit_start_date')) &\n",
    "        (pl.col('visit_start_date') <= pl.col('macrovisit_end_date'))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Mapped {len(microvisits_to_macrovisits):,} visits\")\n",
    "print(f\"  - Visits with macrovisit_id: {microvisits_to_macrovisits.filter(pl.col('macrovisit_id').is_not_null()).height:,} ({microvisits_to_macrovisits.filter(pl.col('macrovisit_id').is_not_null()).height/len(microvisits_to_macrovisits):.2%})\")\n",
    "display(microvisits_to_macrovisits.filter(pl.col('macrovisit_id').is_not_null()).head())\n",
    "\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5vcsxy11kwx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Filter to macrovisits with IP/ER anchor\n",
    "print(\"Step 6: Filtering to macrovisits with IP or ER anchor...\")\n",
    "\n",
    "# Identify macrovisits that have at least one IP or ER visit\n",
    "ip_er_macrovisits = microvisits_to_macrovisits.filter(\n",
    "    pl.col('macrovisit_id').is_not_null() &\n",
    "    (\n",
    "        pl.col('visit_concept_id').is_in(list(inpatient_descendants)) |\n",
    "        pl.col('visit_concept_id').is_in(list(er_descendants))\n",
    "    )\n",
    ").select('macrovisit_id').unique()\n",
    "\n",
    "print(f\"Macrovisits with IP/ER anchor: {len(ip_er_macrovisits):,}\")\n",
    "\n",
    "# Keep only visits that belong to these macrovisits (or have no macrovisit)\n",
    "microvisits_to_macrovisits = microvisits_to_macrovisits.filter(\n",
    "    pl.col('macrovisit_id').is_null() |\n",
    "    pl.col('macrovisit_id').is_in(ip_er_macrovisits['macrovisit_id'].implode())\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal result:\")\n",
    "print(f\"  - Total visits (macrovisits): {len(microvisits_to_macrovisits):,}\")\n",
    "print(f\"  - Visits with macrovisit_id: {microvisits_to_macrovisits.filter(pl.col('macrovisit_id').is_not_null()).height:,}\")\n",
    "print(f\"  - Unique macrovisits: {microvisits_to_macrovisits['macrovisit_id'].n_unique():,}\")\n",
    "print(f\"  - Unique persons with macrovisits: {microvisits_to_macrovisits.filter(pl.col('macrovisit_id').is_not_null())['person_id'].n_unique():,}\")\n",
    "\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb10815-d165-43f1-942a-55e361a197bd",
   "metadata": {},
   "source": [
    "## Step 4: Extract Macrovisits Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a833222-c29c-47f6-adcc-dbc5e1c3c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create macrovisits summary from microvisits mapping\n",
    "ip_er_macrovisits_df = microvisits_to_macrovisits.filter(\n",
    "    pl.col('macrovisit_id').is_not_null()\n",
    ").group_by(['person_id', 'macrovisit_id', 'macrovisit_start_date', 'macrovisit_end_date']).agg([\n",
    "    pl.len().alias('n_microvisits')\n",
    "])\n",
    "\n",
    "print(f\"Macrovisits summary:\")\n",
    "print(f\"  - Total macrovisits: {len(ip_er_macrovisits_df):,}\")\n",
    "print(f\"  - Median microvisits per macrovisit: {ip_er_macrovisits_df['n_microvisits'].median():.0f}\")\n",
    "\n",
    "display(ip_er_macrovisits_df.sort('person_id', 'macrovisit_start_date').head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_er_macrovisits_df.write_csv(f'{bucket}/cohorts/ip_er_macrovisits.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9201b14-3fc8-4e2d-a808-7a1c8d712b6c",
   "metadata": {},
   "source": [
    "## Step 5: Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f102e3a-1e2b-4bfc-9c60-09d71ce6433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running validation checks...\\n\")\n",
    "\n",
    "# Check 1: No overlapping macrovisits\n",
    "overlapping = ip_er_macrovisits_df.join(\n",
    "    ip_er_macrovisits_df,\n",
    "    on='person_id',\n",
    "    suffix='_other'\n",
    ").filter(\n",
    "    (pl.col('macrovisit_id') != pl.col('macrovisit_id_other')) &\n",
    "    (pl.col('macrovisit_start_date') <= pl.col('macrovisit_end_date_other')) &\n",
    "    (pl.col('macrovisit_start_date_other') <= pl.col('macrovisit_end_date'))\n",
    ")\n",
    "\n",
    "print(f\"Check 1 - Overlapping macrovisits: {len(overlapping)} (expected: 0)\")\n",
    "if len(overlapping) > 0:\n",
    "    print(\"WARNING: Found overlapping macrovisits!\")\n",
    "    print(overlapping.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962061e-42d0-4dc2-aaef-fcbb5d68d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2: Each microvisit maps to ≤1 macrovisit\n",
    "multi_mapped = microvisits_to_macrovisits.filter(\n",
    "    pl.col('macrovisit_id').is_not_null()\n",
    ").group_by('visit_occurrence_id').agg([\n",
    "    pl.col('macrovisit_id').n_unique().alias('n_macrovisits')\n",
    "]).filter(pl.col('n_macrovisits') > 1)\n",
    "\n",
    "print(f\"Check 2 - Microvisits mapped to >1 macrovisit: {len(multi_mapped)} (expected: 0)\")\n",
    "if len(multi_mapped) > 0:\n",
    "    print(\"WARNING: Found microvisits mapped to multiple macrovisits!\")\n",
    "    print(multi_mapped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481622f-61a8-4963-a861-5c883008c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 3: Microvisit start dates within macrovisit boundaries\n",
    "out_of_bounds = microvisits_to_macrovisits.filter(\n",
    "    pl.col('macrovisit_id').is_not_null()\n",
    ").filter(\n",
    "    (pl.col('visit_start_date') < pl.col('macrovisit_start_date')) |\n",
    "    (pl.col('visit_start_date') > pl.col('macrovisit_end_date'))\n",
    ")\n",
    "\n",
    "print(f\"Check 3 - Microvisits outside macrovisit boundaries: {len(out_of_bounds)} (expected: 0)\")\n",
    "if len(out_of_bounds) > 0:\n",
    "    print(\"WARNING: Found microvisits outside macrovisit date boundaries!\")\n",
    "    print(out_of_bounds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478c7d4-e2fb-446d-be3f-966adf9e850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 4: Confirm all macrovisits have IP/ER anchor (should be guaranteed by filtering)\n",
    "# Use descendant concept IDs, not just core concepts\n",
    "ip_er_descendants_combined = inpatient_descendants.union(er_descendants)\n",
    "\n",
    "missing_anchor = microvisits_to_macrovisits.filter(\n",
    "    pl.col('macrovisit_id').is_not_null()\n",
    ").group_by('macrovisit_id').agg([\n",
    "    pl.col('visit_concept_id').is_in(list(ip_er_descendants_combined)).sum().alias('n_ip_er')\n",
    "]).filter(pl.col('n_ip_er') == 0)\n",
    "\n",
    "print(f\"Check 4 - Macrovisits without IP/ER anchor: {len(missing_anchor)} (expected: 0)\")\n",
    "if len(missing_anchor) > 0:\n",
    "    print(\"ERROR: This should not happen - filtering logic may be broken!\")\n",
    "    print(missing_anchor.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d8969f-a421-4df3-b3a6-7da94576c63d",
   "metadata": {},
   "source": [
    "## Step 6: Generate Validation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf946c-1581-40e2-89de-614bbae59601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate LOS in Polars\n",
    "macrovisits_with_los = ip_er_macrovisits_df.with_columns([\n",
    "    (pl.col('macrovisit_end_date') - pl.col('macrovisit_start_date')).dt.total_days().alias('los_days')\n",
    "])\n",
    "\n",
    "los_summary = macrovisits_with_los.group_by('los_days').agg([\n",
    "    pl.len().alias('n')\n",
    "]).sort('los_days')\n",
    "\n",
    "# Apply <20 suppression for display\n",
    "los_plot_data = los_summary.filter(pl.col('los_days') <= 100)  # Focus on reasonable LOS\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# Filter to counts >=20 for plotting\n",
    "plottable = los_plot_data.filter(pl.col('n') >= 20)\n",
    "ax.bar(plottable['los_days'], plottable['n'])\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Length of Stay (days)')\n",
    "ax.set_ylabel('Number of Macrovisits')\n",
    "ax.set_title('Macrovisit Length of Stay Distribution (counts <20 suppressed)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total macrovisits: {len(ip_er_macrovisits_df):,}\")\n",
    "print(f\"Median LOS: {macrovisits_with_los['los_days'].median():.1f} days\")\n",
    "print(f\"Mean LOS: {macrovisits_with_los['los_days'].mean():.1f} days\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6120a7e-740a-46ff-a4fb-79d9796231a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microvisit count per macrovisit\n",
    "microvisit_counts = ip_er_macrovisits_df.select(['macrovisit_id', 'n_microvisits'])\n",
    "\n",
    "count_dist = microvisit_counts.group_by('n_microvisits').agg([\n",
    "    pl.len().alias('n_macrovisits')\n",
    "]).sort('n_microvisits')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Only plot non-suppressed counts (>=20)\n",
    "plottable = count_dist.filter(pl.col('n_macrovisits') >= 20)\n",
    "if len(plottable) > 0:\n",
    "    ax.bar(plottable['n_microvisits'], plottable['n_macrovisits'])\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Number of Microvisits per Macrovisit')\n",
    "    ax.set_ylabel('Number of Macrovisits')\n",
    "    ax.set_title('Microvisits per Macrovisit Distribution (counts <20 suppressed)')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'All counts <20 (suppressed per AoU policy)', \n",
    "            ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title('Microvisits per Macrovisit Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Median microvisits per macrovisit: {microvisit_counts['n_microvisits'].median()}\")\n",
    "print(f\"Max microvisits in a single macrovisit: {microvisit_counts['n_microvisits'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb5900-790e-4e73-bdb7-ce3f9df1acd0",
   "metadata": {},
   "source": [
    "## Step 7: Extract and Unify Temperature Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924d48c-d7c0-411f-b6b6-3c98fea29e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables for Unifier\n",
    "cohort = \"allofus\"\n",
    "version = dataset.split('.')[1]\n",
    "file_type = \"parquet\" # csv also possible\n",
    "file_path = f\"{bucket}/data/{cohort}/{version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717ad8f-f385-4609-8e4b-81b8846f5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the explorer object \n",
    "explorer = Explorer(variable_type=\"measurements\", cohort=cohort, version=version)\n",
    "\n",
    "# Display cohorts\n",
    "explorer.cohorts()\n",
    "\n",
    "# Display annotated variables\n",
    "metadata = explorer.variables()\n",
    "print(f\"Loaded {len(metadata)} annotated measurement variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1b0ed-27af-4ab4-8e47-eea2f7b8a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for temperature concepts in LOINC\n",
    "temp_concept_query = f\"\"\"\n",
    "WITH filtered_concepts AS (\n",
    "    SELECT DISTINCT concept_id, concept_code, concept_name\n",
    "    FROM {dataset}.concept c\n",
    "    WHERE vocabulary_id = 'LOINC'\n",
    "        AND LOWER(concept_name) LIKE '%temperature%'\n",
    "        AND LOWER(concept_name) NOT LIKE '%environmental%'\n",
    ")\n",
    "SELECT concept_name, concept_id, COUNT(measurement_id) AS count\n",
    "FROM {dataset}.measurement m\n",
    "LEFT JOIN filtered_concepts fc ON fc.concept_id = m.measurement_concept_id\n",
    "WHERE measurement_concept_id IN (SELECT concept_id FROM filtered_concepts) \n",
    "GROUP BY concept_name, concept_id\n",
    "ORDER BY count DESC\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "polars_gbq(temp_concept_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f185df8-9231-4754-996f-58a11423d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_cid = 3020891                    # OMOP measurement_concept_id \n",
    "\n",
    "test_concept_query = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    c.concept_name as lab_concept_name,\n",
    "    c.concept_id as lab_concept_id, \n",
    "    c.concept_code as lab_concept_code,\n",
    "    c.vocabulary_id as lab_vocab,\n",
    "    c1.concept_name as standard_unit,\n",
    "    count(measurement_id) AS count\n",
    "FROM {dataset}.measurement m\n",
    "JOIN {dataset}.concept c ON m.measurement_concept_id = c.concept_id\n",
    "LEFT JOIN {dataset}.concept c1 ON m.unit_concept_id = c1.concept_id\n",
    "WHERE c.concept_id = {measurement_cid}\n",
    "GROUP BY lab_concept_name, lab_concept_id, lab_concept_code, lab_vocab, standard_unit\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "polars_gbq(test_concept_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_concepts = {\n",
    "    \"Body temperature\":\t[3020891, \"degree Celsius\"],\n",
    "    \"Oral temperature\":\t[3006322, \"degree Fahrenheit\"],\n",
    "    \"Body temperature|Temperature|Moment in time|Without specimen\":\t[40654162, None],\n",
    "    \"Body temperature - Temporal artery\": [46235152, \"degree Fahrenheit\"],\n",
    "    # \"Skin temperature --in microenvironment\": [21490591, \"no value\"],  --- all missing\n",
    "    \"Tympanic membrane temperature\": [3025163, \"degree Fahrenheit\"],\n",
    "    # \"Blood temperature\": [21490586, None],  --- integers\n",
    "    \"Axillary temperature\":\t[3025085, \"degree Fahrenheit\"],\n",
    "    \"Body temperature - Core\": [3025926, \"degree Fahrenheit\"],\n",
    "    \"Temperature of Skin\": [3039856, \"degree Fahrenheit\"],\n",
    "    \"Rectal temperature\": [3022060, \"degree Fahrenheit\"],\n",
    "    \"Esophageal temperature\": [21490588, \"degree Fahrenheit\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e5c88-6213-487b-9842-5fb5aca119bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mapper for temperature measurements\n",
    "# Based on the unit distribution above, set the standard unit\n",
    "# ============================= CYCLE STARTING HERE\n",
    "measurement_name, (measurement_cid, standard_unit) = \\\n",
    "    list(temperature_concepts.items())[0]\n",
    "\n",
    "min_value = 30                            # Minimum physiologic value\n",
    "max_value = 45                           # Maximum physiologic value\n",
    "group = \"vitals\"                          # Grouping of the measurement\n",
    "\n",
    "units = Mapper(\n",
    "    measurement_cid=measurement_cid, \n",
    "    measurement_name=measurement_name,\n",
    "    standard_unit=standard_unit,\n",
    "    min_value=min_value,\n",
    "    max_value=max_value,\n",
    "    group=group,\n",
    "    cohort=cohort,\n",
    "    version=version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dfd6b-5c8e-42dd-872a-06bee4259f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metadata\n",
    "print(\"Mapper initialized with the following metadata:\")\n",
    "print(units.metadata)\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c467c18-ca2b-499e-93ee-ba03ea571da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the unit mapping process\n",
    "units.init_map()\n",
    "\n",
    "# Display the predominant unit histogram\n",
    "units.predominant_unit_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008324e-3f89-4a3c-a77f-32fb4d71466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_unit = units.previous_unit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98065740-e70d-40f9-b6d9-f3c7b011fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_unit = units.next_unit()    # .previous_unit() if user wants to return to the last unit\n",
    "units.unit_histogram()              # Display the histogram for the current unit_concept_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611064e-2b4a-473e-9db4-717edbe96553",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_factor = 1      # conversion factor to convert to the standard unit\n",
    "multimodal = 0             # 1 if distribution is multimodal or 0 if not\n",
    "\n",
    "units.unit_update(current_unit, \n",
    "                  conversion_factor = conversion_factor, \n",
    "                  multimodal = multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f0b1b-e5ba-40d1-8937-537b38f8e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_factor = 0\n",
    "multimodal = 0\n",
    "\n",
    "units.unit_update(current_unit, \n",
    "                  conversion_factor = conversion_factor, \n",
    "                  multimodal = multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16247f16-555a-4a09-ae3d-90e0545b0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "units.save()\n",
    "# ============================= CYCLE STARTING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fd9e5-625d-48b0-868d-7f12b0d65bd3",
   "metadata": {},
   "source": [
    "## Step 8: Clean Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = Explorer(variable_type=\"measurements\", cohort=cohort, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=explorer.variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573bb522",
   "metadata": {},
   "outputs": [],
   "source": [
    "for measurement_cid in metadata.filter(pl.col('group')==\"vitals\")['measurement_concept_id']:\n",
    "    measurement = Unifier(measurement_cid = measurement_cid,\n",
    "                          save_dir=file_path,\n",
    "                          cohort=cohort,\n",
    "                          version=version,\n",
    "                          file_type=file_type,\n",
    "                          save_annotation=True,\n",
    "                          drop_sites=True)\n",
    "    measurement.unify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dfa284-22ab-4d16-b415-2deb1c93c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unified temperature data from Unifier output\n",
    "# The Unifier has already handled unit conversion to standard unit (Celsius)\n",
    "\n",
    "# Create Unifier instance to load the processed data\n",
    "unifier = Unifier(\n",
    "    measurement_cid=measurement_cid,\n",
    "    measurement_name=measurement_name,\n",
    "    cohort=cohort,\n",
    "    version=version\n",
    ")\n",
    "\n",
    "# Load the unified temperature data\n",
    "temp_df_unified = unifier.load_data()\n",
    "\n",
    "print(f\"Loaded unified temperature data: {len(temp_df_unified):,} measurements\")\n",
    "print(f\"  - Unique persons: {temp_df_unified['person_id'].n_unique():,}\")\n",
    "\n",
    "# The Unifier should have already:\n",
    "# 1. Converted all units to the standard unit (Celsius)\n",
    "# 2. Applied min/max physiological value filtering (32-42°C)\n",
    "# 3. Removed multimodal/erroneous measurements\n",
    "\n",
    "# Check the distribution\n",
    "print(\"\\nTemperature distribution (unified, Celsius):\")\n",
    "print(f\"  Min: {temp_df_unified['value_as_number'].min():.1f}°C\")\n",
    "print(f\"  1st percentile: {temp_df_unified['value_as_number'].quantile(0.01):.1f}°C\")\n",
    "print(f\"  Median: {temp_df_unified['value_as_number'].median():.1f}°C\")\n",
    "print(f\"  99th percentile: {temp_df_unified['value_as_number'].quantile(0.99):.1f}°C\")\n",
    "print(f\"  Max: {temp_df_unified['value_as_number'].max():.1f}°C\")\n",
    "\n",
    "# Rename column for clarity\n",
    "temp_df_clean = temp_df_unified.rename({'value_as_number': 'temp_celsius'})\n",
    "\n",
    "print(f\"\\n✓ Temperature data unified and ready for analysis\")\n",
    "print(f\"  Final N: {len(temp_df_clean):,} measurements\")\n",
    "print(f\"  Unique persons: {temp_df_clean['person_id'].n_unique():,}\")\n",
    "\n",
    "print_resource_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f64be3-5117-42e5-938f-8a18423eefef",
   "metadata": {},
   "source": [
    "## Step 9: Identify Fever Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89fd5b-8555-4517-bd0b-0ffb5b655284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique macrovisits with start dates\n",
    "inpatient_macrovisits = macrovisits_df.select([\n",
    "    'person_id',\n",
    "    'macrovisit_id',\n",
    "    'macrovisit_start_date',\n",
    "    'macrovisit_end_date'\n",
    "])\n",
    "\n",
    "# Convert dates to datetime for comparison\n",
    "# Add 24 hours to start date to define window\n",
    "inpatient_macrovisits = inpatient_macrovisits.with_columns([\n",
    "    pl.col('macrovisit_start_date').cast(pl.Datetime).alias('start_datetime'),\n",
    "    (pl.col('macrovisit_start_date').cast(pl.Datetime) + pl.duration(hours=24)).alias('end_24hr')\n",
    "])\n",
    "\n",
    "# Join temperatures to macrovisits\n",
    "# Filter to first 24 hours only\n",
    "first_24hr_temps = temp_df_clean.join(\n",
    "    inpatient_macrovisits,\n",
    "    on='person_id',\n",
    "    how='inner'\n",
    ").filter(\n",
    "    (pl.col('measurement_datetime') >= pl.col('start_datetime')) &\n",
    "    (pl.col('measurement_datetime') < pl.col('end_24hr'))\n",
    ")\n",
    "\n",
    "print(f\"Temperature measurements in first 24hr of admissions: {len(first_24hr_temps):,}\")\n",
    "\n",
    "# Calculate fever episodes\n",
    "# Fever definition: ≥2 measurements >38°C\n",
    "fever_episodes = first_24hr_temps.with_columns([\n",
    "    (pl.col('temp_celsius') > 38.0).cast(pl.Int32).alias('is_fever')\n",
    "]).group_by(['person_id', 'macrovisit_id', 'macrovisit_start_date']).agg([\n",
    "    pl.col('is_fever').sum().alias('fever_measurements'),\n",
    "    pl.col('temp_celsius').max().alias('peak_temp'),\n",
    "    pl.col('measurement_datetime').min().alias('first_temp_time'),\n",
    "    pl.col('measurement_datetime').max().alias('last_temp_time'),\n",
    "    pl.len().alias('total_measurements')\n",
    "]).filter(\n",
    "    pl.col('fever_measurements') >= 2  # At least 2 measurements >38°C\n",
    ")\n",
    "\n",
    "print(f\"\\nFever episodes identified: {len(fever_episodes):,}\")\n",
    "print(f\"  - Unique persons with fever: {fever_episodes['person_id'].n_unique():,}\")\n",
    "print(f\"\\n✓ Fever cohort construction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd3ef4-495f-4cc8-b27c-3ff08b4b45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 2: Post-Fever Data or Death Requirement\n",
    "# Ensure patients either have data after fever OR have a recorded death\n",
    "# This avoids including patients who simply drop out of the healthcare system\n",
    "\n",
    "# Get death dates from death table\n",
    "death_query = f\"\"\"\n",
    "SELECT person_id, death_date\n",
    "FROM `{dataset}`.death\n",
    "\"\"\"\n",
    "death_df = polars_gbq(death_query)\n",
    "\n",
    "print(f\"Loaded {len(death_df):,} death records\")\n",
    "\n",
    "# Join with fever episodes to check for death\n",
    "fever_episodes_with_death = fever_episodes_filtered.join(\n",
    "    death_df,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Get all visit dates for patients in fever cohort (to check for post-fever data)\n",
    "post_fever_visits_query = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    v.person_id,\n",
    "    v.visit_start_date\n",
    "FROM `{dataset}`.visit_occurrence v\n",
    "WHERE v.person_id IN ({','.join(map(str, fever_episodes_filtered['person_id'].unique().to_list()))})\n",
    "\"\"\"\n",
    "\n",
    "# Note: If person_id list is very large (>10K), may need to use temp table approach\n",
    "print(\"Checking for post-fever visits...\")\n",
    "post_fever_visits_df = polars_gbq(post_fever_visits_query)\n",
    "\n",
    "# Join to identify visits that occur after the fever episode\n",
    "fever_with_post_visits = fever_episodes_with_death.join(\n",
    "    post_fever_visits_df,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ").filter(\n",
    "    # Post-fever visit = visit after the macrovisit end date\n",
    "    pl.col('visit_start_date') > pl.col('macrovisit_end_date')\n",
    ").select(['person_id', 'macrovisit_id']).unique()\n",
    "\n",
    "print(f\"Patients with post-fever visits: {fever_with_post_visits['person_id'].n_unique():,}\")\n",
    "\n",
    "# Create indicator for \"has post-fever data or death\"\n",
    "# Use .implode() when checking Polars column membership\n",
    "fever_episodes_final = fever_episodes_with_death.with_columns([\n",
    "    # Has post-fever visit\n",
    "    pl.col('macrovisit_id').is_in(fever_with_post_visits['macrovisit_id'].implode()).alias('has_post_fever_visit'),\n",
    "    # Has death record\n",
    "    pl.col('death_date').is_not_null().alias('has_death_record')\n",
    "]).with_columns([\n",
    "    # Pass filter if either condition is true\n",
    "    (pl.col('has_post_fever_visit') | pl.col('has_death_record')).alias('has_followup_data')\n",
    "])\n",
    "\n",
    "n_before_data_filter = len(fever_episodes_final)\n",
    "n_with_followup = fever_episodes_final.filter(pl.col('has_followup_data')).height\n",
    "n_without_followup = n_before_data_filter - n_with_followup\n",
    "\n",
    "print(f\"\\nFilter 2 - Post-Fever Data or Death Requirement:\")\n",
    "print(f\"  - Episodes before filter: {n_before_data_filter:,}\")\n",
    "print(f\"  - Episodes with post-fever data or death: {n_with_followup:,}\")\n",
    "print(f\"  - Excluded (no follow-up data): {n_without_followup:,} ({100*n_without_followup/n_before_data_filter:.1f}%)\")\n",
    "print(f\"  - Reason: No healthcare visits after fever AND no death record\")\n",
    "print()\n",
    "\n",
    "# Apply the filter\n",
    "fever_episodes_final = fever_episodes_final.filter(pl.col('has_followup_data'))\n",
    "\n",
    "print(f\"✓ Final fever cohort after bias protection filters: {len(fever_episodes_final):,} episodes\")\n",
    "print(f\"  - Unique persons: {fever_episodes_final['person_id'].n_unique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831df6b-a076-4eed-8817-ca3d16ebe96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 1: Longitudinal Data Requirement\n",
    "# Remove admissions that start < 45 days before end of study (2023-10-01)\n",
    "# This ensures sufficient potential follow-up time to observe outcomes\n",
    "\n",
    "study_end_date = pl.date(2023, 10, 1)\n",
    "\n",
    "fever_episodes_with_followup = fever_episodes.with_columns([\n",
    "    (study_end_date - pl.col('macrovisit_start_date')).dt.total_days().alias('days_potential_followup')\n",
    "])\n",
    "\n",
    "n_before_filter = len(fever_episodes_with_followup)\n",
    "\n",
    "fever_episodes_filtered = fever_episodes_with_followup.filter(\n",
    "    pl.col('days_potential_followup') >= 45\n",
    ")\n",
    "\n",
    "n_after_followup_filter = len(fever_episodes_filtered)\n",
    "n_excluded_followup = n_before_filter - n_after_followup_filter\n",
    "\n",
    "print(f\"Filter 1 - Longitudinal Data Requirement:\")\n",
    "print(f\"  - Episodes before filter: {n_before_filter:,}\")\n",
    "print(f\"  - Episodes after filter (≥45 days potential follow-up): {n_after_followup_filter:,}\")\n",
    "print(f\"  - Excluded: {n_excluded_followup:,} ({100*n_excluded_followup/n_before_filter:.1f}%)\")\n",
    "print(f\"  - Reason: Admission within 45 days of study end date (insufficient follow-up time)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9fd61-8b8b-48d2-b2af-6386198397e9",
   "metadata": {},
   "source": [
    "## Step 9b: Apply Bias Protection Filters\n",
    "\n",
    "To protect against various biases (immortal time bias, surveillance bias), we apply additional filters based on N3C RECOVER methodology:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c02ad-5793-412b-8648-efe59054f9df",
   "metadata": {},
   "source": [
    "## Step 10: Generate Final Cohort Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eac0e7-d72d-4f3d-9a64-f8aadae3efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique participants and admissions (using filtered cohort)\n",
    "n_participants = fever_episodes_final['person_id'].n_unique()\n",
    "n_admissions = len(fever_episodes_final)\n",
    "\n",
    "# Apply <20 suppression for display\n",
    "def format_count(n):\n",
    "    return '<20' if n < 20 else f\"{n:,}\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL FEVER COHORT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"N = {format_count(n_participants)} participants with fever in first 24 hours of admission\")\n",
    "print(f\"N = {format_count(n_admissions)} inpatient admissions with fever\")\n",
    "print()\n",
    "print(\"Inclusion criteria:\")\n",
    "print(\"  ✓ ≥2 temperature measurements >38°C in first 24 hours\")\n",
    "print(\"  ✓ ≥45 days potential follow-up before study end (2023-10-01)\")\n",
    "print(\"  ✓ Post-fever healthcare data OR death record\")\n",
    "print()\n",
    "\n",
    "# Temperature statistics\n",
    "print(\"Temperature Statistics:\")\n",
    "print(f\"  - Median peak temperature: {fever_episodes_final['peak_temp'].median():.1f}°C\")\n",
    "print(f\"  - Mean peak temperature: {fever_episodes_final['peak_temp'].mean():.1f}°C\")\n",
    "print(f\"  - Max peak temperature: {fever_episodes_final['peak_temp'].max():.1f}°C\")\n",
    "print(f\"  - Median fever measurements per admission: {fever_episodes_final['fever_measurements'].median():.0f}\")\n",
    "print()\n",
    "\n",
    "# Distribution of fever measurements per admission\n",
    "fever_count_dist = fever_episodes_final.group_by('fever_measurements').agg([\n",
    "    pl.len().alias('n_admissions')\n",
    "]).sort('fever_measurements')\n",
    "\n",
    "print(\"Distribution of fever measurements (>38°C) per admission:\")\n",
    "for row in fever_count_dist.iter_rows(named=True):\n",
    "    count = row['n_admissions']\n",
    "    count_display = '<20' if count < 20 else f\"{count:,}\"\n",
    "    print(f\"  - {row['fever_measurements']} fever measurements: {count_display} admissions\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Follow-up data breakdown\n",
    "n_with_post_visit = fever_episodes_final.filter(pl.col('has_post_fever_visit')).height\n",
    "n_with_death_only = fever_episodes_final.filter(\n",
    "    ~pl.col('has_post_fever_visit') & pl.col('has_death_record')\n",
    ").height\n",
    "\n",
    "print(\"Follow-up Data Breakdown:\")\n",
    "print(f\"  - Admissions with post-fever visits: {format_count(n_with_post_visit)}\")\n",
    "print(f\"  - Admissions with death (no post-fever visit): {format_count(n_with_death_only)}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(f\"Ready for downstream analysis!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (omop)",
   "language": "python",
   "name": "omop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
