{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#README\" data-toc-modified-id=\"README-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>README</a></span></li></ul></li><li><span><a href=\"#Initial-setup\" data-toc-modified-id=\"Initial-setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initial setup</a></span></li><li><span><a href=\"#Function\" data-toc-modified-id=\"Function-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Function</a></span></li><li><span><a href=\"#Gather-Variant-Counts\" data-toc-modified-id=\"Gather-Variant-Counts-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Gather Variant Counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-Files-for-Analysis\" data-toc-modified-id=\"Read-in-Files-for-Analysis-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Read in Files for Analysis</a></span></li><li><span><a href=\"#Define-hyperparameters\" data-toc-modified-id=\"Define-hyperparameters-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Define hyperparameters</a></span></li><li><span><a href=\"#Run\" data-toc-modified-id=\"Run-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Run</a></span></li><li><span><a href=\"#Summary-Statistics\" data-toc-modified-id=\"Summary-Statistics-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Summary Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Save</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README ##\n",
    "\n",
    "This notebook creates a cohort using hail starting from a variant list.\n",
    "\n",
    "__For VM configurations__, __dataproc VM__ with __main instance__ having __64 CPUs, 240 GB RAM__, and __10 workers (2 standard, 8 preemptible)__ having __4 CPUs, 15GB RAM (5.76 USD/hour)__ is suggested. Storage just needs to be enough to meet requirement, e.g., ~ 150GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Will need for the initial installation of polars\n",
    "## You will need to restart the kernel after installing\n",
    "# !pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import subprocess\n",
    "import hail as hl\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "print(bucket)\n",
    "CDR = os.getenv('WORKSPACE_CDR')\n",
    "print(CDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_fmt_str_lengths(128)\n",
    "\n",
    "# Set the row limit to a higher value\n",
    "pl.Config.set_tbl_rows(50)\n",
    "\n",
    "# show all columns in pandas\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# show full column width\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    \"\"\"\n",
    "    Initialize Hail\n",
    "    :param query: BigQuery SQL query\n",
    "    :return: polars dataframe\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    start\n",
    "\n",
    "    hl.init(idempotent = True)\n",
    "    hl.default_reference('GRCh38')\n",
    "    logging.getLogger('hail').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_exome(mt, interval):\n",
    "    \"\"\"\n",
    "    Filter exome matrix table to gene interval of interest\n",
    "    :param mt: Hail matrix table\n",
    "    :param interval: gene interval\n",
    "    :return: Hail matrix table of filtered exome, with n_alt column\n",
    "    \"\"\"\n",
    "    \n",
    "    mt = hl.filter_intervals(mt, [hl.parse_locus_interval(x) for x in interval])\n",
    "    mt = mt.annotate_entries(n_alt = mt.GT.n_alt_alleles())\n",
    "    mt = mt.drop(mt.filters, mt.variant_qc, mt.info)\n",
    "\n",
    "    return mt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_exome(vat_ht, interval, variant_ids, filtered_exome, maf_threshold=0.001):\n",
    "    \"\"\"\n",
    "    :param Annotate exome, including specified variants and pLOF variants\n",
    "    :param vat_ht: Hail variant annotation table\n",
    "    :param interval: interval for filtering (e.g., gene of interest)\n",
    "    :param variant_ids: variant IDs of interest\n",
    "    :param filtered_exome: Hail filtered exome\n",
    "    :param maf_threshold: threshold for minor allele frequency in gnomad (all populations) \n",
    "    \"\"\"\n",
    "\n",
    "    # Essential columns only - reduce memory footprint\n",
    "    essential_cols = ['vid', 'gene_symbol', 'gnomad_all_af', 'aa_change', \n",
    "                      'consequence', 'dna_change_in_transcript', 'genomic_location',\n",
    "                      'dbsnp_rsid', 'is_canonical_transcript']\n",
    "    \n",
    "    # Parse interval for validation\n",
    "    parsed_intervals = [hl.parse_locus_interval(x) for x in interval]\n",
    "    \n",
    "    # Filter VAT to gene interval and canonical transcripts\n",
    "    vat_ht = hl.filter_intervals(vat_ht, parsed_intervals)\n",
    "    vat_ht = vat_ht.filter(vat_ht.is_canonical_transcript)\n",
    "    vat_ht = vat_ht.select(*essential_cols)\n",
    "    \n",
    "    # Define pLOF consequences\n",
    "    lof_consequences = hl.set(['start_lost', 'stop_lost', 'stop_gained', 'frameshift_variant'])\n",
    "    \n",
    "    # Validate that specified variants are in the interval\n",
    "    if variant_ids:\n",
    "        # Create a table from variant vids to check interval membership\n",
    "        variant_set_vid = set(variant_ids)\n",
    "        \n",
    "        # Parse vids to locus format for interval checking\n",
    "        # Assuming vid format is like \"3-98532100-C-T\"\n",
    "        def parse_vid_to_locus(vid):\n",
    "            chrom, pos, *_ = vid.split('-')\n",
    "            return f\"chr{chrom}:{pos}\"\n",
    "        \n",
    "        variant_set_locus = {parse_vid_to_locus(v) for v in variant_ids}\n",
    "        \n",
    "        # Get loci actually in the interval from vat_ht\n",
    "        vat_loci_in_interval = vat_ht.aggregate(\n",
    "            hl.agg.collect_as_set(hl.str(vat_ht.locus))\n",
    "        )\n",
    "        \n",
    "        # Check for variant loci outside interval\n",
    "        variants_outside = variant_set_locus - set(vat_loci_in_interval)\n",
    "        if variants_outside:\n",
    "            print(f\"Warning: {len(variants_outside)} specified variants are outside the gene interval:\")\n",
    "            print(f\"  {list(variants_outside)[:5]}...\" if len(variants_outside) > 5 else variants_outside)\n",
    "        else:\n",
    "            matched_count = len(variant_set_locus & set(vat_loci_in_interval))\n",
    "            print(f\"All provided variants are in the interval ({matched_count}/{len(variant_ids)} matched).\")\n",
    "        \n",
    "        # Make a Hail set literal for annotating\n",
    "        variant_set_hl = hl.literal(variant_set_vid)\n",
    "    else:\n",
    "        variant_set_hl = hl.empty_set(hl.tstr)\n",
    "\n",
    "    # Add variant classification\n",
    "    vat_ht = vat_ht.annotate(\n",
    "        is_target_variant = variant_set_hl.contains(vat_ht.vid),\n",
    "        is_plof = (\n",
    "            hl.coalesce(vat_ht.gnomad_all_af, 0) <= maf_threshold\n",
    "        ) & lof_consequences.contains(vat_ht.consequence)\n",
    "    )\n",
    "    \n",
    "    # Filter to only variants we care about before joining\n",
    "    vat_ht = vat_ht.filter(vat_ht.is_target_variant | vat_ht.is_plof)\n",
    "    \n",
    "    # Annotate the exome\n",
    "    ann_exome_mt = filtered_exome.annotate_rows(annotations = vat_ht[filtered_exome.row_key])\n",
    "    \n",
    "    # Filter to only rows with annotations\n",
    "    ann_exome_mt = ann_exome_mt.filter_rows(hl.is_defined(ann_exome_mt.annotations))\n",
    "    \n",
    "    # Add cleaned columns for analysis\n",
    "    ann_exome_mt = ann_exome_mt.annotate_rows(\n",
    "        vid = ann_exome_mt.annotations.vid,\n",
    "        gene = ann_exome_mt.annotations.gene_symbol,\n",
    "        gnomad_af = hl.coalesce(ann_exome_mt.annotations.gnomad_all_af, 0),\n",
    "        aa_change = ann_exome_mt.annotations.aa_change,\n",
    "        rsid = ann_exome_mt.annotations.dbsnp_rsid,\n",
    "        consequence = ann_exome_mt.annotations.consequence,\n",
    "        dna_change_in_transcript = ann_exome_mt.annotations.dna_change_in_transcript,\n",
    "        genomic_location = ann_exome_mt.annotations.genomic_location,\n",
    "        is_canonical_transcript = ann_exome_mt.annotations.is_canonical_transcript,\n",
    "        is_plof = ann_exome_mt.annotations.is_plof,\n",
    "        is_target = ann_exome_mt.annotations.is_target_variant\n",
    "    )\n",
    "    \n",
    "    return ann_exome_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_participant_variants(ann_exome_mt, min_alt_alleles=1):\n",
    "    \"\"\"\n",
    "    Get participant-variant information efficiently\n",
    "    \n",
    "    :param ann_exome_mt: Annotated exome MT\n",
    "    :param min_alt_alleles: Minimum alt alleles to include (1 for any carrier, 2 for biallelic)\n",
    "    :return: Hail Table with participant-variant information\n",
    "    \"\"\"\n",
    "    # Annotate entries with n_alt\n",
    "    ann_exome_mt = ann_exome_mt.annotate_entries(\n",
    "        n_alt = ann_exome_mt.GT.n_alt_alleles()\n",
    "    )\n",
    "    \n",
    "    # Convert to entries table and filter to carriers\n",
    "    entries = ann_exome_mt.entries()\n",
    "    carriers = entries.filter(entries.n_alt > 0)\n",
    "\n",
    "    # Remove keys so 's' is a regular column\n",
    "    carriers = carriers.key_by()\n",
    "\n",
    "    # Convert complex types to Arrow/pandas-friendly formats\n",
    "    carriers = carriers.annotate(\n",
    "        locus_str = hl.str(carriers.locus),\n",
    "        alleles_str = hl.delimit(carriers.alleles, \"/\")\n",
    "    ).drop('locus', 'alleles')\n",
    "\n",
    "    # Select essential columns\n",
    "    carriers = carriers.select(\n",
    "        's',\n",
    "        'locus_str',\n",
    "        'alleles_str',\n",
    "        'n_alt',\n",
    "        'vid',\n",
    "        'gene',\n",
    "        'gnomad_af',\n",
    "        'aa_change',\n",
    "        'dna_change_in_transcript',\n",
    "        'rsid',\n",
    "        'consequence',\n",
    "        'is_plof',\n",
    "        'is_target'\n",
    "    )\n",
    "\n",
    "    # Export to Polars directly\n",
    "    carriers_pd = carriers.to_pandas()\n",
    "    carriers_pl = pl.from_pandas(carriers_pd)\n",
    "    \n",
    "    # Add participant-level summaries using Polars operations\n",
    "    participant_summary = carriers_pl.group_by('s').agg([\n",
    "        pl.col('n_alt').sum().alias('total_alt_alleles'),\n",
    "        pl.col('vid').count().alias('total_variants'),\n",
    "        pl.col('is_plof').any().alias('has_plof'),\n",
    "        pl.col('is_target').any().alias('has_target')\n",
    "    ])\n",
    "    \n",
    "    # Join back to get full information\n",
    "    carriers_with_summary = carriers_pl.join(\n",
    "        participant_summary,\n",
    "        on='s',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Filter by min_alt_alleles if needed\n",
    "    if min_alt_alleles > 1:\n",
    "        carriers_with_summary = carriers_with_summary.filter(\n",
    "            pl.col('total_alt_alleles') >= min_alt_alleles\n",
    "        )\n",
    "    \n",
    "    return carriers_with_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cohort_summary(carriers_pl):\n",
    "    \"\"\"\n",
    "    Get summary statistics for the cohort\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'unique_participants': carriers_pl['s'].n_unique(),\n",
    "        'biallelic_participants': carriers_pl.filter(\n",
    "            pl.col('total_alt_alleles') >= 2\n",
    "        )['s'].n_unique(),\n",
    "        'total_variant_observations': len(carriers_pl),\n",
    "        'unique_variants': carriers_pl['vid'].n_unique(),\n",
    "        'plof_observations': carriers_pl.filter(pl.col('is_plof'))['vid'].count(),\n",
    "        'target_observations': carriers_pl.filter(pl.col('is_target'))['vid'].count()\n",
    "    }\n",
    "    \n",
    "    # Variant breakdown\n",
    "    variant_stats = carriers_pl.group_by('vid').agg([\n",
    "        pl.col('is_plof').first(),\n",
    "        pl.col('is_target').first(),\n",
    "        pl.col('s').count().alias('carrier_count'),\n",
    "        pl.col('gnomad_af').first()\n",
    "    ]).sort('carrier_count', descending=True)\n",
    "    \n",
    "    return summary, variant_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Variant Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Files for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Setup/initialize the program\n",
    "initialize()\n",
    "\n",
    "#exome matrix table: contains all participants, all variants in exonic regions (+ 15bp into the introns)\n",
    "exome_mt = hl.read_matrix_table(\"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/exome/splitMT/hail.mt\")\n",
    "\n",
    "#variant annotation table\n",
    "vat_ht = hl.read_table(f'{bucket or my_bucket}/wgs_v8/vat_v8_all.ht/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_pl = pl.read_csv(f'{bucket}/data/variants/AllVariantsHTT(in).csv', has_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_ids = variant_pl['column_1'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gene interval\n",
    "# ENG Chromosome 9: 127,811,130-127,854,773\n",
    "# https://useast.ensembl.org/Homo_sapiens/Gene/Summary?db=core;g=ENSG00000106991;r=9:127811130-127854773\n",
    "# ACVRL1 Chromosome 12: 51,906,908-51,923,361\n",
    "# https://useast.ensembl.org/Homo_sapiens/Gene/Summary?db=core;g=ENSG00000139567;r=12:51906908-51923361\n",
    "#we used the canonical range as defined in the transcript table******\n",
    "\n",
    "gene_interval = ['chr12:51907504-51923361', 'chr9:127815016-127854658']\n",
    "                        \n",
    "#define cut-off for minor allele frequency (MAF) in gnomad (all populations) \n",
    "maf_threshold = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter exome to gene interval\n",
    "filtered_exome = filter_exome(exome_mt, gene_interval)\n",
    "print(\"Exome filtered\")\n",
    "\n",
    "# Annotate exome\n",
    "annotated_exome = annotate_exome(\n",
    "    vat_ht, gene_interval, variant_ids, filtered_exome, maf_threshold\n",
    ")\n",
    "print(\"Exome annotated\")\n",
    "\n",
    "# Get all carriers using Polars\n",
    "print(\"Finding carriers and converting to Polars...\")\n",
    "all_carriers_pl = get_participant_variants(annotated_exome, min_alt_alleles=1)\n",
    "\n",
    "# Get biallelic carriers only\n",
    "biallelic_carriers_pl = all_carriers_pl.filter(\n",
    "    pl.col('total_alt_alleles') >= 2\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "print(\"\\nCohort Summary:\")\n",
    "summary, variant_stats = get_cohort_summary(all_carriers_pl)\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create participant summary using Polars\n",
    "participant_summary_pl = all_carriers_pl.group_by('s').agg([\n",
    "    pl.col('total_alt_alleles').first(),\n",
    "    pl.col('total_variants').first(),\n",
    "    pl.col('has_plof').first(),\n",
    "    pl.col('has_target').first(),\n",
    "    pl.col('vid').unique().alias('variant_list')\n",
    "]).sort('total_alt_alleles', descending=True)\n",
    "\n",
    "print(f\"\\nParticipant summary shape: {participant_summary_pl.shape}\")\n",
    "print(f\"All carriers dataframe shape: {all_carriers_pl.shape}\")\n",
    "print(f\"Biallelic carriers: {len(biallelic_carriers_pl['s'].unique())}\")\n",
    "\n",
    "# Show top variants by carrier count\n",
    "print(\"\\nTop 5 variants by carrier count:\")\n",
    "print(variant_stats.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_people = all_carriers_pl.group_by('s').len().filter(pl.col('len')>1)['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_carriers_pl.filter(pl.col('s').is_in(duplicate_people.implode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_carriers_pl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary by vid (including missing variant_ids)\n",
    "summary_df = (\n",
    "    pl.DataFrame({'vid': variant_ids})\n",
    "    .join(\n",
    "        all_carriers_pl.group_by('vid').agg([\n",
    "            pl.col('locus_str').first(),\n",
    "            pl.col('aa_change').first(), \n",
    "            pl.col('dna_change_in_transcript').first(),\n",
    "            pl.col('rsid').first(),\n",
    "            pl.col('is_plof').first(),\n",
    "            pl.col('is_target').first(),\n",
    "            (pl.col('n_alt') == 1).sum().alias('het_count'),\n",
    "            (pl.col('n_alt') == 2).sum().alias('hom_count')\n",
    "        ]),\n",
    "        on='vid',\n",
    "        how='left'\n",
    "    )\n",
    "    .with_columns([\n",
    "        pl.col('het_count').fill_null(0),\n",
    "        pl.col('hom_count').fill_null(0)\n",
    "    ])\n",
    "    .with_columns(\n",
    "        (pl.col('het_count') + pl.col('hom_count')).alias('total_het_or_hom')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_carriers_pl.write_csv(f'{bucket}/data/cohorts/v0/eng_acvrl1_variant_carriers.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.write_csv(f'{bucket}/data/cohorts/v0/eng_acvrl1_variant_summary.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed\n",
    "!pip install upsetplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upsetplot import plot\n",
    "from upsetplot import from_memberships\n",
    "import matplotlib, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean columns for each annotation \n",
    "annotation_cols = ['is_plof', 'is_target']\n",
    "\n",
    "# Plot 1: All variants\n",
    "data_all = all_carriers_pl.select(annotation_cols).cast(pl.Boolean)\n",
    "result_list = [\n",
    "    [col for col, val in zip(annotation_cols, row) if val]\n",
    "    for row in data_all.iter_rows()\n",
    "]\n",
    "\n",
    "# Create the UpSet plot data\n",
    "data = from_memberships(result_list)\n",
    "# Create the UpSet plot\n",
    "plot(data, subset_size=\"count\", sort_by='cardinality', sort_categories_by='-cardinality', show_counts=True)\n",
    "plt.title(f'UpSet Plot of Variant Annotations: All Participant Variants ({len(all_carriers_pl)})')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Unique variants\n",
    "# Get unique variants\n",
    "data_all_unique = (\n",
    "    all_carriers_pl\n",
    "    .unique(subset=['dna_change_in_transcript'])\n",
    "    .select(annotation_cols)\n",
    "    .cast(pl.Boolean)\n",
    ")\n",
    "result_list_unique = [\n",
    "    [col for col, val in zip(annotation_cols, row) if val]\n",
    "    for row in data_all_unique.iter_rows()\n",
    "]\n",
    "\n",
    "# Create the UpSet plot data\n",
    "data_unique = from_memberships(result_list_unique)\n",
    "# Create the UpSet plot\n",
    "plot(data_unique, subset_size=\"count\", sort_by='cardinality', sort_categories_by='-cardinality', show_counts=True)\n",
    "plt.title(f'UpSet Plot of Variant Annotations: Unique Variants ({len(data_all_unique)})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# copy csv file from the bucket to the current working space\n",
    "os.system(f\"gsutil cp '{bucket}/data/cohorts/v0/eng_acvrl1_variant_summary.tsv' .\")\n",
    "#print(f'[INFO] {name_of_file_in_bucket} is successfully downloaded into your working space')\n",
    "# save dataframe in a csv file in the same workspace as the notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
