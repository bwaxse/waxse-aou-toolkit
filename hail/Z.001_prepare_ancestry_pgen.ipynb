{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy, scipy\n",
    "import plotnine as plt9\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "version = %env WORKSPACE_CDR\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dsub for pgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_pgen_subset.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Subset whole-cohort pgen by ancestry\n",
    "# Input: whole cohort pgen files + sample file for ancestry\n",
    "# Output: ancestry-specific pgen files\n",
    "\n",
    "INPUT_PGEN_BASE=\"${INPUT_PGEN_PGEN%.*}\"  # Remove .pgen extension\n",
    "echo \"Derived INPUT_PGEN_BASE: $INPUT_PGEN_BASE\"\n",
    "\n",
    "nthread=$(python -c \"import os; print(len(os.sched_getaffinity(0)))\");\n",
    "echo \"Running with $nthread threads\";\n",
    "\n",
    "OUTPUT_PREFIX=\"${OUTPUT_RESULTS%\\*}\"\n",
    "# Set ancestry-specific output\n",
    "\n",
    "echo \"Subsetting pgen for samples in: $SAMPLE_FILE\"\n",
    "echo \"Output prefix: $OUTPUT_PREFIX\"\n",
    "\n",
    "# Use plink2 to subset the pgen files\n",
    "# FILTER='PASS' & HWE>1e-10 & F_MISSING<0.05\n",
    "plink2 \\\n",
    "    --pfile $INPUT_PGEN_BASE \\\n",
    "    --keep $SAMPLE_FILE \\\n",
    "    --min-af ${MAF}:minor \\\n",
    "    --max-alleles 2 \\\n",
    "    --snps-only \\\n",
    "    --var-filter \\\n",
    "    --hwe ${HWE_PVAL} \\\n",
    "    --geno ${MISSING_RATE} \\\n",
    "    --make-pgen \\\n",
    "    --memory 100000 \\\n",
    "    --threads $nthread \\\n",
    "    --out $OUTPUT_PREFIX\n",
    "\n",
    "echo \"Pgen subsetting complete for ancestry\"\n",
    "echo \"Files created with prefix: $OUTPUT_PREFIX\"\n",
    "ls -la ${OUTPUT_PREFIX}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(query_dir):\n",
    "    tmp = subprocess.run(\n",
    "        f'gsutil ls {query_dir}',\n",
    "        shell=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    files = tmp.stdout.decode('utf-8').split('\\n')\n",
    "    return(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsub_pgen_subset(\n",
    "    machine_type,\n",
    "    input_pgen_base,\n",
    "    sample_file,\n",
    "    out_base,\n",
    "    minor_allele_freq=0.01,\n",
    "    hwe_pval=1e-10,\n",
    "    missing_rate=0.05,\n",
    "    script='run_pgen_subset.sh'\n",
    "):\n",
    "    \n",
    "    dsub_user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    user_name = os.getenv(\"OWNER_EMAIL\").split('@')[0].replace('.', '-')\n",
    "\n",
    "    ancestry = sample_file.split('/')[-2]  # Get ancestry from path\n",
    "    chrom = input_pgen_base.split('chr')[-1].split('.')[0]  # Extract chr number\n",
    "    job_name = f\"{ancestry}-c{chrom}\"  # e.g., \"eur-c22\", \"afr-c1\"\n",
    "\n",
    "    # Environment variables\n",
    "    env_vars = {\n",
    "        'DSUB_USER_NAME': dsub_user_name,\n",
    "        'USER_NAME': user_name,\n",
    "        'JOB_NAME': job_name,\n",
    "        'MACHINE_TYPE': machine_type,\n",
    "        'SCRIPT': script,\n",
    "        'MAF': str(minor_allele_freq),\n",
    "        'HWE_PVAL': str(hwe_pval),\n",
    "        'MISSING_RATE': str(missing_rate)\n",
    "    }\n",
    "    \n",
    "    # Set environment variables\n",
    "    for key, value in env_vars.items():\n",
    "        os.environ[key] = value\n",
    "    \n",
    "    # Build dsub command\n",
    "    cmd = [\n",
    "        'dsub',\n",
    "        '--provider', 'google-cls-v2',\n",
    "        '--machine-type', machine_type,\n",
    "        '--disk-type', 'pd-ssd',\n",
    "        '--boot-disk-size', '200',\n",
    "        '--disk-size', '300',\n",
    "        '--user-project', os.environ['GOOGLE_PROJECT'],\n",
    "        '--project', os.environ['GOOGLE_PROJECT'],\n",
    "        '--image', 'us.gcr.io/broad-dsp-gcr-public/terra-jupyter-aou:2.2.14',\n",
    "        '--network', 'network',\n",
    "        '--subnetwork', 'subnetwork',\n",
    "        '--service-account', subprocess.check_output(['gcloud', 'config', 'get-value', 'account']).decode().strip(),\n",
    "        '--user', dsub_user_name,\n",
    "        '--logging', f\"{os.environ['WORKSPACE_BUCKET']}/dsub/logs/{{job-name}}/{{user-id}}/{{job-id}}-{{task-id}}-{{task-attempt}}.log\",\n",
    "        '--name', env_vars['JOB_NAME'],\n",
    "        '--env', f'GOOGLE_PROJECT={os.environ[\"GOOGLE_PROJECT\"]}',\n",
    "        '--env', f'MAF={minor_allele_freq}',\n",
    "        '--env', f'HWE_PVAL={hwe_pval}',\n",
    "        '--env', f'MISSING_RATE={missing_rate}',\n",
    "        # Input pgen files (all 3 components)\n",
    "        '--input', f'INPUT_PGEN_PGEN={input_pgen_base}.pgen',\n",
    "        '--input', f'INPUT_PGEN_PSAM={input_pgen_base}.psam', \n",
    "        '--input', f'INPUT_PGEN_PVAR={input_pgen_base}.pvar',\n",
    "        '--input', f'SAMPLE_FILE={sample_file}',\n",
    "        # Output files\n",
    "        '--output', f'OUTPUT_RESULTS={out_base}*',\n",
    "        '--script', script\n",
    "    ]\n",
    "    \n",
    "    subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pgen_ancestry_pipeline(\n",
    "    df,\n",
    "    ancestries,\n",
    "    whole_cohort_pgen_base,\n",
    "    base_out_dir,\n",
    "    maf=0.01,\n",
    "    hwe_pval=1e-10,\n",
    "    missing_rate=0.05,\n",
    "    script='run_pgen_subset.sh',\n",
    "    chroms=range(1, 23)\n",
    "):\n",
    "    \"\"\"\n",
    "    Subset whole-cohort pgen files by ancestry\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare all sample files upfront\n",
    "    sample_files = {}\n",
    "    \n",
    "    for anc in ancestries:\n",
    "        df_anc = df.filter(pl.col('ancestry_pred_other') == anc)\n",
    "        sample_ids = df_anc['research_id'].to_list()\n",
    "        \n",
    "        sample_file = f'{anc}_samples.txt'\n",
    "        with open(sample_file, 'w') as f:\n",
    "            f.writelines([str(x) + '\\n' for x in sample_ids])\n",
    "        \n",
    "        # Upload to bucket\n",
    "        out_dir = f'{base_out_dir}/{anc}'\n",
    "        os.system(f'gsutil cp {sample_file} {out_dir}/samples.txt')\n",
    "        sample_files[anc] = f'{out_dir}/samples.txt'\n",
    "    \n",
    "    # Process each chromosome for each ancestry\n",
    "    for chrom in chroms:\n",
    "        for anc in ancestries:\n",
    "            out_dir = f'{base_out_dir}/{anc}'\n",
    "            \n",
    "            # Check if already exists\n",
    "            existing_files = [x.split('/')[-1] for x in get_file_list(out_dir) if x.endswith('.pgen')]\n",
    "            if f'genotypes_chr{chrom}.pgen' not in existing_files:\n",
    "                dsub_pgen_subset(\n",
    "                    machine_type='c3-highmem-22', # check if this machine is required next iteration\n",
    "                    input_pgen_base=whole_cohort_pgen_base.format(chrom),\n",
    "                    sample_file=sample_files[anc],\n",
    "                    out_base=f'{out_dir}/genotypes_chr{chrom}',\n",
    "                    minor_allele_freq=maf,\n",
    "                    hwe_pval=hwe_pval,\n",
    "                    missing_rate=missing_rate,\n",
    "                    script=script\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sex_pgen_ancestry_pipeline(\n",
    "    df,\n",
    "    ancestries,\n",
    "    whole_cohort_pgen_base,\n",
    "    base_out_dir,\n",
    "    maf=0.01,\n",
    "    hwe_pval=1e-10,\n",
    "    missing_rate=0.05,\n",
    "    script='run_pgen_subset.sh'\n",
    "):\n",
    "    \"\"\"\n",
    "    Subset whole-cohort pgen files by ancestry\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare all sample files upfront\n",
    "    sample_files = {}\n",
    "    \n",
    "    for anc in ancestries:\n",
    "        df_anc = df.filter(pl.col('ancestry_pred_other') == anc)\n",
    "        sample_ids = df_anc['research_id'].to_list()\n",
    "        \n",
    "        sample_file = f'{anc}_samples.txt'\n",
    "        with open(sample_file, 'w') as f:\n",
    "            f.writelines([str(x) + '\\n' for x in sample_ids])\n",
    "        \n",
    "        # Upload to bucket\n",
    "        out_dir = f'{base_out_dir}/{anc}'\n",
    "        os.system(f'gsutil cp {sample_file} {out_dir}/samples.txt')\n",
    "        sample_files[anc] = f'{out_dir}/samples.txt'\n",
    "    \n",
    "    # Process each chromosome for each ancestry\n",
    "    for chrom in ['X', 'Y']:\n",
    "        for anc in ancestries:\n",
    "            out_dir = f'{base_out_dir}/{anc}'\n",
    "            \n",
    "            # Check if already exists\n",
    "            existing_files = [x.split('/')[-1] for x in get_file_list(out_dir) if x.endswith('.pgen')]\n",
    "            if f'genotypes_chr{chrom}.pgen' not in existing_files:\n",
    "                dsub_pgen_subset(\n",
    "                    machine_type='c3-highmem-22', # check if this machine is required next iteration\n",
    "                    input_pgen_base=whole_cohort_pgen_base.format(chrom),\n",
    "                    sample_file=sample_files[anc],\n",
    "                    out_base=f'{out_dir}/genotypes_chr{chrom}',\n",
    "                    minor_allele_freq=maf,\n",
    "                    hwe_pval=hwe_pval,\n",
    "                    missing_rate=missing_rate,\n",
    "                    script=script\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pgen_pipeline(\n",
    "    df,\n",
    "    whole_cohort_pgen_base,\n",
    "    base_out_dir,\n",
    "    maf=0.01,\n",
    "    hwe_pval=1e-10,\n",
    "    missing_rate=0.05,\n",
    "    script='run_pgen_subset.sh'\n",
    "):\n",
    "    \"\"\"\n",
    "    Subset whole-cohort pgen files\n",
    "    \"\"\"\n",
    "        \n",
    "    sample_ids = df['research_id'].to_list()\n",
    "\n",
    "    sample_file = f'all_samples.txt'\n",
    "    with open(sample_file, 'w') as f:\n",
    "        f.writelines([str(x) + '\\n' for x in sample_ids])\n",
    "\n",
    "    # Upload to bucket\n",
    "    out_dir = f'{base_out_dir}/all'\n",
    "    os.system(f'gsutil cp {sample_file} {out_dir}/samples.txt')\n",
    "    final_sample_file = f'{out_dir}/samples.txt'\n",
    "    \n",
    "    # Process each chromosome for each ancestry\n",
    "    for chrom in list(range(1, 23)) + ['X', 'Y']:\n",
    "        out_dir = f'{base_out_dir}/all'\n",
    "\n",
    "        # Check if already exists\n",
    "        existing_files = [x.split('/')[-1] for x in get_file_list(out_dir) if x.endswith('.pgen')]\n",
    "        if f'genotypes_chr{chrom}.pgen' not in existing_files:\n",
    "            dsub_pgen_subset(\n",
    "                machine_type='c3-highmem-22', # check if this machine is required next iteration\n",
    "                input_pgen_base=whole_cohort_pgen_base.format(chrom),\n",
    "                sample_file=final_sample_file,\n",
    "                out_base=f'{out_dir}/genotypes_chr{chrom}',\n",
    "                minor_allele_freq=maf,\n",
    "                hwe_pval=hwe_pval,\n",
    "                missing_rate=missing_rate,\n",
    "                script=script\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dsub_status(user=None, full=False):\n",
    "    \"\"\"Check status of dsub jobs for the specified user\"\"\"\n",
    "    if user is None:\n",
    "        # Get current user if not specified\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "    \n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "\n",
    "    if full:\n",
    "        make_full = ' --full'\n",
    "    else:\n",
    "        make_full = ''\n",
    "    \n",
    "    cmd = f\"dstat --provider google-cls-v2 --user {user} --status '*' --project {project}{make_full}\"\n",
    "    # cmd = f\"ddel --provider google-cls-v2 --project terra-vpc-sc-840afe1e --location us-central1 --jobs 'transances--bwaxse--250319-022343-75' --users 'bwaxse'\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_details(user=None, job=None):\n",
    "    \"\"\"List all jobs for the user, including failed ones\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    if user is None:\n",
    "        user = os.getenv(\"OWNER_EMAIL\").split('@')[0]\n",
    "        \n",
    "    if job is None:\n",
    "        job = \"'*' \"\n",
    "    else:\n",
    "        job = f'--jobs {job} '\n",
    "    \n",
    "    cmd = f\"dstat --provider google-cls-v2 --project {project} --user {user} --status {job}--full\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_job(job_id):\n",
    "    \"\"\"Cancel a specific job\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    cmd = f\"ddel --provider google-cls-v2 --project {project} --jobs {job_id}\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    return subprocess.run(cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_running_jobs():\n",
    "    \"\"\"Cancel only running/pending jobs (safer)\"\"\"\n",
    "    project = os.getenv(\"GOOGLE_PROJECT\")\n",
    "    \n",
    "    # Cancel only running jobs\n",
    "    cancel_cmd = f\"ddel --provider google-cls-v2 --project {project} --users 'bwaxse' --jobs '*'\"\n",
    "    print(f\"Canceling running jobs: {cancel_cmd}\")\n",
    "    \n",
    "    return subprocess.run(cancel_cmd, shell=True, capture_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and Ancestry Preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AoU Ancestry Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_file = 'gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/ancestry/ancestry_preds.tsv'\n",
    "\n",
    "# # Copy the file\n",
    "# os.system(f\"gsutil -u $GOOGLE_PROJECT cp {source_file} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_df = pl.read_csv('ancestry_preds.tsv',\n",
    "                          separator='\\t',\n",
    "                          schema_overrides={ 'research_id' : pl.Utf8 })\n",
    "print(f'{ancestry_df.height} research_id in ancestry_preds.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Flagged Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs_file = \"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/qc/flagged_samples.tsv\"\n",
    "# !gsutil -u $$GOOGLE_PROJECT cp {fs_file} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = pl.read_csv(\n",
    "    'flagged_samples.tsv',\n",
    "    separator='\\t',\n",
    "    schema_overrides={ 's' : pl.Utf8 }\n",
    ")\n",
    "fs_samps = fs['s'].to_list()\n",
    "print(f'{fs.height} s in flagged_samples.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~pl.col('research_id').is_in(fs_samps)\n",
    "ancestry_df = ancestry_df.filter(mask)\n",
    "print(f'{ancestry_df.height} research_id in ancestry_preds.tsv after removing flagged samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Related Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rel_file = \"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv\"\n",
    "# !gsutil -u $$GOOGLE_PROJECT cp {rel_file} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = pl.read_csv(\n",
    "    'relatedness_flagged_samples.tsv',\n",
    "    separator='\\t',\n",
    "    schema_overrides={ 'sample_id' : pl.Utf8 }\n",
    ")\n",
    "rel_samps = rel['sample_id'].to_list()\n",
    "print(f'{rel.height} sample_id in relatedness_flagged_samples.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_mask = ~pl.col('research_id').is_in(rel_samps)\n",
    "ancestry_df = ancestry_df.filter(rel_mask)\n",
    "print(f'{ancestry_df.height} research_id in ancestry_preds.tsv after also removing related samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_df.filter(pl.col('ancestry_pred_other')=='eur').height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_df.write_csv(f'{my_bucket}/data/ancestry_metadata.tsv', separator='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ancestries\n",
    "Need massive memory to handle these files (c3-highmem-22, 176 GiB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf = 0.01\n",
    "incl_filt = \"FILTER='PASS' & HWE>0.0000000001 & F_MISSING < 0.05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries_considered = ['eur', 'afr', 'amr', 'eas', 'sas', 'mid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v8 whole-cohort pgen files\n",
    "whole_cohort_base = 'gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/acaf_threshold/pgen/acaf_threshold.chr{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run for each ancestry\n",
    "# run_pgen_ancestry_pipeline(\n",
    "#     ancestry_df,\n",
    "#     ancestries_considered,\n",
    "#     whole_cohort_base,\n",
    "#     f'{my_bucket}/data/stg001',\n",
    "#     maf=0.01,\n",
    "#     hwe_pval=1e-10, \n",
    "#     missing_rate=0.05, \n",
    "#     script='run_pgen_subset.sh'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run for sex chromosomes\n",
    "# run_sex_pgen_ancestry_pipeline(\n",
    "#     ancestry_df,\n",
    "#     ['eur', 'afr', 'amr', 'eas', 'sas'],\n",
    "#     whole_cohort_base,\n",
    "#     f'{my_bucket}/data/stg001',\n",
    "#     maf=0.01,\n",
    "#     hwe_pval=1e-10, \n",
    "#     missing_rate=0.05, \n",
    "#     script='run_pgen_subset.sh'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for whole cohort (all)\n",
    "run_pgen_pipeline(\n",
    "    ancestry_df,\n",
    "    whole_cohort_base,\n",
    "    f'{my_bucket}/data/stg001',\n",
    "    maf=0.01,\n",
    "    hwe_pval=1e-10,\n",
    "    missing_rate=0.05,\n",
    "    script='run_pgen_subset.sh'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check All Statuses\n",
    "check_dsub_status(full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel_running_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 'all-c22--bwaxse--250618-181827-66'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details(job=job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {bucket or my_bucket}/dsub/logs/all-c22/bwaxse/all-c22--bwaxse--250618-181827-66-task-None.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {bucket or my_bucket}/data/stg001/all/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
