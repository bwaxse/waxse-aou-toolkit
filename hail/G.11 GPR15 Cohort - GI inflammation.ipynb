{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#README\" data-toc-modified-id=\"README-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>README</a></span></li></ul></li><li><span><a href=\"#Initial-setup\" data-toc-modified-id=\"Initial-setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initial setup</a></span></li><li><span><a href=\"#Gather-Homozygote/Heterozygote-Counts-(Pathfinder)\" data-toc-modified-id=\"Gather-Homozygote/Heterozygote-Counts-(Pathfinder)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Gather Homozygote/Heterozygote Counts (Pathfinder)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-Files-for-Analysis\" data-toc-modified-id=\"Read-in-Files-for-Analysis-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Read in Files for Analysis</a></span></li><li><span><a href=\"#Define-hyperparameters\" data-toc-modified-id=\"Define-hyperparameters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Define hyperparameters</a></span></li><li><span><a href=\"#Run\" data-toc-modified-id=\"Run-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Run</a></span></li><li><span><a href=\"#Generate-participant-lists\" data-toc-modified-id=\"Generate-participant-lists-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Generate participant lists</a></span></li><li><span><a href=\"#Variant-Counts\" data-toc-modified-id=\"Variant-Counts-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Variant Counts</a></span></li></ul></li><li><span><a href=\"#Generate-Cohort-(PheTK)---check-related\" data-toc-modified-id=\"Generate-Cohort-(PheTK)---check-related-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Generate Cohort (PheTK) - check related</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-Cohorts\" data-toc-modified-id=\"Generate-Cohorts-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Generate Cohorts</a></span></li><li><span><a href=\"#Compare-Pathfinder-and-PheTK\" data-toc-modified-id=\"Compare-Pathfinder-and-PheTK-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Compare Pathfinder and PheTK</a></span></li><li><span><a href=\"#Add-covariate-data\" data-toc-modified-id=\"Add-covariate-data-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Add covariate data</a></span></li><li><span><a href=\"#Add-IBD-data\" data-toc-modified-id=\"Add-IBD-data-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Add IBD data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialize\" data-toc-modified-id=\"Initialize-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Initialize</a></span></li><li><span><a href=\"#IBD-by-AoUQueries-(ICD-and-SNOMED)\" data-toc-modified-id=\"IBD-by-AoUQueries-(ICD-and-SNOMED)-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>IBD by AoUQueries (ICD and SNOMED)</a></span></li><li><span><a href=\"#IBD-by-PheTK-Method-(Phecode-Only)\" data-toc-modified-id=\"IBD-by-PheTK-Method-(Phecode-Only)-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>IBD by PheTK Method (Phecode Only)</a></span></li><li><span><a href=\"#Add-Age-at-IBD\" data-toc-modified-id=\"Add-Age-at-IBD-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Add Age at IBD</a></span></li></ul></li></ul></li><li><span><a href=\"#Assess-Genetic-Ancestry\" data-toc-modified-id=\"Assess-Genetic-Ancestry-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Assess Genetic Ancestry</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README ##\n",
    "\n",
    "This notebook creates a cohort using hail starting from a variant list.\n",
    "\n",
    "__For VM configurations__, __if you are only run PheWAS and Manhattan plot__, a __standard VM__ with __96 CPUs, 86.4 GB RAM (3.51 USD/hour)__ is sufficient.__dataproc VM__ with __main instance__ having __64 CPUs, 240 GB RAM__, and __10 workers (2 standard, 8 preemptible)__ having __4 CPUs, 15GB RAM (5.76 USD/hour)__ is suggested. Storage just needs to be enough to meet requirement, e.g., ~ 150GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install polars && pip install connectorx && pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# !pip install PheTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show PheTK | grep Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import subprocess\n",
    "import math\n",
    "# import hail as hl\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Union\n",
    "from jinja2 import Template, Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "print(bucket)\n",
    "CDR = os.getenv('WORKSPACE_CDR')\n",
    "print(CDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This line allows for the plots to be displayed inline in the Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"ticks\",font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_fmt_str_lengths(128)\n",
    "\n",
    "# Set the row limit to a higher value\n",
    "pl.Config.set_tbl_rows(50)\n",
    "\n",
    "# show all columns in pandas\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# show full column width\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_gbq(query):\n",
    "    \"\"\"\n",
    "    Take a SQL query and return result as polars dataframe\n",
    "    :param query: BigQuery SQL query\n",
    "    :return: polars dataframe\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query)\n",
    "    rows = query_job.result()\n",
    "    df = pl.from_arrow(rows.to_arrow())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    \"\"\"\n",
    "    Initialize Hail\n",
    "    :param query: BigQuery SQL query\n",
    "    :return: polars dataframe\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    start\n",
    "\n",
    "    hl.init(idempotent = True)\n",
    "    hl.default_reference('GRCh38')\n",
    "    logging.getLogger('hail').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_exome(mt, interval):\n",
    "    \"\"\"\n",
    "    Filter exome matrix table to gene interval of interest\n",
    "    :param mt: Hail matrix table\n",
    "    :param interval: gene interval\n",
    "    :return: Hail matrix table of filtered exome, with n_alt column\n",
    "    \"\"\"\n",
    "    \n",
    "    mt = hl.filter_intervals(mt, [hl.parse_locus_interval(x) for x in interval])\n",
    "    mt = mt.annotate_entries(n_alt = mt.GT.n_alt_alleles())\n",
    "    mt = mt.drop(mt.filters, mt.variant_qc, mt.info)\n",
    "\n",
    "    return mt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_variants(variants):\n",
    "    \"\"\"\n",
    "    convert Pandas dataframe uploaded_var_pd to Hail mt\n",
    "    :param variants: pandas df of variants; key by Name if column with transcripts/c-dots \n",
    "                     already in ENST00000332972.9:c.49C>T format (or key by vid)\n",
    "    :return: Hail matrix table of filtered exome, with n_alt column\n",
    "    \"\"\"\n",
    "\n",
    "    variants_ht = hl.Table.from_pandas(variants, key = 'vid')\n",
    "    \n",
    "    return variants_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_exome(vat_ht, interval, variants_ht, filtered_exome):\n",
    "    \"\"\"\n",
    "    Annotates Hail mt using VAT\n",
    "    :param vat_ht: Hail mt of VAT\n",
    "    :param interval: gene interval\n",
    "    :param variants: Hail mt of variants\n",
    "    :param filtered_exome: filtered exome\n",
    "    :return: Hail matrix table of annotated exome\n",
    "    \"\"\"\n",
    "    \n",
    "    vat_cols = ['vid', 'transcript', 'gene_symbol', 'gnomad_all_af', \n",
    "                'aa_change', 'consequence', 'dna_change_in_transcript', \n",
    "                'exon_number', 'intron_number', 'genomic_location', 'dbsnp_rsid', \n",
    "                'is_canonical_transcript', 'revel', 'splice_ai_acceptor_gain_score',\n",
    "               'splice_ai_acceptor_loss_score', 'splice_ai_donor_gain_score',\n",
    "               'splice_ai_donor_loss_score']\n",
    "    \n",
    "    # filter variant annotation table (VAT) to gene interval of interest\n",
    "    vat_ht = hl.filter_intervals(vat_ht, [hl.parse_locus_interval(x) for x in interval])\n",
    "\n",
    "    # filter VAT by canonical transcript\n",
    "    vat_ht = vat_ht.filter(vat_ht.is_canonical_transcript)\n",
    "    \n",
    "    # subset VAT to list of columns\n",
    "    vat_ht = vat_ht.select(*vat_cols)\n",
    " \n",
    "    # annotate VAT with column \"uploaded_var\", which contains information from \"src\" column\n",
    "    if variants_ht is not None:\n",
    "        # Create a lookup dictionary\n",
    "        variant_lookup = variants_ht.to_pandas().set_index('vid')['src'].to_dict()\n",
    "\n",
    "        # Annotate using the lookup\n",
    "        vat_ht = vat_ht.annotate(\n",
    "            uploaded_var = hl.literal(variant_lookup).get(vat_ht.vid)\n",
    "        )\n",
    "#     # annotate VAT with column \"uploaded_var\", which contains information from \"src\" column\n",
    "#     if variants_ht is not None:\n",
    "#         variants_for_join = variants_ht.select('src').key_by('vid')\n",
    "#         vat_ht_keyed = vat_ht.key_by('vid')\n",
    "        \n",
    "#         # annotate\n",
    "#         vat_ht_annotated = vat_ht_keyed.annotate(uploaded_var = variants_for_join[vat_ht_keyed.vid].src)\n",
    "                \n",
    "#         # Re-key back to original\n",
    "#         vat_ht = vat_ht_annotated.key_by('locus', 'alleles')\n",
    "#     else:\n",
    "#         vat_ht = vat_ht.annotate(uploaded_var = hl.missing(hl.tstr))\n",
    "\n",
    "\n",
    "    # annotate (filtered) exome matrix table with VAT\n",
    "    ann_exome_mt = filtered_exome.annotate_rows(annotations = vat_ht[filtered_exome.row_key])\n",
    "\n",
    "    # add allele_freq column from gnomad\n",
    "    ann_exome_mt = ann_exome_mt.annotate_rows(\n",
    "        allele_freq = hl.if_else(\n",
    "            hl.is_defined(ann_exome_mt.annotations.gnomad_all_af),\n",
    "            ann_exome_mt.annotations.gnomad_all_af,\n",
    "            0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add pLOF column - possible values are 0 and 1\n",
    "    lof_consequence_set = hl.set([\n",
    "                            'start_lost', 'stop_lost', \n",
    "                            'stop_gained','frameshift_variant'\n",
    "                        ])\n",
    "    \n",
    "    ann_exome_mt = ann_exome_mt.annotate_rows(\n",
    "        pLOF = hl.if_else(\n",
    "            (ann_exome_mt.allele_freq <= MAF_freq)\n",
    "             &(lof_consequence_set.contains(ann_exome_mt.annotations.consequence)),\n",
    "            1,\n",
    "            0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add clinvar column - possible values are 0 and 1\n",
    "    ann_exome_mt = ann_exome_mt.annotate_rows(\n",
    "        clinvar = hl.if_else(\n",
    "                    (hl.is_defined(ann_exome_mt.annotations.uploaded_var))\n",
    "                     &(ann_exome_mt.annotations.uploaded_var == \"clinvar\"),\n",
    "                    1,\n",
    "                    0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add lab column - possible values are 0 and 1\n",
    "    ann_exome_mt = ann_exome_mt.annotate_rows(\n",
    "        lab = hl.if_else(\n",
    "                    (hl.is_defined(ann_exome_mt.annotations.uploaded_var))\n",
    "                     &(ann_exome_mt.annotations.uploaded_var == \"lab\"),\n",
    "                    1,\n",
    "                    0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add gnomad_plof column - possible values are 0 and 1\n",
    "    ann_exome_mt = ann_exome_mt.annotate_rows(\n",
    "        gnomad_plof = hl.if_else(\n",
    "                    (hl.is_defined(ann_exome_mt.annotations.uploaded_var))\n",
    "                     &(ann_exome_mt.annotations.uploaded_var == \"gnomad_plof\"),\n",
    "                    1,\n",
    "                    0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # add gnomad_other column - possible values are 0 and 1\n",
    "    ann_exome_mt = ann_exome_mt.annotate_rows(\n",
    "        gnomad_other = hl.if_else(\n",
    "                    (hl.is_defined(ann_exome_mt.annotations.uploaded_var))\n",
    "                     &(ann_exome_mt.annotations.uploaded_var == \"gnomad_other\"),\n",
    "                    1,\n",
    "                    0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add gnomad_missense column - possible values are 0 and 1\n",
    "    ann_exome_mt = ann_exome_mt.annotate_rows(\n",
    "        gnomad_missense = hl.if_else(\n",
    "                    (hl.is_defined(ann_exome_mt.annotations.uploaded_var))\n",
    "                     &(ann_exome_mt.annotations.uploaded_var == \"gnomad_missense\"),\n",
    "                    1,\n",
    "                    0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return ann_exome_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pLOF_variants (ann_exome_mt):\n",
    "    \"\"\"\n",
    "    Filter the interested exome matrix table to only include pLOF variants\n",
    "    :param ann_exome_mt: Hail matrix table of annotated exome\n",
    "    :return: pLOF mt\n",
    "    \"\"\"\n",
    "    plof_mt = ann_exome_mt.filter_rows(ann_exome_mt.pLOF == 1)\n",
    "            \n",
    "    return plof_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinvar_variants (ann_exome_mt):\n",
    "    \"\"\"\n",
    "    Filter the interested exome matrix table to only include clinvar variants\n",
    "    :param ann_exome_mt: Hail matrix table of annotated exome\n",
    "    :return: clinvar mt\n",
    "    \"\"\"\n",
    "    clinvar_mt = ann_exome_mt.filter_rows(ann_exome_mt.clinvar == 1)\n",
    "            \n",
    "    return clinvar_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_variants (ann_exome_mt):\n",
    "    \"\"\"\n",
    "    Filter the interested exome matrix table to only include lab variants\n",
    "    :param ann_exome_mt: Hail matrix table of annotated exome\n",
    "    :return: lab mt\n",
    "    \"\"\"\n",
    "    lab_mt = ann_exome_mt.filter_rows(ann_exome_mt.lab == 1)\n",
    "            \n",
    "    return lab_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnomad_plof_variants (ann_exome_mt):\n",
    "    \"\"\"\n",
    "    Filter the interested exome matrix table to only include gnomad pLOF variants\n",
    "    :param ann_exome_mt: Hail matrix table of annotated exome\n",
    "    :return: gnomad_plof mt\n",
    "    \"\"\"\n",
    "    gnomad_plof_mt = ann_exome_mt.filter_rows(ann_exome_mt.gnomad_plof == 1)\n",
    "            \n",
    "    return gnomad_plof_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnomad_other_variants (ann_exome_mt):\n",
    "    \"\"\"\n",
    "    Filter the interested exome matrix table to only include gnomad other variants\n",
    "    :param ann_exome_mt: Hail matrix table of annotated exome\n",
    "    :return: gnomad_other mt\n",
    "    \"\"\"\n",
    "    gnomad_other_mt = ann_exome_mt.filter_rows(ann_exome_mt.gnomad_other == 1)\n",
    "            \n",
    "    return gnomad_other_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnomad_missense_variants (ann_exome_mt):\n",
    "    \"\"\"\n",
    "    Filter the interested exome matrix table to only include gnomad missense variants\n",
    "    :param ann_exome_mt: Hail matrix table of annotated exome\n",
    "    :return: gnomad_missense mt\n",
    "    \"\"\"\n",
    "    gnomad_missense_mt = ann_exome_mt.filter_rows(ann_exome_mt.gnomad_missense == 1)\n",
    "            \n",
    "    return gnomad_missense_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_components(components):\n",
    "    \"\"\"\n",
    "    Takes in a list of component matrix tables and filters to remove duplicate variants\n",
    "    :param components: component matrix tables\n",
    "    :return: combined exome mt\n",
    "    \"\"\"\n",
    "    \n",
    "    int_exome_mt = components[0]\n",
    "    for component in components[1:]:\n",
    "        int_exome_mt = int_exome_mt.union_rows(component)\n",
    "        \n",
    "    int_exome_mt = int_exome_mt.distinct_by_row() # filter to remove duplicate variants\n",
    "    \n",
    "    return int_exome_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_biallelic_participants(int_exome_mt):\n",
    "    \"\"\"\n",
    "    Searches AoU for participants with 2 alternate alleles among variants\n",
    "    :return: sorted_participant_info, all participant + variant information for all sorted participants\n",
    "    :return: signal_set, set of participant IDs ('s' values) that the program finds to be biallelic for a variant of interest\n",
    "    \"\"\"\n",
    "    print(\"searching biallelic patients now!\")\n",
    "\n",
    "    signal_table = int_exome_mt.entries() # entries step\n",
    "    print(\"converted matrix table to table\")\n",
    "    \n",
    "    # aggregated participant list\n",
    "    geno_agg_ht = signal_table.group_by(signal_table.s).aggregate(n_alt_all = hl.agg.sum(signal_table.n_alt))\n",
    "    signal_set = geno_agg_ht.filter(geno_agg_ht.n_alt_all >= 2)\n",
    "    \n",
    "    print(\"Biallelic participants:\")\n",
    "    signal_set.show()\n",
    "    \n",
    "    cnt = signal_set.count()\n",
    "    print(f\"Number of biallelic participants: {cnt}\")\n",
    "    \n",
    "#     # estimated prevalence\n",
    "#     div_cnt = 245394/cnt\n",
    "#     print(f\"Estimated prevalence based on this feature is 1 in {div_cnt}\")\n",
    "    \n",
    "    # entries table (variant + participant information)\n",
    "    participant_info = signal_table.filter(signal_table.n_alt > 0)\n",
    "    participant_info = participant_info.filter(hl.is_defined(signal_set.index(participant_info['s'])))\n",
    "    \n",
    "    # @Kate DELETE THIS TO BE FASTER... \n",
    "    sorted_participant_info = participant_info.order_by(participant_info.s)\n",
    "\n",
    "    sorted_participant_info.show(100)\n",
    "    \n",
    "    return sorted_participant_info, signal_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_monallelic_participants(int_exome_mt):\n",
    "    \"\"\"\n",
    "    Searches AoU for participants with 1 (or more) alternate alleles among likely pathogenic variants\n",
    "    :return: sorted_participant_info, all participant + variant information for all sorted participants\n",
    "    :return: signal_set, set of participant IDs ('s' values) that the program finds to be monallelic for a variant of interest. \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"searching monallelic patients now!\")\n",
    "\n",
    "    signal_table = int_exome_mt.entries()\n",
    "    print(\"turned matrix table to table\")\n",
    "    \n",
    "    # filter entries table to only include entries with n_alt > 0\n",
    "    signal_set = signal_table.filter(signal_table.n_alt >= 1)\n",
    "    \n",
    "    # export hail table to pandas (~5 minutes) -- alternative is to make a Hail checkpoint...\n",
    "    signal_set_pd = signal_set.to_pandas()\n",
    "    \n",
    "    # group by 's' and sum 'n_alt' (aggregation step)\n",
    "    geno_agg = signal_set_pd.groupby('s')['n_alt'].sum().reset_index()\n",
    "\n",
    "    # merge the aggregated sum back to the original dataframe\n",
    "    geno_agg_pd = pd.merge(signal_set_pd, geno_agg, on='s', how='left', suffixes=('', '_sum'))\n",
    "\n",
    "    # rename columns\n",
    "    geno_agg_pd.rename(columns={'n_alt_sum': 'n_alt_agg', 'annotations.vid':'vid', \n",
    "                           'annotations.gene_symbol':'gene', 'annotations.gnomad_all_af':'gnomad_all_af',\n",
    "                           'annotations.aa_change':'aa_change', 'annotations.dna_change_in_transcript':'dna_change_in_transcript',\n",
    "                           'annotations.genomic_location':'genomic_location', 'annotations.dbsnp_rsid':'rsid',\n",
    "                           'annotations.is_canonical_transcript':'is_canonical_transcript',\n",
    "                           'annotations.revel':'revel',\n",
    "                            'annotations.splice_ai_acceptor_gain_score':'splice_ai_acceptor_gain_score',\n",
    "                           'annotations.splice_ai_acceptor_loss_score':'splice_ai_acceptor_loss_score',\n",
    "                            'annotations.splice_ai_donor_gain_score':'splice_ai_donor_gain_score',\n",
    "                            'annotations.splice_ai_donor_loss_score':'splice_ai_donor_loss_score'}, inplace=True)\n",
    "    \n",
    "    # reduce number of columns (can modify this)\n",
    "    geno_agg_short_pd = geno_agg_pd[['locus', 'alleles', 'vid', 'gene', 'gnomad_all_af', 'aa_change',\n",
    "                                     'dna_change_in_transcript', 'rsid', 'revel', 'splice_ai_acceptor_gain_score',\n",
    "                                     'splice_ai_acceptor_loss_score', 'splice_ai_donor_gain_score',\n",
    "                                     'splice_ai_donor_loss_score', 'pLOF', 'clinvar', 'lab', 'gnomad_plof', 'gnomad_other',\n",
    "                                     'gnomad_missense', 's', 'n_alt_agg']]\n",
    "\n",
    "    geno_agg_short_pd.head()\n",
    "\n",
    "    return geno_agg_short_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_counts(geno_agg_short_pd):\n",
    "    pLOF_pd = geno_agg_short_pd[geno_agg_short_pd['pLOF'] == 1]\n",
    "    pLOF_count = pLOF_pd['vid'].nunique()\n",
    "    print(f\"The pLOF stream contributes: {str(pLOF_count)} variants\")\n",
    "\n",
    "    lab_pd = geno_agg_short_pd[geno_agg_short_pd['lab'] == 1]\n",
    "    lab_count = lab_pd['vid'].nunique()\n",
    "    print(f\"The lab stream contributes: {str(lab_count)} variants\")\n",
    "\n",
    "    clinvar_pd = geno_agg_short_pd[geno_agg_short_pd['clinvar'] == 1]\n",
    "    clinvar_count = clinvar_pd['vid'].nunique()\n",
    "    print(f\"The ClinVar stream contributes: {str(clinvar_count)} variants\")\n",
    " \n",
    "    gnomad_plof_pd = geno_agg_short_pd[geno_agg_short_pd['gnomad_plof'] == 1]\n",
    "    gnomad_plof_count = gnomad_plof_pd['vid'].nunique()\n",
    "    print(f\"The gnomAD pLOF stream contributes: {str(gnomad_plof_count)} variants\")\n",
    " \n",
    "    gnomad_other_pd = geno_agg_short_pd[geno_agg_short_pd['gnomad_other'] == 1]\n",
    "    gnomad_other_count = gnomad_other_pd['vid'].nunique()\n",
    "    print(f\"The gnomAD other stream contributes: {str(gnomad_other_count)} variants\")\n",
    " \n",
    "    gnomad_missense_pd = geno_agg_short_pd[geno_agg_short_pd['gnomad_missense'] == 1]\n",
    "    gnomad_missense_count = gnomad_missense_pd['vid'].nunique()\n",
    "    print(f\"The gnomAD missense stream contributes: {str(gnomad_missense_count)} variants\")\n",
    "    \n",
    "    geno_agg_unique_count = geno_agg_short_pd['vid'].nunique()\n",
    "    print(f\"The total unique variant count is: {str(geno_agg_unique_count)} variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AouQueries:\n",
    "    def __init__(self, version=None, bucket=None):\n",
    "        \"\"\"\n",
    "        Specialized query builder for common All of Us research patterns.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.version = version or os.getenv('WORKSPACE_CDR')\n",
    "        self.bucket = bucket or os.getenv('WORKSPACE_BUCKET')\n",
    "        self.client = bigquery.Client()\n",
    "        self.query_text = None\n",
    "\n",
    "        # Validate required environment\n",
    "        if not self.version:\n",
    "            raise ValueError(\"CDR version not found. Check WORKSPACE_CDR environment variable if none provided.\")\n",
    "        if not self.bucket:\n",
    "            raise ValueError(\"Workspace bucket not found. Check WORKSPACE_BUCKET environment variable if none provided.\")\n",
    "\n",
    "        print(\"version: \" + self.version)\n",
    "        print(\"bucket: \" + self.bucket)\n",
    "            \n",
    "    def find_diagnosis_codes(self, \n",
    "                            vocabularies: List[str] = None,\n",
    "                            search_terms: List[str] = None, \n",
    "                            exclude_terms: List[str] = None,\n",
    "                            exact_codes: Dict[str, List[str]] = None,\n",
    "                            pattern_codes: Dict[str, List[str]] = None,\n",
    "                            exclude_codes: Dict[str, List[str]] = None,\n",
    "                            person_ids: Union[List[int], str] = None):\n",
    "        \"\"\"\n",
    "        Find diagnosis codes in condition_occurrence and observation tables.\n",
    "        \"\"\"\n",
    "\n",
    "        if not vocabularies:\n",
    "            # Default to standard vocabularies if nothing specified\n",
    "            vocabularies = [\"ICD9CM\", \"ICD10CM\", \"SNOMED\"]\n",
    "\n",
    "        # Add any vocabularies from exact_codes or pattern_codes that aren't already included\n",
    "        vocab_additions = set()\n",
    "        for vocab in list(exact_codes.keys() if exact_codes else []) + list(pattern_codes.keys() if pattern_codes else []):\n",
    "            if vocab not in vocabularies:\n",
    "                vocab_additions.add(vocab)\n",
    "                vocabularies.append(vocab)\n",
    "\n",
    "        # Notify if vocabularies were added\n",
    "        if vocab_additions:\n",
    "            print(f\"Added vocabularies from code specifications: {', '.join(vocab_additions)}\")\n",
    "\n",
    "        # Store query parameters for summary\n",
    "        self.last_query_params = {\n",
    "            \"type\": \"diagnosis_codes\",\n",
    "            \"vocabularies\": vocabularies,\n",
    "            \"search_terms\": search_terms,\n",
    "            \"exclude_terms\": exclude_terms,\n",
    "            \"exact_codes\": exact_codes,\n",
    "            \"pattern_codes\": pattern_codes,\n",
    "            \"exclude_codes\": exclude_codes,\n",
    "            \"person_ids\": person_ids\n",
    "        }\n",
    "        \n",
    "        # Generate query summary text\n",
    "        self.query_summary = self._generate_query_summary()\n",
    "    \n",
    "        # Create Jinja2 environment and add filter\n",
    "        env = Environment()\n",
    "        \n",
    "        # Define and add filter to environment\n",
    "        def join_quotes(items):\n",
    "            return ', '.join(f\"'{item}'\" for item in items)\n",
    "\n",
    "        env.filters['join_quotes'] = join_quotes\n",
    "\n",
    "        template = env.from_string(\"\"\"\n",
    "        WITH filtered_concepts AS (\n",
    "            SELECT DISTINCT concept_id, vocabulary_id, concept_code, concept_name\n",
    "            FROM {{ version }}.concept\n",
    "            WHERE \n",
    "                vocabulary_id IN ({{ vocabularies|join_quotes }})\n",
    "                {%- if exclude_terms %}\n",
    "                AND (\n",
    "                    {%- for term in exclude_terms %}\n",
    "                    {{ \"AND \" if not loop.first else \"\" }}LOWER(concept_name) NOT LIKE '%{{ term|lower }}%'\n",
    "                    {%- endfor %}\n",
    "                )\n",
    "                {%- endif %}\n",
    "                {%- for vocab, codes in exclude_codes.items() %}\n",
    "                {%- if codes %}\n",
    "                AND NOT (\n",
    "                    vocabulary_id = '{{ vocab }}' AND concept_code IN ({{ codes|join_quotes }})\n",
    "                )\n",
    "                {%- endif %}\n",
    "                {%- endfor %}\n",
    "                {%- if search_terms or exact_codes or pattern_codes %}\n",
    "                AND (\n",
    "                    {%- if search_terms %}\n",
    "                    (\n",
    "                        {%- for term in search_terms %}\n",
    "                        {{ \"OR \" if not loop.first else \"\" }}LOWER(concept_name) LIKE '%{{ term|lower }}%'\n",
    "                        {%- endfor %}\n",
    "                    )\n",
    "                    {%- endif %}\n",
    "\n",
    "                    {%- if search_terms and (exact_codes or pattern_codes) %}OR{% endif %}\n",
    "\n",
    "                    {%- if exact_codes %}\n",
    "                    (\n",
    "                        {%- for vocab, codes in exact_codes.items() %}\n",
    "                        {%- if codes %}\n",
    "                        {{ \"OR \" if not loop.first else \"\" }}(vocabulary_id = '{{ vocab }}' AND concept_code IN ({{ codes|join_quotes }}))\n",
    "                        {%- endif %}\n",
    "                        {%- endfor %}\n",
    "                    )\n",
    "                    {%- endif %}\n",
    "\n",
    "                    {%- if (search_terms or exact_codes) and pattern_codes %}OR{% endif %}\n",
    "\n",
    "                    {%- if pattern_codes %}\n",
    "                    (\n",
    "                        {%- for vocab, patterns in pattern_codes.items() %}\n",
    "                        {%- if patterns %}\n",
    "                        {{ \"OR \" if not loop.first else \"\" }}(vocabulary_id = '{{ vocab }}' AND (\n",
    "                            {%- for pattern in patterns %}\n",
    "                            {{ \"OR \" if not loop.first else \"\" }}concept_code LIKE '{{ pattern }}'\n",
    "                            {%- endfor %}\n",
    "                        ))\n",
    "                        {%- endif %}\n",
    "                        {%- endfor %}\n",
    "                    )\n",
    "                    {%- endif %}\n",
    "                )\n",
    "                {%- endif %}\n",
    "        ),\n",
    "        -- Regular events (non-V codes)\n",
    "        regular_codes AS (\n",
    "            -- Get codes from condition_occurrence via source_value (non-V codes)\n",
    "            SELECT\n",
    "                co.person_id,\n",
    "                co.condition_source_value AS concept_code,\n",
    "                fc.vocabulary_id,\n",
    "                fc.concept_name,\n",
    "                'condition' AS domain\n",
    "            FROM {{ version }}.condition_occurrence co\n",
    "            JOIN filtered_concepts fc \n",
    "                ON co.condition_source_value = fc.concept_code\n",
    "            WHERE NOT co.condition_source_value LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND co.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Get codes from condition_occurrence via source_concept_id (non-V codes)\n",
    "            SELECT\n",
    "                co.person_id,\n",
    "                fc.concept_code,\n",
    "                fc.vocabulary_id,\n",
    "                fc.concept_name,\n",
    "                'condition' AS domain\n",
    "            FROM {{ version }}.condition_occurrence co\n",
    "            JOIN filtered_concepts fc \n",
    "                ON co.condition_source_concept_id = fc.concept_id\n",
    "            WHERE NOT fc.concept_code LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND co.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Get codes from observation via source_value (non-V codes)\n",
    "            SELECT\n",
    "                o.person_id,\n",
    "                o.observation_source_value AS concept_code,\n",
    "                fc.vocabulary_id,\n",
    "                fc.concept_name,\n",
    "                'observation' AS domain\n",
    "            FROM {{ version }}.observation o\n",
    "            JOIN filtered_concepts fc \n",
    "                ON o.observation_source_value = fc.concept_code\n",
    "            WHERE NOT o.observation_source_value LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND o.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Get codes from observation via source_concept_id (non-V codes)\n",
    "            SELECT\n",
    "                o.person_id,\n",
    "                fc.concept_code,\n",
    "                fc.vocabulary_id,\n",
    "                fc.concept_name,\n",
    "                'observation' AS domain\n",
    "            FROM {{ version }}.observation o\n",
    "            JOIN filtered_concepts fc \n",
    "                ON o.observation_source_concept_id = fc.concept_id\n",
    "            WHERE NOT fc.concept_code LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND o.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "        ),\n",
    "\n",
    "        -- V code events from condition_occurrence with special handling\n",
    "        v_codes AS (\n",
    "            -- source_value path\n",
    "            SELECT\n",
    "                co.person_id,\n",
    "                co.condition_source_value AS concept_code,\n",
    "                co.condition_concept_id AS concept_id,\n",
    "                'condition' AS domain\n",
    "            FROM {{ version }}.condition_occurrence co\n",
    "            JOIN filtered_concepts fc \n",
    "                ON co.condition_source_value = fc.concept_code\n",
    "            WHERE co.condition_source_value LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND co.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- source_concept_id path\n",
    "            SELECT\n",
    "                co.person_id,\n",
    "                fc.concept_code,\n",
    "                co.condition_concept_id AS concept_id,\n",
    "                'condition' AS domain\n",
    "            FROM {{ version }}.condition_occurrence co\n",
    "            JOIN filtered_concepts fc \n",
    "                ON co.condition_source_concept_id = fc.concept_id\n",
    "            WHERE fc.concept_code LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND co.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "            \n",
    "            UNION ALL\n",
    "\n",
    "            -- source_value path\n",
    "            SELECT\n",
    "                o.person_id,\n",
    "                o.observation_source_value AS concept_code,\n",
    "                o.observation_concept_id AS concept_id,\n",
    "                'observation' AS domain\n",
    "            FROM {{ version }}.observation o\n",
    "            JOIN filtered_concepts fc \n",
    "                ON o.observation_source_value = fc.concept_code\n",
    "            WHERE o.observation_source_value LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND o.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- source_concept_id path\n",
    "            SELECT\n",
    "                o.person_id,\n",
    "                fc.concept_code,\n",
    "                o.observation_concept_id AS concept_id,\n",
    "                'observation' AS domain\n",
    "            FROM {{ version }}.observation o\n",
    "            JOIN filtered_concepts fc \n",
    "                ON o.observation_source_concept_id = fc.concept_id\n",
    "            WHERE fc.concept_code LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND o.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "        ),\n",
    "\n",
    "        -- Apply correct vocabulary attribution for V codes\n",
    "        v_codes_corrected AS (\n",
    "            SELECT\n",
    "                v.person_id,\n",
    "                v.concept_code,\n",
    "                c.vocabulary_id,\n",
    "                c.concept_name,\n",
    "                v.domain\n",
    "            FROM v_codes v\n",
    "            JOIN {{ version }}.concept_relationship cr\n",
    "                ON v.concept_id = cr.concept_id_1\n",
    "            JOIN {{ version }}.concept c\n",
    "                ON cr.concept_id_2 = c.concept_id\n",
    "            WHERE c.vocabulary_id IN ({{ vocabularies|join_quotes }})\n",
    "            AND v.concept_code = c.concept_code\n",
    "        ),\n",
    "\n",
    "        -- Combine regular events with corrected V code events\n",
    "        all_events AS (\n",
    "            SELECT * FROM regular_codes\n",
    "            UNION ALL\n",
    "            SELECT * FROM v_codes_corrected\n",
    "        )\n",
    "\n",
    "        -- Final output with counts\n",
    "        SELECT\n",
    "            vocabulary_id,\n",
    "            concept_code,\n",
    "            concept_name,\n",
    "            COUNT(DISTINCT person_id) AS unique_persons,\n",
    "            COUNT(*) AS total_events,\n",
    "        FROM all_events\n",
    "        GROUP BY vocabulary_id, concept_code, concept_name\n",
    "        ORDER BY vocabulary_id ASC, concept_code ASC\n",
    "        \"\"\")       \n",
    "        \n",
    "        # Format person_ids if it's a list\n",
    "        if isinstance(person_ids, list):\n",
    "            person_ids = ', '.join(str(pid) for pid in person_ids)\n",
    "            \n",
    "        # Initialize dictionaries\n",
    "        exact_codes = exact_codes or {}\n",
    "        pattern_codes = pattern_codes or {}\n",
    "        exclude_codes = exclude_codes or {}\n",
    "        \n",
    "        # Render the template\n",
    "        self.query_text = template.render(\n",
    "            version=self.version,\n",
    "            vocabularies=vocabularies,\n",
    "            search_terms=search_terms,\n",
    "            exclude_terms=exclude_terms,\n",
    "            exact_codes=exact_codes,\n",
    "            pattern_codes=pattern_codes,\n",
    "            exclude_codes=exclude_codes,\n",
    "            person_ids=person_ids\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def person_code_df(self,\n",
    "                       name: str,\n",
    "                       vocabularies: List[str] = None,\n",
    "                       search_terms: List[str] = None, \n",
    "                       exclude_terms: List[str] = None,\n",
    "                       exact_codes: Dict[str, List[str]] = None,\n",
    "                       pattern_codes: Dict[str, List[str]] = None,\n",
    "                       exclude_codes: Dict[str, List[str]] = None,\n",
    "                       person_ids: Union[List[int], str] = None,\n",
    "                       dates: bool = False):\n",
    "        \"\"\"\n",
    "        Creates a person-level dataframe with diagnosis information.\n",
    "\n",
    "        Args:\n",
    "            name: String to use as column name prefix (e.g., 'htn')\n",
    "            vocabularies: List of vocabulary IDs to search\n",
    "            search_terms: List of terms to search in concept names\n",
    "            exclude_terms: List of terms to exclude from concept names\n",
    "            exact_codes: Dictionary mapping vocabularies to lists of exact codes\n",
    "            pattern_codes: Dictionary mapping vocabularies to lists of code patterns\n",
    "            exclude_codes: Dictionary mapping vocabularies to lists of codes to exclude\n",
    "            person_ids: List of person IDs to filter to, or a string expression\n",
    "            dates: Whether to include date information (first, second, last)\n",
    "\n",
    "        Returns:\n",
    "            Polars DataFrame with person_id, {name}_n, vocab, and date columns if requested\n",
    "        \"\"\"\n",
    "\n",
    "        if not vocabularies:\n",
    "            # Default to standard vocabularies if nothing specified\n",
    "            vocabularies = [\"ICD9CM\", \"ICD10CM\", \"SNOMED\"]\n",
    "\n",
    "        # Add any vocabularies from exact_codes or pattern_codes that aren't already included\n",
    "        vocab_additions = set()\n",
    "        for vocab in list(exact_codes.keys() if exact_codes else []) + list(pattern_codes.keys() if pattern_codes else []):\n",
    "            if vocab not in vocabularies:\n",
    "                vocab_additions.add(vocab)\n",
    "                vocabularies.append(vocab)\n",
    "\n",
    "        # Notify if vocabularies were added\n",
    "        if vocab_additions:\n",
    "            print(f\"Added vocabularies from code specifications: {', '.join(vocab_additions)}\")\n",
    "\n",
    "        # Store query parameters for summary\n",
    "        self.last_query_params = {\n",
    "            \"type\": \"person_diagnosis\",\n",
    "            \"name\": name,\n",
    "            \"vocabularies\": vocabularies,\n",
    "            \"search_terms\": search_terms,\n",
    "            \"exclude_terms\": exclude_terms,\n",
    "            \"exact_codes\": exact_codes,\n",
    "            \"pattern_codes\": pattern_codes,\n",
    "            \"exclude_codes\": exclude_codes,\n",
    "            \"person_ids\": person_ids,\n",
    "            \"dates\": dates\n",
    "        }\n",
    "\n",
    "        # Generate query summary text\n",
    "        self.query_summary = self._generate_query_summary()\n",
    "\n",
    "        # Create Jinja2 environment and add filter\n",
    "        env = Environment()\n",
    "\n",
    "        # Define and add filter to environment\n",
    "        def join_quotes(items):\n",
    "            return ', '.join(f\"'{item}'\" for item in items)\n",
    "\n",
    "        env.filters['join_quotes'] = join_quotes\n",
    "\n",
    "        template = env.from_string(\"\"\"\n",
    "        WITH filtered_concepts AS (\n",
    "            SELECT DISTINCT concept_id, vocabulary_id, concept_code, concept_name\n",
    "            FROM {{ version }}.concept\n",
    "            WHERE \n",
    "                vocabulary_id IN ({{ vocabularies|join_quotes }})\n",
    "                {%- if exclude_terms %}\n",
    "                AND (\n",
    "                    {%- for term in exclude_terms %}\n",
    "                    {{ \"AND \" if not loop.first else \"\" }}LOWER(concept_name) NOT LIKE '%{{ term|lower }}%'\n",
    "                    {%- endfor %}\n",
    "                )\n",
    "                {%- endif %}\n",
    "                {%- for vocab, codes in exclude_codes.items() %}\n",
    "                {%- if codes %}\n",
    "                AND NOT (\n",
    "                    vocabulary_id = '{{ vocab }}' AND concept_code IN ({{ codes|join_quotes }})\n",
    "                )\n",
    "                {%- endif %}\n",
    "                {%- endfor %}\n",
    "                {%- if search_terms or exact_codes or pattern_codes %}\n",
    "                AND (\n",
    "                    {%- if search_terms %}\n",
    "                    (\n",
    "                        {%- for term in search_terms %}\n",
    "                        {{ \"OR \" if not loop.first else \"\" }}LOWER(concept_name) LIKE '%{{ term|lower }}%'\n",
    "                        {%- endfor %}\n",
    "                    )\n",
    "                    {%- endif %}\n",
    "\n",
    "                    {%- if search_terms and (exact_codes or pattern_codes) %}OR{% endif %}\n",
    "\n",
    "                    {%- if exact_codes %}\n",
    "                    (\n",
    "                        {%- for vocab, codes in exact_codes.items() %}\n",
    "                        {%- if codes %}\n",
    "                        {{ \"OR \" if not loop.first else \"\" }}(vocabulary_id = '{{ vocab }}' AND concept_code IN ({{ codes|join_quotes }}))\n",
    "                        {%- endif %}\n",
    "                        {%- endfor %}\n",
    "                    )\n",
    "                    {%- endif %}\n",
    "\n",
    "                    {%- if (search_terms or exact_codes) and pattern_codes %}OR{% endif %}\n",
    "\n",
    "                    {%- if pattern_codes %}\n",
    "                    (\n",
    "                        {%- for vocab, patterns in pattern_codes.items() %}\n",
    "                        {%- if patterns %}\n",
    "                        {{ \"OR \" if not loop.first else \"\" }}(vocabulary_id = '{{ vocab }}' AND (\n",
    "                            {%- for pattern in patterns %}\n",
    "                            {{ \"OR \" if not loop.first else \"\" }}concept_code LIKE '{{ pattern }}'\n",
    "                            {%- endfor %}\n",
    "                        ))\n",
    "                        {%- endif %}\n",
    "                        {%- endfor %}\n",
    "                    )\n",
    "                    {%- endif %}\n",
    "                )\n",
    "                {%- endif %}\n",
    "        ),\n",
    "        -- Regular events (non-V codes)\n",
    "        regular_events AS (\n",
    "            -- Get codes from condition_occurrence via source_value (non-V codes)\n",
    "            SELECT\n",
    "                co.person_id,\n",
    "                co.condition_source_value AS concept_code,\n",
    "                fc.vocabulary_id,\n",
    "                fc.concept_name,\n",
    "                'condition' AS domain,\n",
    "                co.condition_start_date AS event_date\n",
    "            FROM {{ version }}.condition_occurrence co\n",
    "            JOIN filtered_concepts fc \n",
    "                ON co.condition_source_value = fc.concept_code\n",
    "            WHERE NOT co.condition_source_value LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND co.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Get codes from condition_occurrence via source_concept_id (non-V codes)\n",
    "            SELECT\n",
    "                co.person_id,\n",
    "                fc.concept_code,\n",
    "                fc.vocabulary_id,\n",
    "                fc.concept_name,\n",
    "                'condition' AS domain,\n",
    "                co.condition_start_date AS event_date\n",
    "            FROM {{ version }}.condition_occurrence co\n",
    "            JOIN filtered_concepts fc \n",
    "                ON co.condition_source_concept_id = fc.concept_id\n",
    "            WHERE NOT fc.concept_code LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND co.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Get codes from observation via source_value (non-V codes)\n",
    "            SELECT\n",
    "                o.person_id,\n",
    "                o.observation_source_value AS concept_code,\n",
    "                fc.vocabulary_id,\n",
    "                fc.concept_name,\n",
    "                'observation' AS domain,\n",
    "                o.observation_date AS event_date\n",
    "            FROM {{ version }}.observation o\n",
    "            JOIN filtered_concepts fc \n",
    "                ON o.observation_source_value = fc.concept_code\n",
    "            WHERE NOT o.observation_source_value LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND o.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Get codes from observation via source_concept_id (non-V codes)\n",
    "            SELECT\n",
    "                o.person_id,\n",
    "                fc.concept_code,\n",
    "                fc.vocabulary_id,\n",
    "                fc.concept_name,\n",
    "                'observation' AS domain,\n",
    "                o.observation_date AS event_date\n",
    "            FROM {{ version }}.observation o\n",
    "            JOIN filtered_concepts fc \n",
    "                ON o.observation_source_concept_id = fc.concept_id\n",
    "            WHERE NOT fc.concept_code LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND o.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "        ),\n",
    "\n",
    "        -- V code events from condition_occurrence with special handling\n",
    "        v_events AS (\n",
    "            -- source_value path\n",
    "            SELECT\n",
    "                co.person_id,\n",
    "                co.condition_source_value AS concept_code,\n",
    "                co.condition_concept_id AS concept_id,\n",
    "                'condition' AS domain,\n",
    "                co.condition_start_date AS event_date\n",
    "            FROM {{ version }}.condition_occurrence co\n",
    "            JOIN filtered_concepts fc \n",
    "                ON co.condition_source_value = fc.concept_code\n",
    "            WHERE co.condition_source_value LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND co.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- source_concept_id path\n",
    "            SELECT\n",
    "                co.person_id,\n",
    "                fc.concept_code,\n",
    "                co.condition_concept_id AS concept_id,\n",
    "                'condition' AS domain,\n",
    "                co.condition_start_date AS event_date\n",
    "            FROM {{ version }}.condition_occurrence co\n",
    "            JOIN filtered_concepts fc \n",
    "                ON co.condition_source_concept_id = fc.concept_id\n",
    "            WHERE fc.concept_code LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND co.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- source_value path\n",
    "            SELECT\n",
    "                o.person_id,\n",
    "                o.observation_source_value AS concept_code,\n",
    "                o.observation_concept_id AS concept_id,\n",
    "                'observation' AS domain,\n",
    "                o.observation_date AS event_date\n",
    "            FROM {{ version }}.observation o\n",
    "            JOIN filtered_concepts fc \n",
    "                ON o.observation_source_value = fc.concept_code\n",
    "            WHERE o.observation_source_value LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND o.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- source_concept_id path\n",
    "            SELECT\n",
    "                o.person_id,\n",
    "                fc.concept_code,\n",
    "                o.observation_concept_id AS concept_id,\n",
    "                'observation' AS domain,\n",
    "                o.observation_date AS event_date\n",
    "            FROM {{ version }}.observation o\n",
    "            JOIN filtered_concepts fc \n",
    "                ON o.observation_source_concept_id = fc.concept_id\n",
    "            WHERE fc.concept_code LIKE 'V%'\n",
    "            {%- if person_ids %}\n",
    "            AND o.person_id IN ({{ person_ids }})\n",
    "            {%- endif %}\n",
    "        ),\n",
    "\n",
    "        -- Apply correct vocabulary attribution for V codes\n",
    "        v_events_corrected AS (\n",
    "            SELECT\n",
    "                v.person_id,\n",
    "                v.concept_code,\n",
    "                c.vocabulary_id,\n",
    "                c.concept_name,\n",
    "                v.domain,\n",
    "                v.event_date\n",
    "            FROM v_events v\n",
    "            JOIN {{ version }}.concept_relationship cr\n",
    "                ON v.concept_id = cr.concept_id_1\n",
    "            JOIN {{ version }}.concept c\n",
    "                ON cr.concept_id_2 = c.concept_id\n",
    "            WHERE c.vocabulary_id IN ({{ vocabularies|join_quotes }})\n",
    "            AND v.concept_code = c.concept_code\n",
    "        ),\n",
    "\n",
    "        -- Combine regular events with corrected V code events\n",
    "        all_events AS (\n",
    "            SELECT * FROM regular_events\n",
    "            UNION ALL\n",
    "            SELECT * FROM v_events_corrected\n",
    "        ),\n",
    "\n",
    "        -- Get distinct dates per person\n",
    "        distinct_dates AS (\n",
    "            SELECT \n",
    "                person_id,\n",
    "                vocabulary_id,\n",
    "                event_date\n",
    "            FROM all_events\n",
    "            GROUP BY person_id, vocabulary_id, event_date\n",
    "        ),\n",
    "\n",
    "        -- Calculate vocabulary summary per person\n",
    "        vocab_summary AS (\n",
    "            SELECT\n",
    "                person_id,\n",
    "                STRING_AGG(CASE \n",
    "                    WHEN vocabulary_id = 'ICD9CM' THEN '9'\n",
    "                    WHEN vocabulary_id = 'ICD10CM' THEN '10'\n",
    "                    WHEN vocabulary_id = 'SNOMED' THEN 'SNO'\n",
    "                    ELSE SUBSTR(vocabulary_id, 1, 3)\n",
    "                END, ' ' ORDER BY vocabulary_id) AS vocab\n",
    "            FROM (\n",
    "                SELECT DISTINCT person_id, vocabulary_id\n",
    "                FROM all_events\n",
    "            )\n",
    "            GROUP BY person_id\n",
    "        ),\n",
    "\n",
    "        -- Count distinct dates per person\n",
    "        date_counts AS (\n",
    "            SELECT\n",
    "                person_id,\n",
    "                COUNT(DISTINCT event_date) AS {{ name }}_n\n",
    "            FROM all_events\n",
    "            GROUP BY person_id\n",
    "        ){% if dates %},\n",
    "\n",
    "        -- Get ordered dates per person\n",
    "        ordered_dates AS (\n",
    "            SELECT\n",
    "                person_id,\n",
    "                event_date,\n",
    "                ROW_NUMBER() OVER (PARTITION BY person_id ORDER BY event_date ASC) AS date_order,\n",
    "                ROW_NUMBER() OVER (PARTITION BY person_id ORDER BY event_date DESC) AS rev_date_order\n",
    "            FROM (\n",
    "                SELECT DISTINCT person_id, event_date\n",
    "                FROM all_events\n",
    "            )\n",
    "        ),\n",
    "\n",
    "        -- Get first, second, and last dates\n",
    "        key_dates AS (\n",
    "            SELECT\n",
    "                person_id,\n",
    "                MAX(CASE WHEN date_order = 1 THEN event_date END) AS {{ name }}_1,\n",
    "                MAX(CASE WHEN date_order = 2 THEN event_date END) AS {{ name }}_2,\n",
    "                MAX(CASE WHEN rev_date_order = 1 THEN event_date END) AS {{ name }}_last\n",
    "            FROM ordered_dates\n",
    "            GROUP BY person_id\n",
    "        ){% endif %}\n",
    "\n",
    "        -- Final person-level output\n",
    "        SELECT\n",
    "            dc.person_id,\n",
    "            dc.{{ name }}_n,\n",
    "            vs.vocab{% if dates %},\n",
    "            kd.{{ name }}_1,\n",
    "            kd.{{ name }}_2,\n",
    "            kd.{{ name }}_last{% endif %}\n",
    "        FROM date_counts dc\n",
    "        JOIN vocab_summary vs ON dc.person_id = vs.person_id\n",
    "        {% if dates %}JOIN key_dates kd ON dc.person_id = kd.person_id{% endif %}\n",
    "        ORDER BY dc.person_id\n",
    "        \"\"\")       \n",
    "\n",
    "        # Format person_ids if it's a list\n",
    "        if isinstance(person_ids, list):\n",
    "            person_ids = ', '.join(str(pid) for pid in person_ids)\n",
    "\n",
    "        # Initialize dictionaries\n",
    "        exact_codes = exact_codes or {}\n",
    "        pattern_codes = pattern_codes or {}\n",
    "        exclude_codes = exclude_codes or {}\n",
    "\n",
    "        # Render the template\n",
    "        self.query_text = template.render(\n",
    "            version=self.version,\n",
    "            name=name,\n",
    "            vocabularies=vocabularies,\n",
    "            search_terms=search_terms,\n",
    "            exclude_terms=exclude_terms,\n",
    "            exact_codes=exact_codes,\n",
    "            pattern_codes=pattern_codes,\n",
    "            exclude_codes=exclude_codes,\n",
    "            person_ids=person_ids,\n",
    "            dates=dates\n",
    "        )\n",
    "\n",
    "        return self\n",
    " \n",
    "    def count_participants_with_data(self,\n",
    "                                     include_icd_codes: bool = True,\n",
    "                                     include_snomed_codes: bool = True,\n",
    "                                     include_loinc: bool = True,\n",
    "                                     include_drugs: bool = True,\n",
    "                                     measurement_registration_exclusion: bool = True,\n",
    "                                     custom_conditions: Dict[str, str] = None):\n",
    "        \"\"\"\n",
    "        Count unique persons with data in specified domains.\n",
    "\n",
    "        Args:\n",
    "            include_icd_codes: Include ICD9/ICD10 codes from condition_occurrence and observation source fields\n",
    "            include_snomed_codes: Include SNOMED codes from condition_occurrence and observation source fields\n",
    "            include_loinc: Include LOINC codes from measurement\n",
    "            include_drugs: Include drug exposures with domain_id = \"Drug\"\n",
    "            measurement_registration_exclusion: Exclude All of Us registration vitals from measurement count \n",
    "                (i.e. look for EHR measurements, not registration measurements)\n",
    "            custom_conditions: Dictionary of {table_name: WHERE clause} for custom filtering\n",
    "\n",
    "        Returns:\n",
    "            The query object, call execute_gbq() to run the query and get the count\n",
    "        \"\"\"\n",
    "        # Default vitals exclusions\n",
    "        default_vitals_exclusion = [3022318, 3027018, 3031203, 40759207, 40765148, 3036277, \n",
    "                                    3025315, 3012888, 3004249, 3038553, 3022281]\n",
    "        \n",
    "        # Set string for measurement exclusion\n",
    "        if measurement_registration_exclusion:\n",
    "            exclude_concept_ids_str = ', '.join(map(str, default_vitals_exclusion))\n",
    "            excluded_vitals = default_vitals_exclusion\n",
    "        else:\n",
    "            exclude_concept_ids_str = \"\"\n",
    "            excluded_vitals = []\n",
    "\n",
    "        custom_conditions = custom_conditions or {}\n",
    "\n",
    "        # Determine which vocabularies to include\n",
    "        condition_list = []\n",
    "        if include_icd_codes:\n",
    "            condition_list.extend(['ICD9CM', 'ICD10CM'])\n",
    "        if include_snomed_codes:\n",
    "            condition_list.append('SNOMED')\n",
    "\n",
    "        # Store query parameters for summary\n",
    "        self.last_query_params = {\n",
    "            \"type\": \"person_count\",\n",
    "            \"include_icd_codes\": include_icd_codes,\n",
    "            \"include_snomed_codes\": include_snomed_codes,\n",
    "            \"include_loinc\": include_loinc,\n",
    "            \"include_drugs\": include_drugs,\n",
    "            \"excluded_vitals\": excluded_vitals\n",
    "        }\n",
    "\n",
    "        # Generate query summary text\n",
    "        self.query_summary = self._generate_query_summary()\n",
    "\n",
    "        # Create Jinja2 environment\n",
    "        env = Environment()\n",
    "\n",
    "        # Define and add filter to environment\n",
    "        def join_quotes(items):\n",
    "            return ', '.join(f\"'{item}'\" for item in items)\n",
    "\n",
    "        env.filters['join_quotes'] = join_quotes\n",
    "       \n",
    "        template = env.from_string(\"\"\"\n",
    "        WITH combined AS (\n",
    "            {% if condition_list %}\n",
    "            -- Codes from observation (source_value)\n",
    "            SELECT o.person_id\n",
    "            FROM {{ version }}.observation AS o\n",
    "            JOIN {{ version }}.concept AS c ON o.observation_source_value = c.concept_code\n",
    "            WHERE c.vocabulary_id IN ({{ condition_list|join_quotes }})\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Codes from observation (source_concept_id)\n",
    "            SELECT o.person_id\n",
    "            FROM {{ version }}.observation AS o\n",
    "            JOIN {{ version }}.concept AS c ON o.observation_source_concept_id = c.concept_id\n",
    "            WHERE c.vocabulary_id IN ({{ condition_list|join_quotes }})\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Codes from condition_occurrence (source_value)\n",
    "            SELECT co.person_id\n",
    "            FROM {{ version }}.condition_occurrence AS co\n",
    "            JOIN {{ version }}.concept AS c ON co.condition_source_value = c.concept_code\n",
    "            WHERE c.vocabulary_id IN ({{ condition_list|join_quotes }})\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- Codes from condition_occurrence (source_concept_id)\n",
    "            SELECT co.person_id\n",
    "            FROM {{ version }}.condition_occurrence AS co\n",
    "            JOIN {{ version }}.concept AS c ON co.condition_source_concept_id = c.concept_id\n",
    "            WHERE c.vocabulary_id IN ({{ condition_list|join_quotes }})\n",
    "            {% endif %}\n",
    "\n",
    "            {% if include_loinc %}\n",
    "            {% if condition_list %}UNION ALL{% endif %}\n",
    "            -- LOINC codes from measurement\n",
    "            SELECT m.person_id\n",
    "            FROM {{ version }}.measurement AS m\n",
    "            JOIN {{ version }}.concept AS c ON m.measurement_concept_id = c.concept_id\n",
    "            WHERE c.vocabulary_id = 'LOINC'\n",
    "            {% if measurement_registration_exclusion %}\n",
    "                AND c.concept_id NOT IN ({{ exclude_concept_ids_str }})\n",
    "            {% endif %}\n",
    "            {% endif %}\n",
    "    \n",
    "            {% if include_drugs %}\n",
    "            {% if condition_list or include_loinc %}UNION ALL{% endif %}\n",
    "            -- Drug exposures\n",
    "            SELECT de.person_id\n",
    "            FROM {{ version }}.drug_exposure AS de\n",
    "            JOIN {{ version }}.concept AS c ON de.drug_concept_id = c.concept_id\n",
    "            WHERE c.domain_id = 'Drug'\n",
    "            {% endif %}\n",
    "        )\n",
    "\n",
    "        SELECT COUNT(DISTINCT person_id) AS person_count\n",
    "        FROM combined\n",
    "        \"\"\")\n",
    "\n",
    "        # Render the template\n",
    "        self.query_text = template.render(\n",
    "            version=self.version,\n",
    "            condition_list=condition_list,\n",
    "            include_icd_codes=include_icd_codes,\n",
    "            include_snomed_codes=include_snomed_codes,\n",
    "            include_loinc=include_loinc,\n",
    "            include_drugs=include_drugs,\n",
    "            measurement_registration_exclusion=measurement_registration_exclusion,\n",
    "            exclude_concept_ids_str=exclude_concept_ids_str,\n",
    "            custom_conditions=custom_conditions\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def print_query(self):\n",
    "        \"\"\"Print the current query for inspection\"\"\"\n",
    "        if not self.query_text:\n",
    "            print(\"No query built yet\")\n",
    "            return self\n",
    "            \n",
    "        print(self.query_text)\n",
    "        return self\n",
    "\n",
    "    def query_to_variable(self, var_name):\n",
    "        \"\"\"Set a variable to the current query text\"\"\"\n",
    "        if not self.query_text:\n",
    "            raise ValueError(\"No query has been built yet\")\n",
    "        \n",
    "        globals()[var_name] = self.query_text\n",
    "        print(f\"Query saved to variable '{var_name}'\")\n",
    "        return self\n",
    "\n",
    "    def _generate_query_summary(self):\n",
    "        \"\"\"Generate a human-readable summary of the last query parameters\"\"\"\n",
    "        if not hasattr(self, 'last_query_params'):\n",
    "            return \"No query parameters available\"\n",
    "\n",
    "        params = self.last_query_params\n",
    "        query_type = params.get(\"type\", \"unknown\")\n",
    "\n",
    "        if query_type == \"diagnosis_codes\":\n",
    "            summary = []\n",
    "\n",
    "            # Describe data sources\n",
    "            summary.append(f\"Query Type: Diagnosis Codes\")\n",
    "\n",
    "            # Describe vocabularies\n",
    "            vocabs = params.get(\"vocabularies\")\n",
    "            if vocabs:\n",
    "                summary.append(f\"Vocabularies: {', '.join(vocabs)}\")\n",
    "\n",
    "            # Describe tables\n",
    "            summary.append(\"Tables: condition_occurrence, observation\")\n",
    "            summary.append(\"Fields: Using both source_value and source_concept_id paths\")\n",
    "\n",
    "            # Describe search criteria\n",
    "            search_terms = params.get(\"search_terms\")\n",
    "            if search_terms:\n",
    "                terms_with_quotes = [f'\"{term}\"' for term in search_terms]\n",
    "                terms_joined = ', '.join(terms_with_quotes)\n",
    "                summary.append(f\"Search Terms: {terms_joined}\")\n",
    "\n",
    "            exclude_terms = params.get(\"exclude_terms\")\n",
    "            if exclude_terms:\n",
    "                terms_with_quotes = [f'\"{term}\"' for term in exclude_terms]\n",
    "                terms_joined = ', '.join(terms_with_quotes)\n",
    "                summary.append(f\"Exclude Terms: {terms_joined}\")\n",
    "\n",
    "            # Describe code criteria\n",
    "            exact_codes = params.get(\"exact_codes\")\n",
    "            if exact_codes:\n",
    "                code_lists = []\n",
    "                for vocab, codes in exact_codes.items():\n",
    "                    if codes:\n",
    "                        sorted_codes = sorted(codes)  \n",
    "                        code_lists.append(f\"\\n  {vocab}: {', '.join(sorted_codes)}\")\n",
    "                if code_lists:\n",
    "                    summary.append(f\"Exact Codes: {' '.join(code_lists)}\")\n",
    "\n",
    "            pattern_codes = params.get(\"pattern_codes\")\n",
    "            if pattern_codes:\n",
    "                pattern_lists = []\n",
    "                for vocab, patterns in pattern_codes.items():\n",
    "                    if patterns:\n",
    "                        sorted_patterns = sorted(patterns)\n",
    "                        pattern_lists.append(f\"\\n  {vocab}: {', '.join(sorted_patterns)}\")\n",
    "                if pattern_lists:\n",
    "                    summary.append(f\"Pattern Codes: {' '.join(pattern_lists)}\")\n",
    "\n",
    "            exclude_codes = params.get(\"exclude_codes\")\n",
    "            if exclude_codes:\n",
    "                exclude_lists = []\n",
    "                for vocab, codes in exclude_codes.items():\n",
    "                    if codes:\n",
    "                        sorted_codes = sorted(codes) \n",
    "                        exclude_lists.append(f\"\\n  {vocab}: {', '.join(sorted_codes)}\")\n",
    "                if exclude_lists:\n",
    "                    summary.append(f\"Exclude Codes: {' '.join(exclude_lists)}\")\n",
    "\n",
    "            # Describe person filtering\n",
    "            person_ids = params.get(\"person_ids\")\n",
    "            if person_ids:\n",
    "                if isinstance(person_ids, list):\n",
    "                    summary.append(f\"Filtered to {len(person_ids)} specific person IDs\")\n",
    "                else:\n",
    "                    summary.append(f\"Filtered to specific person IDs\")\n",
    "\n",
    "            # V code handling\n",
    "            summary.append(\"Special handling for V codes to ensure correct vocabulary attribution\")\n",
    "\n",
    "            return \"\\n\".join(summary)\n",
    "        \n",
    "        elif query_type == \"person_diagnosis\":\n",
    "\n",
    "            summary = []\n",
    "\n",
    "            # Describe data sources\n",
    "            summary.append(f\"Query Type: Person-Level Diagnosis Data\")\n",
    "            summary.append(f\"Column Name Prefix: {params.get('name', 'unknown')}\")\n",
    "\n",
    "            # Describe vocabularies\n",
    "            vocabs = params.get(\"vocabularies\")\n",
    "            if vocabs:\n",
    "                summary.append(f\"Vocabularies: {', '.join(vocabs)}\")\n",
    "\n",
    "            # Describe tables\n",
    "            summary.append(\"Tables: condition_occurrence, observation\")\n",
    "            summary.append(\"Fields: Using both source_value and source_concept_id paths\")\n",
    "\n",
    "            # Describe search criteria\n",
    "            search_terms = params.get(\"search_terms\")\n",
    "            if search_terms:\n",
    "                terms_with_quotes = [f'\"{term}\"' for term in search_terms]\n",
    "                terms_joined = ', '.join(terms_with_quotes)\n",
    "                summary.append(f\"Search Terms: {terms_joined}\")\n",
    "\n",
    "            exclude_terms = params.get(\"exclude_terms\")\n",
    "            if exclude_terms:\n",
    "                terms_with_quotes = [f'\"{term}\"' for term in exclude_terms]\n",
    "                terms_joined = ', '.join(terms_with_quotes)\n",
    "                summary.append(f\"Exclude Terms: {terms_joined}\")\n",
    "\n",
    "            # Describe code criteria\n",
    "            exact_codes = params.get(\"exact_codes\")\n",
    "            if exact_codes:\n",
    "                code_lists = []\n",
    "                for vocab, codes in exact_codes.items():\n",
    "                    if codes:\n",
    "                        sorted_codes = sorted(codes)  \n",
    "                        code_lists.append(f\"\\n  {vocab}: {', '.join(sorted_codes)}\")\n",
    "                if code_lists:\n",
    "                    summary.append(f\"Exact Codes: {' '.join(code_lists)}\")\n",
    "\n",
    "            pattern_codes = params.get(\"pattern_codes\")\n",
    "            if pattern_codes:\n",
    "                pattern_lists = []\n",
    "                for vocab, patterns in pattern_codes.items():\n",
    "                    if patterns:\n",
    "                        sorted_patterns = sorted(patterns)\n",
    "                        pattern_lists.append(f\"\\n  {vocab}: {', '.join(sorted_patterns)}\")\n",
    "                if pattern_lists:\n",
    "                    summary.append(f\"Pattern Codes: {' '.join(pattern_lists)}\")\n",
    "\n",
    "            exclude_codes = params.get(\"exclude_codes\")\n",
    "            if exclude_codes:\n",
    "                exclude_lists = []\n",
    "                for vocab, codes in exclude_codes.items():\n",
    "                    if codes:\n",
    "                        sorted_codes = sorted(codes) \n",
    "                        exclude_lists.append(f\"\\n  {vocab}: {', '.join(sorted_codes)}\")\n",
    "                if exclude_lists:\n",
    "                    summary.append(f\"Exclude Codes: {' '.join(exclude_lists)}\")\n",
    "\n",
    "            # Describe person filtering\n",
    "            person_ids = params.get(\"person_ids\")\n",
    "            if person_ids:\n",
    "                if isinstance(person_ids, list):\n",
    "                    summary.append(f\"Filtered to {len(person_ids)} specific person IDs\")\n",
    "                else:\n",
    "                    summary.append(f\"Filtered to specific person IDs\")\n",
    "\n",
    "            # Date columns\n",
    "            dates = params.get(\"dates\")\n",
    "            if dates:\n",
    "                summary.append(f\"Including date columns: first, second, and last occurrences\")\n",
    "\n",
    "            # V code handling\n",
    "            summary.append(\"Special handling for V codes to ensure correct vocabulary attribution\")\n",
    "\n",
    "            return \"\\n\".join(summary)\n",
    "        \n",
    "        elif query_type == \"person_count\":\n",
    "            \n",
    "            summary = []\n",
    "\n",
    "            # Describe query type\n",
    "            summary.append(\"Query Type: Participant Count\")\n",
    "\n",
    "            # Describe included data types\n",
    "            data_types = []\n",
    "            if params.get(\"include_icd_codes\"):\n",
    "                data_types.append(\"ICD codes (ICD9CM, ICD10CM)\")\n",
    "            if params.get(\"include_snomed_codes\"):\n",
    "                data_types.append(\"SNOMED codes\")\n",
    "            if params.get(\"include_loinc\"):\n",
    "                data_types.append(\"LOINC measurements\")\n",
    "            if params.get(\"include_drugs\"):\n",
    "                data_types.append(\"Drug exposures\")\n",
    "\n",
    "            if data_types:\n",
    "                summary.append(f\"Included Data: {', '.join(data_types)}\")\n",
    "\n",
    "            # Describe excluded concepts\n",
    "            excluded_vitals = params.get(\"excluded_vitals\", [])\n",
    "\n",
    "            if params.get(\"include_loinc\") and excluded_vitals:\n",
    "                summary.append(f\"{len(excluded_vitals)} common vitals concept IDs (from All of Us registration) excluded from measurement\")\n",
    "\n",
    "            return \"\\n\".join(summary)\n",
    "            \n",
    "        else:\n",
    "            return \"Unknown query type\"\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print a summary of the last query\"\"\"\n",
    "        if hasattr(self, 'query_summary'):\n",
    "            print(self.query_summary)\n",
    "        else:\n",
    "            print(\"No query summary available\")\n",
    "        return self\n",
    "\n",
    "    def execute_gbq(self, quiet=False):\n",
    "        \"\"\"Execute the query and return a Polars DataFrame\"\"\"\n",
    "        if not self.query_text:\n",
    "            raise ValueError(\"No query has been built yet\")\n",
    "                \n",
    "        if not quiet and hasattr(self, 'query_summary'):\n",
    "            print(self.query_summary)\n",
    "\n",
    "        # Execute query\n",
    "        query_job = self.client.query(self.query_text)\n",
    "\n",
    "        # Get results and convert to polars\n",
    "        rows = query_job.result()\n",
    "        df = pl.from_arrow(rows.to_arrow())\n",
    "\n",
    "        return df\n",
    "\n",
    "    def results_to_code_dict(self, df):\n",
    "        \"\"\"\n",
    "        Convert a diagnosis codes dataframe to an exact_codes dictionary for use in find_diagnosis_codes.\n",
    "\n",
    "        Args:\n",
    "            df: Polars DataFrame with vocabulary_id and concept_code columns\n",
    "\n",
    "        Returns:\n",
    "            Dict mapping vocabularies to lists of concept codes\n",
    "        \"\"\"\n",
    "        if not {'vocabulary_id', 'concept_code'}.issubset(df.columns):\n",
    "            raise ValueError(\"DataFrame must contain 'vocabulary_id' and 'concept_code' columns\")\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        # Group by vocabulary_id and collect concept_codes\n",
    "        for vocab in df['vocabulary_id'].unique():\n",
    "            codes = df.filter(pl.col('vocabulary_id') == vocab)['concept_code'].to_list()\n",
    "            result[vocab] = codes\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_icd_dict_from_phecodes(phecodex_map, phecodes):\n",
    "    # Convert single phecode to list if needed\n",
    "    if isinstance(phecodes, str):\n",
    "        phecodes = [phecodes]\n",
    "    \n",
    "    # Filter the DataFrame to only include rows with phecodes in the list\n",
    "    filtered_df = phecodex_map.filter(pl.col(\"phecode\").is_in(phecodes))\n",
    "    \n",
    "    # Group by vocabulary_id and aggregate unique ICD codes into lists\n",
    "    result = (filtered_df\n",
    "              .group_by(\"vocabulary_id\")\n",
    "              .agg(pl.col(\"ICD\").unique().alias(\"icd_codes\"))\n",
    "              .to_dict(as_series=False))\n",
    "    \n",
    "    # Convert to the desired dictionary format\n",
    "    icd_dict = {vocab: codes for vocab, codes in zip(result[\"vocabulary_id\"], result[\"icd_codes\"])}\n",
    "    \n",
    "    return icd_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Homozygote/Heterozygote Counts (Pathfinder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Files for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # copy csv file to the bucket\n",
    "# args = [\"gsutil\", \"cp\", f\"./GPR15_variants_v0.csv\", f\"{bucket}/gpr15/variants/\"]\n",
    "# output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# # print output from gsutil\n",
    "# output.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List objects in the bucket\n",
    "# print(subprocess.check_output(f\"gsutil -u $GOOGLE_PROJECT ls -r gs://fc-aou-datasets-controlled/v8\", shell=True).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup/initialize the program\n",
    "initialize()\n",
    "\n",
    "#exome matrix table: contains all participants, all variants in exonic regions (+ 15bp into the introns)\n",
    "exome_mt = hl.read_matrix_table(\"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/exome/splitMT/hail.mt\")\n",
    "\n",
    "#variant annotation table\n",
    "vat_8_ht = hl.read_table('{bucket or my_bucket}/wgs_v8/vat_v8_all.ht')\n",
    "\n",
    "# df of variants with vid\n",
    "uploaded_var_pd = pd.read_csv(f'{bucket}/gpr15/variants/GPR15_variants_v0.csv', sep = ',')\n",
    "\n",
    "# add hail_id\n",
    "uploaded_var_pd['hail_id'] = 'chr' + uploaded_var_pd['vid'].str.replace('-', ':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define gene interval for GPR15\n",
    "gene_interval = ['chr3:98531978-98534681']\n",
    "\n",
    "#define cut-off for minor allele frequency (MAF) in gnomad (all populations) \n",
    "MAF_freq = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_exome = filter_exome(exome_mt, gene_interval)\n",
    "\n",
    "# mt with processed variants\n",
    "processed_variants = process_variants(uploaded_var_pd)\n",
    "\n",
    "# mt with filtered variants and all participants\n",
    "annotated_exome = annotate_exome(vat_8_ht, gene_interval, processed_variants, filtered_exome)\n",
    "\n",
    "# pLOF_mt is a matrix table containing only variants from the pLOF stream\n",
    "pLOF_mt = pLOF_variants(annotated_exome)\n",
    "\n",
    "# matrix tables containing specific variants\n",
    "clinvar_mt = clinvar_variants(annotated_exome)\n",
    "lab_mt = lab_variants(annotated_exome)\n",
    "gnomad_plof_variants_mt = gnomad_plof_variants(annotated_exome)\n",
    "gnomad_other_variants_mt = gnomad_other_variants(annotated_exome)\n",
    "gnomad_missense_variants_mt = gnomad_missense_variants(annotated_exome)\n",
    "\n",
    "# components you want to include in the filtering process\n",
    "components = [clinvar_mt, lab_mt, gnomad_plof_variants_mt, gnomad_other_variants_mt, gnomad_missense_variants_mt, pLOF_mt]\n",
    "\n",
    "# matrix table containing only variants of interest (as specified in \"components\")\n",
    "interested_exome = combine_components(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_exome.filter_rows(hl.is_defined(annotated_exome.annotations.uploaded_var)).rows().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate participant lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the slow step, takes about 5 min for PKD1\n",
    "# filters to only include participants with 1 or more variant of interest\n",
    "# converts hail mt to ht; then, converts ht to pandas df\n",
    "\n",
    "entries_pd = find_monallelic_participants(interested_exome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Find people who appear exactly twice in the original data\n",
    "duplicate_people = entries_pd['s'].value_counts()\n",
    "people_appearing_twice = duplicate_people[duplicate_people == 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_pd[entries_pd['s'].isin(people_appearing_twice)].sort_values('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_pd.to_csv(f'{bucket}/data/gpr15/cohorts/pathfinder_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create summary by vid\n",
    "summary_df = entries_pd.groupby('vid').agg({\n",
    "    'locus': 'first',\n",
    "    'aa_change': 'first', \n",
    "    'dna_change_in_transcript': 'first',\n",
    "    'rsid': 'first',\n",
    "    'pLOF': 'first',\n",
    "    'lab': 'first',\n",
    "    's': lambda x: (entries_pd.loc[x.index, 'n_alt_agg'] == 1).sum(),  # het count\n",
    "}).rename(columns={'s': 'het_count'})\n",
    "\n",
    "# Add homozygote counts\n",
    "summary_df['hom_count'] = entries_pd.groupby('vid').apply(\n",
    "    lambda x: (x['n_alt_agg'] == 2).sum()\n",
    ").values\n",
    "\n",
    "summary_df = summary_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv(f'{bucket}/data/gpr15/cohorts/pathfinder_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upsetplot import plot\n",
    "from upsetplot import from_memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean columns for each annotation \n",
    "annotation_cols = ['pLOF', 'lab', 'clinvar', 'gnomad_plof', 'gnomad_other', 'gnomad_missense']\n",
    "\n",
    "# Plot 1: All variants\n",
    "data_all = entries_pd[annotation_cols].astype(bool).reset_index(drop=True)\n",
    "result_list = [\n",
    "    list(data_all.columns[row].to_list()) \n",
    "    for row in data_all.values.astype(bool)\n",
    "]\n",
    "\n",
    "# Create the UpSet plot data\n",
    "data = from_memberships(result_list)\n",
    "\n",
    "# Create the UpSet plot\n",
    "plot(data, subset_size=\"count\", sort_by='cardinality', sort_categories_by='-cardinality', show_counts=True)\n",
    "plt.title(f'UpSet Plot of Variant Annotations: All Participant Variants ({len(entries_pd)})')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Unique variants\n",
    "# Create composite key for unique variants\n",
    "# Get unique variants\n",
    "entries_pd_unique = entries_pd.drop_duplicates(subset=['dna_change_in_transcript'])\n",
    "data_all_unique = entries_pd_unique[annotation_cols].astype(bool).reset_index(drop=True)\n",
    "result_list_unique = [\n",
    "    list(data_all_unique.columns[row].to_list()) \n",
    "    for row in data_all_unique.values.astype(bool)\n",
    "]\n",
    "\n",
    "# Create the UpSet plot data\n",
    "data_unique = from_memberships(result_list_unique)\n",
    "\n",
    "# Create the UpSet plot\n",
    "plot(data_unique, subset_size=\"count\", sort_by='cardinality', sort_categories_by='-cardinality', show_counts=True)\n",
    "plt.title(f'UpSet Plot of Variant Annotations: Unique Variants ({len(entries_pd_unique)})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyr132ser_pathfinder_df = entries_pd[entries_pd['vid']=='3-98532428-A-C']\n",
    "tyr215ter_pathfinder_df = entries_pd[entries_pd['vid']=='3-98532678-C-G']\n",
    "asp306asn_pathfinder_df = entries_pd[entries_pd['vid']=='3-98532949-G-A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Cohort (PheTK) - check related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PheTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PheTK.Cohort import Cohort\n",
    "from PheTK.Phecode import Phecode\n",
    "from PheTK.PheWAS import PheWAS\n",
    "from PheTK.Plot import Plot\n",
    "import PheTK._queries as _queries\n",
    "from PheTK import _utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate class Cohort object for _All of Us_\n",
    "cohort = Cohort(platform=\"aou\", aou_db_version=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one variant above to create the entire AoU cohort, with case == 1 set for that variant\n",
    "cohort.by_genotype(\n",
    "    chromosome_number=3,\n",
    "    genomic_position=98532428,\n",
    "    ref_allele=\"A\",\n",
    "    alt_allele=\"C\",\n",
    "    case_gt=\"1/1\",\n",
    "    control_gt=\"0/0\",\n",
    "    reference_genome=\"GRCh38\",\n",
    "    mt_path=\"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/exome/multiMT/hail.mt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one variant above to create the entire AoU cohort, with case == 1 set for that variant\n",
    "cohort.by_genotype(\n",
    "    chromosome_number=3,\n",
    "    genomic_position=98532678,\n",
    "    ref_allele=\"C\",\n",
    "    alt_allele=\"G\",\n",
    "    case_gt=\"0/1\",\n",
    "    control_gt=\"0/0\",\n",
    "    reference_genome=\"GRCh38\",\n",
    "    mt_path=\"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/exome/multiMT/hail.mt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one variant above to create the entire AoU cohort, with case == 1 set for that variant\n",
    "cohort.by_genotype(\n",
    "    chromosome_number=3,\n",
    "    genomic_position=98532949,\n",
    "    ref_allele=\"G\",\n",
    "    alt_allele=\"A\",\n",
    "    case_gt=\"0/1\",\n",
    "    control_gt=\"0/0\",\n",
    "    reference_genome=\"GRCh38\",\n",
    "    mt_path=\"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/exome/multiMT/hail.mt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy csv file to the bucket\n",
    "# args = [\"gsutil\", \"cp\", f\"./aou_chr3_98532428_A_C.csv\", f\"{bucket}/data/gpr15/cohorts/\"]\n",
    "# output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# # print output from gsutil\n",
    "# output.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy csv file to the bucket\n",
    "# args = [\"gsutil\", \"cp\", f\"./aou_chr3_98532678_C_G.csv\", f\"{bucket}/data/gpr15/cohorts/\"]\n",
    "# output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# # print output from gsutil\n",
    "# output.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy csv file to the bucket\n",
    "# args = [\"gsutil\", \"cp\", f\"./aou_chr3_98532949_G_A.csv\", f\"{bucket}/data/gpr15/cohorts/\"]\n",
    "# output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# # print output from gsutil\n",
    "# output.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyr132ser_df = pd.read_csv(f'{bucket}/data/gpr15/cohorts/aou_chr3_98532428_A_C.csv')\n",
    "tyr215ter_df = pd.read_csv(f'{bucket}/data/gpr15/cohorts/aou_chr3_98532678_C_G.csv')\n",
    "asp306asn_df = pd.read_csv(f'{bucket}/data/gpr15/cohorts/aou_chr3_98532949_G_A.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Pathfinder and PheTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyr132ser_pathfinder_df['s'] = tyr132ser_pathfinder_df['s'].astype(str)\n",
    "tyr132ser_df['person_id'] = tyr132ser_df['person_id'].astype(str)\n",
    "\n",
    "tyr132ser_pathfinder_df = tyr132ser_pathfinder_df.merge(\n",
    "    tyr132ser_df,\n",
    "    left_on='s',\n",
    "    right_on='person_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tyr132ser_pathfinder_df[tyr132ser_pathfinder_df['n_alt_agg']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct filtering and selection\n",
    "genotype_info = annotated_exome.filter_entries(\n",
    "    (annotated_exome.s == \"1591765\") & \n",
    "    (annotated_exome.locus == hl.parse_locus(\"chr3:98532428\", reference_genome='GRCh38'))\n",
    ").select_entries('GT', 'n_alt').entries().collect()\n",
    "\n",
    "print(\"Genotype info:\")\n",
    "for entry in genotype_info:\n",
    "    print(f\"Locus: {entry.locus}\")\n",
    "    print(f\"Alleles: {entry.alleles}\")\n",
    "    print(f\"Genotype: {entry.GT}\")\n",
    "    print(f\"n_alt: {entry.n_alt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct filtering and selection\n",
    "genotype_info = annotated_exome.filter_entries(\n",
    "    (annotated_exome.s == \"1639589\") & \n",
    "    (annotated_exome.locus == hl.parse_locus(\"chr3:98532428\", reference_genome='GRCh38'))\n",
    ").select_entries('GT', 'n_alt').entries().collect()\n",
    "\n",
    "print(\"Genotype info:\")\n",
    "for entry in genotype_info:\n",
    "    print(f\"Locus: {entry.locus}\")\n",
    "    print(f\"Alleles: {entry.alleles}\")\n",
    "    print(f\"Genotype: {entry.GT}\")\n",
    "    print(f\"n_alt: {entry.n_alt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyr215ter_pathfinder_df['s'] = tyr215ter_pathfinder_df['s'].astype(str)\n",
    "tyr215ter_df['person_id'] = tyr215ter_df['person_id'].astype(str)\n",
    "\n",
    "tyr215ter_pathfinder_df = tyr215ter_pathfinder_df.merge(\n",
    "    tyr215ter_df,\n",
    "    left_on='s',\n",
    "    right_on='person_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tyr215ter_pathfinder_df[tyr215ter_pathfinder_df['n_alt_agg']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asp306asn_pathfinder_df['s'] = asp306asn_pathfinder_df['s'].astype(str)\n",
    "asp306asn_df['person_id'] = asp306asn_df['person_id'].astype(str)\n",
    "\n",
    "asp306asn_df = asp306asn_df.merge(\n",
    "    asp306asn_pathfinder_df,\n",
    "    right_on='s',\n",
    "    left_on='person_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asp306asn_df[(asp306asn_df['case']==1) & (asp306asn_df['n_alt_agg']!=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct filtering and selection\n",
    "genotype_info = annotated_exome.filter_entries(\n",
    "    (annotated_exome.s == \"1639589\") & \n",
    "    (annotated_exome.locus == hl.parse_locus(\"chr3:98532949\", reference_genome='GRCh38'))\n",
    ").select_entries('GT', 'n_alt').entries().collect()\n",
    "\n",
    "print(\"Genotype info:\")\n",
    "for entry in genotype_info:\n",
    "    print(f\"Locus: {entry.locus}\")\n",
    "    print(f\"Alleles: {entry.alleles}\")\n",
    "    print(f\"Genotype: {entry.GT}\")\n",
    "    print(f\"n_alt: {entry.n_alt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add covariate data\n",
    "Covariate descriptions:<br>\n",
    "\n",
    "natural_age: current age or age at death<br>\n",
    "age_at_last_event: age at last diagnosis event (ICD or SNOMED) in EHR.<br>\n",
    "sex_at_birth: sex at birth<br>\n",
    "ehr_length: EHR duration, in year, from first to last diagnosis code<br>\n",
    "dx_code_occurrence_count: counts the occurrences of diagnosis codes throughout EHR of each participant. For example: person 1 having R50 (fever) code on 5 different dates, R05 (cough) code on 3 different dates, and R05.1 (acute cough) code on 2 different dates, will have a dx_code_occurrence_count = 10.<br>\n",
    "dx_condition_count: counts the number of unique conditions occurred throughout EHR of each participant. For example, for the same person 1 above, the dx_condition_count = 3 (R05 - cough, R05.1 - acute cough, R50 - fever).<br>\n",
    "genetic_ancestry: returns string values of predicted ancestries, e.g., \"eur\", \"afr\", etc. These are only useful if user would like to filter data by genetic ancestries.<br>\n",
    "first_n_pcs: retrieves first n genetic PC components from genetic PCA data generated by All of Us.<br>\n",
    "drop_nulls: remove rows containing null values in any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cohort.add_covariates(\n",
    "    cohort_csv_path=f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532428_A_C.csv\",\n",
    "    natural_age=False,\n",
    "    age_at_last_event=True,\n",
    "    sex_at_birth=True,\n",
    "    ehr_length=True,\n",
    "    dx_code_occurrence_count=False,\n",
    "    dx_condition_count=False,\n",
    "    genetic_ancestry=True,\n",
    "    first_n_pcs=10,\n",
    "    drop_nulls=True,\n",
    "    output_file_name=f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532428_A_C_with_covariates.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cohort.add_covariates(\n",
    "    cohort_csv_path=f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532678_C_G.csv\",\n",
    "    natural_age=False,\n",
    "    age_at_last_event=True,\n",
    "    sex_at_birth=True,\n",
    "    ehr_length=True,\n",
    "    dx_code_occurrence_count=False,\n",
    "    dx_condition_count=False,\n",
    "    genetic_ancestry=True,\n",
    "    first_n_pcs=10,\n",
    "    drop_nulls=True,\n",
    "    output_file_name=f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532678_C_G_with_covariates.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cohort.add_covariates(\n",
    "    cohort_csv_path=f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532949_G_A.csv\",\n",
    "    natural_age=False,\n",
    "    age_at_last_event=True,\n",
    "    sex_at_birth=True,\n",
    "    ehr_length=True,\n",
    "    dx_code_occurrence_count=False,\n",
    "    dx_condition_count=False,\n",
    "    genetic_ancestry=True,\n",
    "    first_n_pcs=10,\n",
    "    drop_nulls=True,\n",
    "    output_file_name=f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532949_G_A_with_covariates.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add IBD data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phecodex_map = pl.read_csv('phecodeX_unrolled_ICD_CM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "aou = AouQueries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBD by AoUQueries (ICD and SNOMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find IBD diagnoses by string\n",
    "gi_inflammation_codes_string_df = (aou.find_diagnosis_codes(\n",
    "    vocabularies=[\"ICD9CM\", \"ICD10CM\", \"SNOMED\"],\n",
    "    search_terms=[\"ulcerative colitis\", \"crohn\", 'ulcerative (chronic) pancolitis',\n",
    "                  'ulcerative (chronic) rectosigmoiditis', 'ulcerative (chronic) proctitis', \n",
    "                  'left sided colitis', 'regional enteritis', 'ulcerative (chronic) enterocolitis',\n",
    "                  'ulcerative (chronic) colitis', 'ulcerative (chronic) proctosigmoiditis', \n",
    "                  'ulcerative (chronic) ileocolitis', 'pseudopolyposis', 'inflammatory polyps of colon',\n",
    "                  'lymphocytic colitis', 'collagenous colitis', 'microscopic colitis', 'enteritis and colitis',\n",
    "                  'protein-induced enteropathy', 'protein-induced enterocolitis', 'eosinophilic esophagitis',\n",
    "                  'eosinophilic gastr', 'eosinophilic colitis', 'ulceration of intestine', 'ulcer of anus', \n",
    "                  'ulcer of intestine', 'ulcer of anus', 'duodenitis', 'gastric mucosal hypertrophy', \n",
    "                  'specified gastritis', 'alcoholic gastritis', 'gastritis, unspecified', 'other gastritis',\n",
    "                  'chronic gastritis', 'superficial gastritis', 'acute gastritis', 'atrophic gastritis', \n",
    "                  'noninfective gastroenteritis', 'indeterminate colitis'\n",
    "                 ],\n",
    "    exclude_terms=['fh:', 'h/o:', 'radiation', 'infectious gastroenteritis and colitis, unspecified', \n",
    "                  'helicobacter', 'history of']\n",
    ").execute_gbq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi_inflammation_dict = create_icd_dict_from_phecodes(phecodex_map, ['GI_522'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find IBD diagnoses by phecodes\n",
    "gi_inflammation_codes_phecode_df = (aou.find_diagnosis_codes(\n",
    "    exact_codes=gi_inflammation_dict\n",
    ").execute_gbq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined dataframe with source labels\n",
    "phecode_df = gi_inflammation_codes_phecode_df.unique(subset=[\"vocabulary_id\", \"concept_code\"]).with_columns(pl.lit(\"phecode\").alias(\"source\"))\n",
    "string_df = gi_inflammation_codes_string_df.unique(subset=[\"vocabulary_id\", \"concept_code\"]).with_columns(pl.lit(\"string\").alias(\"source\"))\n",
    "\n",
    "# Find rows in phecode_df not in string_df\n",
    "unique_to_phecode = phecode_df.join(\n",
    "    string_df.select([\"vocabulary_id\", \"concept_code\"]),\n",
    "    on=[\"vocabulary_id\", \"concept_code\"],\n",
    "    how=\"anti\"\n",
    ")\n",
    "\n",
    "# Find rows in string_df not in phecode_df\n",
    "unique_to_string = string_df.join(\n",
    "    phecode_df.select([\"vocabulary_id\", \"concept_code\"]),\n",
    "    on=[\"vocabulary_id\", \"concept_code\"],\n",
    "    how=\"anti\"\n",
    ")\n",
    "\n",
    "# Concatenate the unique rows\n",
    "unique_rows_df = pl.concat([unique_to_phecode, unique_to_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi_inflammation_code_df = (aou.person_code_df(\n",
    "    name=\"gi_inflammation\",\n",
    "    vocabularies=[\"ICD9CM\", \"ICD10CM\", \"SNOMED\"],\n",
    "    search_terms=[\"ulcerative colitis\", \"crohn\", 'ulcerative (chronic) pancolitis',\n",
    "                  'ulcerative (chronic) rectosigmoiditis', 'ulcerative (chronic) proctitis', \n",
    "                  'left sided colitis', 'regional enteritis', 'ulcerative (chronic) enterocolitis',\n",
    "                  'ulcerative (chronic) colitis', 'ulcerative (chronic) proctosigmoiditis', \n",
    "                  'ulcerative (chronic) ileocolitis', 'pseudopolyposis', 'inflammatory polyps of colon',\n",
    "                  'lymphocytic colitis', 'collagenous colitis', 'microscopic colitis', 'enteritis and colitis',\n",
    "                  'protein-induced enteropathy', 'protein-induced enterocolitis', 'eosinophilic esophagitis',\n",
    "                  'eosinophilic gastr', 'eosinophilic colitis', 'ulceration of intestine', 'ulcer of anus', \n",
    "                  'ulcer of intestine', 'ulcer of anus', 'duodenitis', 'gastric mucosal hypertrophy', \n",
    "                  'specified gastritis', 'alcoholic gastritis', 'gastritis, unspecified', 'other gastritis',\n",
    "                  'chronic gastritis', 'superficial gastritis', 'acute gastritis', 'atrophic gastritis', \n",
    "                  'noninfective gastroenteritis', 'indeterminate colitis'\n",
    "                 ],\n",
    "    exclude_terms=['fh:', 'h/o:', 'radiation', 'infectious gastroenteritis and colitis, unspecified', \n",
    "                  'helicobacter', 'history of'],\n",
    "    dates=True\n",
    ").execute_gbq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi_inflammation_code_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{gi_inflammation_code_df.height} person_id with at least 1 GI inflammation code')\n",
    "print(f'{gi_inflammation_code_df.filter(pl.col(\"gi_inflammation_n\")>1).height} person_id with at least 2 GI inflammation codes')\n",
    "\n",
    "# Keep those with at least 1 code\n",
    "result_df = gi_inflammation_code_df.filter(pl.col('gi_inflammation_n')>0).select(['person_id', 'gi_inflammation_1', 'gi_inflammation_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBD by PheTK Method (Phecode Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phecode = Phecode(platform=\"aou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phecode.icd_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phecode_df = _utils.get_phecode_mapping_table(\n",
    "#     phecode_version=\"X\",\n",
    "#     icd_version=\"US\",\n",
    "#     phecode_map_file_path=None,\n",
    "#     keep_all_columns=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phecode.icd_events = phecode.icd_events.join(\n",
    "#     phecode_df,\n",
    "#     how='inner',\n",
    "#     on=['ICD', 'flag']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter for relevant phecodes\n",
    "# gi_inflammation_codes = phecode.icd_events.filter(\n",
    "#    pl.col('phecode').is_in(['GI_522.11', 'GI_522.12'])\n",
    "# )\n",
    "\n",
    "# # Sort by person_id and date to get chronological order\n",
    "# gi_inflammation_codes = gi_inflammation_codes.sort(['person_id', 'date'])\n",
    "\n",
    "# # Get first two IBD events (regardless of CD/UC)\n",
    "# gi_inflammation_events = (\n",
    "#     gi_inflammation_codes\n",
    "#     .group_by('person_id')\n",
    "#     .agg([\n",
    "#         pl.col('date').head(2).alias('dates'),\n",
    "#         pl.col('phecode').head(2).alias('phecodes')\n",
    "#     ])\n",
    "#     .with_columns([\n",
    "#         pl.col('dates').list.get(0, null_on_oob=True).alias('gi_inflammation_1'),\n",
    "#         pl.col('dates').list.get(1, null_on_oob=True).alias('gi_inflammation_2'),\n",
    "#         pl.col('phecodes').list.get(0, null_on_oob=True).alias('gi_inflammation_1_code'),\n",
    "#         pl.col('phecodes').list.get(1, null_on_oob=True).alias('gi_inflammation_2_code')\n",
    "#     ])\n",
    "#     .drop(['dates', 'phecodes'])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count occurrences for percentage\n",
    "# code_counts = (\n",
    "#    gi_inflammation_codes\n",
    "#    .group_by('person_id')\n",
    "#    .agg([\n",
    "#        pl.len().alias('total_events'),\n",
    "#        pl.col('phecode').filter(pl.col('phecode') == 'GI_522.11').len().alias('cd_count'),\n",
    "#        pl.col('phecode').filter(pl.col('phecode') == 'GI_522.12').len().alias('uc_count')\n",
    "#    ])\n",
    "#    .with_columns([\n",
    "#        pl.when(pl.col('total_events') >= 1)\n",
    "#        .then((pl.col('cd_count') / pl.col('total_events') * 100).round(1))\n",
    "#        .otherwise(None)\n",
    "#        .alias('crohns_pct')\n",
    "#    ])\n",
    "#    .with_columns([\n",
    "#        pl.when(pl.col('total_events') >= 1)\n",
    "#        .then((pl.col('uc_count') / pl.col('total_events') * 100).round(1))\n",
    "#        .otherwise(None)\n",
    "#        .alias('uc_pct')\n",
    "#    ])\n",
    "# )\n",
    "\n",
    "# # Join the results\n",
    "# result_df = gi_inflammation_events.join(code_counts, on='person_id', how='left')\n",
    "\n",
    "# print(f'{result_df.height} person_id with at least 1 IBD code')\n",
    "# print(f'{result_df.filter(pl.col(\"gi_inflammation_2\").is_not_null()).height} person_id with at least 2 IBD codes')\n",
    "\n",
    "# # Create breakdown column\n",
    "# result_df = result_df.with_columns([\n",
    "#     pl.when(pl.col('crohns_pct').is_not_null())\n",
    "#     .then(pl.format(\"{}% Crohn's, {}% UC\", pl.col('crohns_pct'), pl.col('uc_pct')))\n",
    "#     .otherwise(pl.lit(\"Insufficient events\"))\n",
    "#     .alias('breakdown')\n",
    "# ])\n",
    "\n",
    "# result_df = result_df.filter(pl.col('gi_inflammation_1').is_not_null()).select(['person_id', 'gi_inflammation_1', 'total_events', \n",
    "#                                                                     'cd_count', 'uc_count', 'breakdown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Age at IBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = %env WORKSPACE_CDR\n",
    "\n",
    "dob_q = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    p.person_id,\n",
    "    CAST(p.birth_datetime AS DATE) AS dob,\n",
    "FROM\n",
    "    {version}.person p\n",
    "\"\"\"\n",
    "\n",
    "dob_df = polars_gbq(dob_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.join(\n",
    "    dob_df,\n",
    "    on='person_id',\n",
    "    how='left'\n",
    ").with_columns([\n",
    "   pl.when(pl.col('gi_inflammation_1').is_not_null())\n",
    "   .then((pl.col('gi_inflammation_1') - pl.col('dob')).dt.total_days() / 365.25)\n",
    "   .otherwise(None)\n",
    "   .alias('age_at_gi_inflammation_1'),\n",
    "   pl.when(pl.col('gi_inflammation_2').is_not_null())\n",
    "   .then((pl.col('gi_inflammation_2') - pl.col('dob')).dt.total_days() / 365.25)\n",
    "   .otherwise(None)\n",
    "   .alias('age_at_gi_inflammation_2')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write Files\n",
    "# files = {\n",
    "#    'Tyr132Ser 1/1': f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532428_A_C_with_covariates\",\n",
    "#    'Tyr215Ter 0/1': f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532678_C_G_with_covariates\", \n",
    "#    'Asp306Asn 0/1': f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532949_G_A_with_covariates\"\n",
    "# }\n",
    "\n",
    "# for _, file in files.items():\n",
    "#     df = pl.read_csv(f'{file}.csv', separator=',')\n",
    "#     df = df.join(\n",
    "#         result_df.select(['person_id', 'gi_inflammation_1', 'age_at_gi_inflammation_1', 'total_events', 'cd_count',\n",
    "#                           'uc_count', 'breakdown']),\n",
    "#         on='person_id',\n",
    "#         how='left'\n",
    "#     ).with_columns([\n",
    "#         pl.col('gi_inflammation_1').fill_null(pl.lit(\"1901-01-01\").str.to_date()),\n",
    "#         pl.col('age_at_gi_inflammation_1').fill_null(-9),\n",
    "#         pl.col('total_events').fill_null(-9),\n",
    "#         pl.col('cd_count').fill_null(-9),\n",
    "#         pl.col('uc_count').fill_null(-9),\n",
    "#         pl.col('breakdown').fill_null(\"NA\")\n",
    "#     ])\n",
    "#     df.write_csv(f'{file}_and_gi_inflammation_phetk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Files\n",
    "files = {\n",
    "   'Tyr132Ser 1/1': f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532428_A_C_with_covariates\",\n",
    "   'Tyr215Ter 0/1': f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532678_C_G_with_covariates\", \n",
    "   'Asp306Asn 0/1': f\"{bucket}/data/gpr15/cohorts/aou_chr3_98532949_G_A_with_covariates\"\n",
    "}\n",
    "\n",
    "for _, file in files.items():\n",
    "    df = pl.read_csv(f'{file}.csv', separator=',')\n",
    "    df = df.join(\n",
    "        result_df.select(['person_id', 'gi_inflammation_1', 'age_at_gi_inflammation_1', \n",
    "                          'gi_inflammation_2', 'age_at_gi_inflammation_2']),\n",
    "        on='person_id',\n",
    "        how='left'\n",
    "    ).with_columns([\n",
    "        pl.col('gi_inflammation_1').fill_null(pl.lit(\"1901-01-01\").str.to_date()),\n",
    "        pl.col('age_at_gi_inflammation_1').fill_null(-9),\n",
    "        pl.col('gi_inflammation_2').fill_null(pl.lit(\"1901-01-01\").str.to_date()),\n",
    "        pl.col('age_at_gi_inflammation_2').fill_null(-9),\n",
    "    ])\n",
    "    df.write_csv(f'{file}_and_gi_inflammation_aouq.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Genetic Ancestry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard code consistent color palette before the loop\n",
    "ancestry_colors = {\n",
    "   'eur': '#1f77b4',  # blue\n",
    "   'afr': '#ff7f0e',  # orange\n",
    "   'amr': '#2ca02c',  # green\n",
    "   'eas': '#d62728',  # red\n",
    "   'mid': '#9467bd',  # purple\n",
    "   'sas': '#8c564b'   # brown\n",
    "}\n",
    "\n",
    "for variant_name, file_path in files.items():\n",
    "    df = pl.read_csv(f'{file_path}.csv', separator=',')\n",
    "\n",
    "    # 1. Display breakdown for genetic_ancestry where case == 1\n",
    "    cases_df = df.filter(pl.col('case') == 1)\n",
    "    ancestry_breakdown = cases_df.group_by('genetic_ancestry').agg(pl.len().alias('count'))\n",
    "    print(f\"\\nVariant: {variant_name}\")\n",
    "    print(\"Cases by genetic ancestry:\")\n",
    "    display(ancestry_breakdown)\n",
    "\n",
    "    # Convert to pandas for plotting\n",
    "    df_pd = df.to_pandas()\n",
    "    cases_pd = df_pd[df_pd['case'] == 1]\n",
    "\n",
    "    # 2. Create scatterplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4.5), dpi=300)\n",
    "    fig.suptitle(f'{variant_name} - PC Analysis', fontsize=16)\n",
    "\n",
    "    # PC1 vs PC2\n",
    "    sns.scatterplot(data=df_pd, x='pc1', y='pc2', hue='genetic_ancestry', \n",
    "                  alpha=0.1, ax=axes[0], legend=False, palette=color_map)\n",
    "    sns.scatterplot(data=cases_pd, x='pc1', y='pc2', hue='genetic_ancestry', \n",
    "                  s=50, ax=axes[0], legend=False, palette=color_map)\n",
    "    axes[0].set_title('PC1 vs PC2')\n",
    "\n",
    "    # PC2 vs PC3  \n",
    "    sns.scatterplot(data=df_pd, x='pc2', y='pc3', hue='genetic_ancestry', \n",
    "                  alpha=0.1, ax=axes[1], legend=False, palette=color_map)\n",
    "    sns.scatterplot(data=cases_pd, x='pc2', y='pc3', hue='genetic_ancestry', \n",
    "                  s=50, ax=axes[1], legend=False, palette=color_map)\n",
    "    axes[1].set_title('PC2 vs PC3')\n",
    "\n",
    "    # PC1 vs PC3\n",
    "    sns.scatterplot(data=df_pd, x='pc1', y='pc3', hue='genetic_ancestry', \n",
    "                  alpha=0.1, ax=axes[2], legend=False, palette=color_map)\n",
    "    sns.scatterplot(data=cases_pd, x='pc1', y='pc3', hue='genetic_ancestry', \n",
    "                  s=50, ax=axes[2], legend=False, palette=color_map)\n",
    "    axes[2].set_title('PC1 vs PC3')\n",
    "\n",
    "    # Add legend to bottom left of third facet\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[ancestry], \n",
    "                        markersize=8, label=ancestry) for ancestry in unique_ancestries]\n",
    "    axes[2].legend(handles=handles, loc='lower left', title='Genetic Ancestry')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
